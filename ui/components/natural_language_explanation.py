# ui/components/natural_language_explanation.py
# -*- coding: utf-8 -*-
"""
ìì—°ì–´ ì„¤ëª… ìƒì„± ì»´í¬ë„ŒíŠ¸
LLMì„ ì‚¬ìš©í•˜ì—¬ ë°©ì±… ì¶”ì²œ ì‚¬ìœ ë¥¼ ì‚¬ëŒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ìì—°ì–´ë¡œ ì„¤ëª…
"""
import streamlit as st
import json
from typing import Dict, List, Optional


def generate_natural_language_explanation(agent_result: Dict, core) -> str:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì²œ ë°©ì±…ì˜ ì‚¬ìœ ë¥¼ ìì—°ì–´ë¡œ ì„¤ëª… ìƒì„±
    
    Args:
        agent_result: Agent ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        core: CorePipeline ì¸ìŠ¤í„´ìŠ¤ (LLM ì ‘ê·¼ìš©)
    
    Returns:
        ìì—°ì–´ ì„¤ëª… í…ìŠ¤íŠ¸
    """
    if not agent_result or not core:
        return "ì„¤ëª…ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # LLM ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    if not core.llm_manager.is_available():
        return _generate_fallback_explanation(agent_result)
    
    try:
        # 1. ì¶”ì²œ ì •ë³´ ì¶”ì¶œ
        recommendations = agent_result.get("recommendations", [])
        if not recommendations:
            return "ì¶”ì²œëœ ë°©ì±…ì´ ì—†ìŠµë‹ˆë‹¤."
        
        top_recommendation = recommendations[0]
        situation_info = agent_result.get("situation_info", {})
        score_breakdown = top_recommendation.get("score_breakdown", {})
        situation_analysis = agent_result.get("situation_analysis", {})
        
        # 2. ìƒí™© ì •ë³´ ì •ë¦¬
        threat_level = situation_info.get('ì‹¬ê°ë„', situation_info.get('ìœ„í˜‘ìˆ˜ì¤€', 'N/A'))
        if isinstance(threat_level, (int, float)):
            if threat_level > 1.0:
                threat_level = threat_level / 100.0
            threat_level_text = f"{threat_level:.1%}"
        else:
            threat_level_text = str(threat_level)
        
        threat_type = situation_info.get('ìœ„í˜‘ìœ í˜•', 'N/A')
        location = situation_info.get('ë°œìƒì¥ì†Œ', situation_info.get('ì¥ì†Œ', 'N/A'))
        
        # 3. ì ìˆ˜ breakdown ì •ë³´ ì •ë¦¬
        breakdown_text = ""
        if score_breakdown:
            factor_names = {
                "threat": "ìœ„í˜‘ ìˆ˜ì¤€",
                "resources": "ìì› ê°€ìš©ì„±",
                "assets": "ì „ë ¥ ëŠ¥ë ¥",
                "environment": "í™˜ê²½ ì í•©ì„±",
                "historical": "ê³¼ê±° íš¨ê³¼ì„±",
                "chain": "ì—°ê³„ì„±"
            }
            
            breakdown_items = []
            for key, name in factor_names.items():
                score = score_breakdown.get(key, 0)
                if score > 0:
                    breakdown_items.append(f"- {name}: {score:.3f}")
            
            if breakdown_items:
                breakdown_text = "\n".join(breakdown_items)
        
        # 4. ë¹„êµ ì •ë³´ (ìƒìœ„ 3ê°œ ë°©ì±…)
        comparison_text = ""
        if len(recommendations) > 1:
            top3 = recommendations[:3]
            comparison_items = []
            for i, rec in enumerate(top3, 1):
                coa_name = rec.get('coa_name', 'Unknown')
                score = rec.get('score', 0)
                comparison_items.append(f"{i}. {coa_name} (ì ìˆ˜: {score:.3f})")
            
            if comparison_items:
                comparison_text = "\n".join(comparison_items)
        
        # 5. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°©ì±… ì¶”ì²œ ì‚¬ìœ ë¥¼ ì‚¬ëŒì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

## í˜„ì¬ ìƒí™©
- ìœ„í˜‘ ìˆ˜ì¤€: {threat_level_text}
- ìœ„í˜‘ ìœ í˜•: {threat_type}
- ë°œìƒ ì¥ì†Œ: {location}

## ì¶”ì²œ ë°©ì±…
- ë°©ì±…ëª…: {top_recommendation.get('coa_name', 'N/A')}
- ì¢…í•© ì ìˆ˜: {top_recommendation.get('score', 0):.3f}
- ê¸°ì¡´ ì¶”ì²œ ì‚¬ìœ : {top_recommendation.get('reason', 'N/A')}

## í‰ê°€ ìš”ì†Œë³„ ì ìˆ˜
{breakdown_text if breakdown_text else "ì ìˆ˜ ìƒì„¸ ì •ë³´ ì—†ìŒ"}

## ë‹¤ë¥¸ ë°©ì±…ê³¼ì˜ ë¹„êµ
{comparison_text if comparison_text else "ë¹„êµí•  ë‹¤ë¥¸ ë°©ì±… ì—†ìŒ"}

## ì„¤ëª… ìš”ì²­ì‚¬í•­
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”:

### 1. í˜„ì¬ ìƒí™© ìš”ì•½
í˜„ì¬ ìƒí™©ì„ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”.

### 2. ì´ ë°©ì±…ì´ ì„ íƒëœ ì£¼ìš” ì´ìœ  (3ê°€ì§€)
ì™œ ì´ ë°©ì±…ì´ í˜„ì¬ ìƒí™©ì— ê°€ì¥ ì í•©í•œì§€ 3ê°€ì§€ ì£¼ìš” ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.

### 3. ê° í‰ê°€ ìš”ì†Œë³„ í‰ê°€ ê²°ê³¼
ê° í‰ê°€ ìš”ì†Œ(ìœ„í˜‘ ìˆ˜ì¤€, ìì› ê°€ìš©ì„±, ì „ë ¥ ëŠ¥ë ¥, í™˜ê²½ ì í•©ì„±, ê³¼ê±° íš¨ê³¼ì„±, ì—°ê³„ì„±)ë³„ë¡œ ì–´ë–»ê²Œ í‰ê°€ë˜ì—ˆëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.

### 4. ì˜ˆìƒ íš¨ê³¼ ë° ì£¼ì˜ì‚¬í•­
ì´ ë°©ì±…ì„ ì‹¤í–‰í–ˆì„ ë•Œ ì˜ˆìƒë˜ëŠ” íš¨ê³¼ì™€ ì£¼ì˜í•´ì•¼ í•  ì‚¬í•­ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì„¤ëª…ì€ êµ°ì‚¬ ì‘ì „ ë‹´ë‹¹ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì „ë¬¸ì ì´ë©´ì„œë„ ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        # 6. LLM í˜¸ì¶œ
        explanation = core.llm_manager.generate(prompt, max_tokens=800, temperature=0.7)
        
        return explanation
        
    except Exception as e:
        st.warning(f"ìì—°ì–´ ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return _generate_fallback_explanation(agent_result)


def _generate_fallback_explanation(agent_result: Dict) -> str:
    """
    LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œ ê¸°ë³¸ ì„¤ëª… ìƒì„±
    
    Args:
        agent_result: Agent ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        ê¸°ë³¸ ì„¤ëª… í…ìŠ¤íŠ¸
    """
    recommendations = agent_result.get("recommendations", [])
    if not recommendations:
        return "ì¶”ì²œëœ ë°©ì±…ì´ ì—†ìŠµë‹ˆë‹¤."
    
    top_recommendation = recommendations[0]
    situation_info = agent_result.get("situation_info", {})
    
    coa_name = top_recommendation.get('coa_name', 'Unknown')
    score = top_recommendation.get('score', 0)
    reason = top_recommendation.get('reason', 'N/A')
    
    threat_level = situation_info.get('ì‹¬ê°ë„', situation_info.get('ìœ„í˜‘ìˆ˜ì¤€', 'N/A'))
    if isinstance(threat_level, (int, float)):
        if threat_level > 1.0:
            threat_level = threat_level / 100.0
        threat_level_text = f"{threat_level:.1%}"
    else:
        threat_level_text = str(threat_level)
    
    explanation = f"""## ë°©ì±… ì¶”ì²œ ì„¤ëª…

### í˜„ì¬ ìƒí™©
- ìœ„í˜‘ ìˆ˜ì¤€: {threat_level_text}
- ìœ„í˜‘ ìœ í˜•: {situation_info.get('ìœ„í˜‘ìœ í˜•', 'N/A')}
- ë°œìƒ ì¥ì†Œ: {situation_info.get('ë°œìƒì¥ì†Œ', situation_info.get('ì¥ì†Œ', 'N/A'))}

### ì¶”ì²œ ë°©ì±…
**{coa_name}** (ì¢…í•© ì ìˆ˜: {score:.3f})

### ì¶”ì²œ ì‚¬ìœ 
{reason if reason != 'N/A' else 'ì¶”ì²œ ì‚¬ìœ  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'}

### ì°¸ê³ ì‚¬í•­
LLMì„ ì‚¬ìš©í•œ ìƒì„¸ ì„¤ëª…ì„ ìƒì„±í•˜ë ¤ë©´ LLM ëª¨ë¸ì„ ë¡œë“œí•˜ê±°ë‚˜ OpenAI APIë¥¼ ì„¤ì •í•˜ì„¸ìš”."""
    
    return explanation


def render_natural_language_explanation(agent_result: Dict, core, key_prefix: str = "nl_explanation"):
    """
    ìì—°ì–´ ì„¤ëª…ì„ Streamlitì— ë Œë”ë§
    
    Args:
        agent_result: Agent ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        core: CorePipeline ì¸ìŠ¤í„´ìŠ¤
        key_prefix: Streamlit ìœ„ì ¯ í‚¤ ì ‘ë‘ì‚¬
    """
    if not agent_result:
        st.info("ì¶”ì²œ ê²°ê³¼ê°€ ì—†ì–´ ì„¤ëª…ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.subheader("ğŸ’¬ ìì—°ì–´ ì„¤ëª…")
    st.markdown("ì¶”ì²œ ë°©ì±…ì˜ ì‚¬ìœ ë¥¼ ì‚¬ëŒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ìì—°ì–´ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.")
    
    # ì„¤ëª… ìƒì„± ë²„íŠ¼
    if st.button("ğŸ“ ì„¤ëª… ìƒì„±", key=f"{key_prefix}_generate"):
        with st.spinner("ìì—°ì–´ ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            explanation = generate_natural_language_explanation(agent_result, core)
            
            # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.session_state[f"{key_prefix}_explanation"] = explanation
            st.session_state[f"{key_prefix}_generated"] = True
    
    # ì €ì¥ëœ ì„¤ëª…ì´ ìˆìœ¼ë©´ í‘œì‹œ
    if st.session_state.get(f"{key_prefix}_generated", False):
        explanation = st.session_state.get(f"{key_prefix}_explanation", "")
        
        if explanation:
            # ì„¤ëª… í‘œì‹œ
            st.markdown("---")
            st.markdown(explanation)
            
            # ë‹¤ì‹œ ìƒì„± ë²„íŠ¼
            if st.button("ğŸ”„ ì„¤ëª… ë‹¤ì‹œ ìƒì„±", key=f"{key_prefix}_regenerate"):
                with st.spinner("ìì—°ì–´ ì„¤ëª…ì„ ë‹¤ì‹œ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    explanation = generate_natural_language_explanation(agent_result, core)
                    st.session_state[f"{key_prefix}_explanation"] = explanation
                    st.rerun()
        else:
            st.warning("ì„¤ëª… ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


def render_inline_natural_language_explanation(agent_result: Dict, core) -> str:
    """
    ìì—°ì–´ ì„¤ëª…ì„ ì¸ë¼ì¸ìœ¼ë¡œ ìƒì„± (ìë™ ìƒì„±, ë²„íŠ¼ ì—†ìŒ)
    
    Args:
        agent_result: Agent ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        core: CorePipeline ì¸ìŠ¤í„´ìŠ¤
    
    Returns:
        ìì—°ì–´ ì„¤ëª… í…ìŠ¤íŠ¸
    """
    return generate_natural_language_explanation(agent_result, core)




