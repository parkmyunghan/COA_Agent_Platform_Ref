# ui/components/ontology_impact_analysis.py
# -*- coding: utf-8 -*-
"""
ì˜¨í†¨ë¡œì§€ ì˜í–¥ë ¥ ë¶„ì„ ì»´í¬ë„ŒíŠ¸
ì˜¨í†¨ë¡œì§€ì—ì„œ ì¶”ì¶œí•œ ì •ë³´ê°€ ì ìˆ˜ ê³„ì‚°ì— ë¯¸ì¹œ ì˜í–¥ì„ ë¶„ì„í•˜ê³  í‘œì‹œ
"""
import streamlit as st
import pandas as pd
from typing import Dict, List, Optional


def render_ontology_impact_analysis(agent_result: Dict, core=None):
    """
    ì˜¨í†¨ë¡œì§€ ì •ë³´ì˜ ì˜í–¥ë ¥ ë¶„ì„ ë Œë”ë§
    
    Args:
        agent_result: Agent ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        core: CorePipeline ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì )
    """
    if not agent_result:
        st.info("ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.subheader("ğŸ” ì˜¨í†¨ë¡œì§€ ì˜í–¥ë ¥ ë¶„ì„")
    st.markdown("ì˜¨í†¨ë¡œì§€ì—ì„œ ì¶”ì¶œí•œ ì •ë³´ê°€ ë°©ì±… ì¶”ì²œ ì ìˆ˜ì— ë¯¸ì¹œ ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    recommendations = agent_result.get("recommendations", [])
    if not recommendations:
        st.warning("ì¶”ì²œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    top_recommendation = recommendations[0]
    situation_analysis = agent_result.get("situation_analysis", {})
    score_breakdown = top_recommendation.get("score_breakdown", {})
    
    # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
    with st.expander("ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ ë° ë””ë²„ê¹… ì •ë³´", expanded=False):
        render_debug_info(agent_result, core, situation_analysis, score_breakdown)
    
    # 1. ì˜¨í†¨ë¡œì§€ ì •ë³´ ìš”ì•½
    with st.expander("ğŸ“Š ì˜¨í†¨ë¡œì§€ ì •ë³´ ìš”ì•½", expanded=True):
        render_ontology_info_summary(situation_analysis, agent_result, core)
    
    # 2. ì ìˆ˜ë³„ ì˜¨í†¨ë¡œì§€ ê¸°ì—¬ë„ ë¶„ì„
    with st.expander("ğŸ“ˆ ì ìˆ˜ë³„ ì˜¨í†¨ë¡œì§€ ê¸°ì—¬ë„", expanded=True):
        if not score_breakdown:
            st.warning("ì ìˆ˜ breakdown ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.info("""
            **ê°€ëŠ¥í•œ ì›ì¸:**
            1. íŒ”ë€í‹°ì–´ ëª¨ë“œê°€ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ê¸°ë³¸ ëª¨ë“œ ì‚¬ìš©)
            2. ì ìˆ˜ ê³„ì‚°ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
            
            **í•´ê²° ë°©ë²•:**
            - íŒ”ë€í‹°ì–´ ëª¨ë“œë¥¼ í™œì„±í™”í•˜ì—¬ Agentë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.
            - "ìƒí™© ì…ë ¥ ë° ì¶”ì²œ" íƒ­ì—ì„œ "íŒ”ë€í‹°ì–´ ëª¨ë“œ" ì²´í¬ë°•ìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”.
            """)
        else:
            render_score_ontology_contribution(score_breakdown, situation_analysis, top_recommendation)
    
    # 3. ì²´ì¸ ì •ë³´ ìƒì„¸ ë¶„ì„
    chain_info = situation_analysis.get("chain_info", {})
    if chain_info:
        with st.expander("ğŸ”— ê´€ê³„ ì²´ì¸ ë¶„ì„", expanded=False):
            render_chain_analysis(chain_info, top_recommendation)
    
    # 4. RAG ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸
    rag_results = situation_analysis.get("rag_results", [])
    if rag_results:
        with st.expander("ğŸ“š RAG ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸", expanded=False):
            render_rag_results_detail(rag_results, score_breakdown)
    
    # 5. ê´€ë ¨ ì—”í‹°í‹° ìƒì„¸
    related_entities = situation_analysis.get("related_entities", [])
    if related_entities:
        with st.expander("ğŸ·ï¸ ê´€ë ¨ ì—”í‹°í‹° ìƒì„¸", expanded=False):
            render_related_entities_detail(related_entities, core)


def render_ontology_info_summary(situation_analysis: Dict, agent_result: Dict, core=None):
    """ì˜¨í†¨ë¡œì§€ ì •ë³´ ìš”ì•½ í‘œì‹œ"""
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        rag_results = situation_analysis.get("rag_results", [])
        st.metric("RAG ê²€ìƒ‰ ê²°ê³¼", f"{len(rag_results)}ê°œ")
        if len(rag_results) == 0:
            if core and not core.rag_manager.is_available():
                st.caption("âš ï¸ RAG ë§¤ë‹ˆì € ë¯¸ì‚¬ìš©")
            else:
                st.caption("â„¹ï¸ RAG ê²€ìƒ‰ ë¯¸ìˆ˜í–‰")
    
    with col2:
        related_entities = situation_analysis.get("related_entities", [])
        st.metric("ê´€ë ¨ ì—”í‹°í‹°", f"{len(related_entities)}ê°œ")
        if len(related_entities) == 0:
            if core and core.ontology_manager.graph is None:
                st.caption("âš ï¸ ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ ì—†ìŒ")
            else:
                st.caption("â„¹ï¸ ê´€ë ¨ ì—”í‹°í‹° ë¯¸íƒìƒ‰")
    
    with col3:
        chain_info = situation_analysis.get("chain_info", {})
        chains = chain_info.get("chains", []) if chain_info else []
        st.metric("ê´€ê³„ ì²´ì¸", f"{len(chains)}ê°œ")
        if len(chains) == 0:
            palantir_mode = agent_result.get("palantir_mode", False)
            if not palantir_mode:
                st.caption("âš ï¸ íŒ”ë€í‹°ì–´ ëª¨ë“œ í•„ìš”")
            else:
                st.caption("â„¹ï¸ ì²´ì¸ ë¯¸ë°œê²¬")
    
    with col4:
        if hasattr(situation_analysis, 'graph') or 'graph_triples' in situation_analysis:
            triples = situation_analysis.get('graph_triples', 0)
            st.metric("ì˜¨í†¨ë¡œì§€ Triples", f"{triples}ê°œ")
        else:
            if core and core.ontology_manager.graph:
                triples_count = len(list(core.ontology_manager.graph.triples((None, None, None))))
                st.metric("ì˜¨í†¨ë¡œì§€ Triples", f"{triples_count}ê°œ")
            else:
                st.metric("ì˜¨í†¨ë¡œì§€ ì‚¬ìš©", "í™œì„±")
    
    # ì •ë³´ê°€ ì—†ì„ ë•Œ ì•ˆë‚´
    if len(situation_analysis) == 0:
        st.warning("âš ï¸ ìƒí™© ë¶„ì„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.info("""
        **ê°€ëŠ¥í•œ ì›ì¸:**
        1. Agent ì‹¤í–‰ ì‹œ ìƒí™© ë¶„ì„ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
        2. RAG ë§¤ë‹ˆì €ê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.
        3. ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
        
        **í™•ì¸ ì‚¬í•­:**
        - RAG ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
        - ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
        """)
    
    # ì˜¨í†¨ë¡œì§€ ì •ë³´ í™œìš© ì—¬ë¶€
    st.markdown("---")
    st.markdown("#### ì˜¨í†¨ë¡œì§€ ì •ë³´ í™œìš© í˜„í™©")
    
    # score_breakdownì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    top_recommendation = agent_result.get("recommendations", [{}])[0] if agent_result.get("recommendations") else {}
    score_breakdown = top_recommendation.get("score_breakdown", {})
    
    # ìì› ë§¤ì¹­ í™•ì¸ (resource_info ë˜ëŠ” score_breakdownì—ì„œ)
    resource_used = False
    if situation_analysis.get("resource_info"):
        resource_used = True
    elif score_breakdown and score_breakdown.get("resources", 0) > 0:
        resource_used = True
    else:
        resource_used = _check_resource_matching(situation_analysis)
    
    # í™˜ê²½ í˜¸í™˜ì„± í™•ì¸ (environment_info ë˜ëŠ” score_breakdownì—ì„œ)
    environment_used = False
    if situation_analysis.get("environment_info"):
        environment_used = True
    elif score_breakdown and score_breakdown.get("environment", 0) > 0:
        environment_used = True
    else:
        environment_used = _check_environment_compatibility(situation_analysis)
    
    ontology_usage = {
        "RAG ê²€ìƒ‰": len(situation_analysis.get("rag_results", [])) > 0,
        "ê´€ê³„ ì²´ì¸": len(situation_analysis.get("chain_info", {}).get("chains", [])) > 0,
        "ê´€ë ¨ ì—”í‹°í‹°": len(situation_analysis.get("related_entities", [])) > 0,
        "ìì› ë§¤ì¹­": resource_used,
        "í™˜ê²½ í˜¸í™˜ì„±": environment_used,
    }
    
    usage_data = []
    for key, value in ontology_usage.items():
        status = "âœ… ì‚¬ìš©ë¨" if value else "âŒ ë¯¸ì‚¬ìš©"
        reason = _get_usage_reason(key, value, situation_analysis, agent_result, core)
        usage_data.append({
            "í•­ëª©": key,
            "í™œìš© ì—¬ë¶€": status,
            "ì›ì¸/ìƒíƒœ": reason
        })
    
    usage_df = pd.DataFrame(usage_data)
    st.dataframe(usage_df, width='stretch', hide_index=True)
    
    # ë¯¸ì‚¬ìš© í•­ëª©ì— ëŒ€í•œ í•´ê²° ë°©ë²• ì œì‹œ
    unused_items = [key for key, value in ontology_usage.items() if not value]
    if unused_items:
        st.markdown("---")
        st.markdown("#### ğŸ’¡ í•´ê²° ë°©ë²•")
        for item in unused_items:
            solution = _get_solution_for_item(item, core, agent_result)
            if solution:
                with st.expander(f"âŒ {item} í™œì„±í™” ë°©ë²•", expanded=False):
                    st.markdown(solution)


def render_score_ontology_contribution(score_breakdown: Dict, situation_analysis: Dict, recommendation: Dict):
    """ì ìˆ˜ë³„ ì˜¨í†¨ë¡œì§€ ê¸°ì—¬ë„ ë¶„ì„"""
    if not score_breakdown:
        st.info("ì ìˆ˜ breakdown ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê° ì ìˆ˜ ìš”ì†Œë³„ ì˜¨í†¨ë¡œì§€ ê¸°ì—¬ë„ ë¶„ì„
    factors = {
        "threat": ("ìœ„í˜‘ ìˆ˜ì¤€", "ìœ„í˜‘ ì •ë³´ëŠ” ì£¼ë¡œ ì…ë ¥ ë°ì´í„°ì—ì„œ ì¶”ì¶œ"),
        "resources": ("ìì› ê°€ìš©ì„±", "ì˜¨í†¨ë¡œì§€ì—ì„œ COA-ìì› ê´€ê³„ ì¡°íšŒ"),
        "assets": ("ì „ë ¥ ëŠ¥ë ¥", "ì˜¨í†¨ë¡œì§€ì—ì„œ ì•„êµ° ìì‚° ì •ë³´ ì¡°íšŒ"),
        "environment": ("í™˜ê²½ ì í•©ì„±", "ì˜¨í†¨ë¡œì§€ì—ì„œ ê¸°ìƒ-ìœ„í˜‘ ê´€ê³„ ì¡°íšŒ"),
        "historical": ("ê³¼ê±° íš¨ê³¼ì„±", "RAG ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜"),
        "chain": ("ì—°ê³„ì„±", "ì˜¨í†¨ë¡œì§€ ê´€ê³„ ì²´ì¸ íƒìƒ‰ ê¸°ë°˜"),
    }
    
    contribution_data = []
    
    for key, (name, description) in factors.items():
        score = score_breakdown.get(key, 0)
        
        # ì˜¨í†¨ë¡œì§€ ê¸°ì—¬ë„ íŒë‹¨
        ontology_contribution = _analyze_ontology_contribution(
            key, score, situation_analysis, recommendation
        )
        
        contribution_data.append({
            "ìš”ì†Œ": name,
            "ì ìˆ˜": f"{score:.3f}",
            "ì˜¨í†¨ë¡œì§€ ê¸°ì—¬ë„": ontology_contribution["level"],
            "ê¸°ì—¬ ë‚´ìš©": ontology_contribution["description"],
            "ë°ì´í„° ì¶œì²˜": ontology_contribution["source"]
        })
    
    df = pd.DataFrame(contribution_data)
    st.dataframe(df, width='stretch', hide_index=True)
    
    # ê¸°ì—¬ë„ ì‹œê°í™”
    try:
        import plotly.express as px
        
        # ê¸°ì—¬ë„ ë ˆë²¨ì„ ìˆ«ìë¡œ ë³€í™˜
        level_map = {"ë†’ìŒ": 3, "ë³´í†µ": 2, "ë‚®ìŒ": 1, "ì—†ìŒ": 0}
        df_viz = df.copy()
        df_viz["ê¸°ì—¬ë„ ì ìˆ˜"] = df_viz["ì˜¨í†¨ë¡œì§€ ê¸°ì—¬ë„"].map(level_map)
        
        fig = px.bar(
            df_viz,
            x="ìš”ì†Œ",
            y="ê¸°ì—¬ë„ ì ìˆ˜",
            color="ì˜¨í†¨ë¡œì§€ ê¸°ì—¬ë„",
            title="ì˜¨í†¨ë¡œì§€ ê¸°ì—¬ë„ ë¶„ì„",
            color_discrete_map={"ë†’ìŒ": "#2ecc71", "ë³´í†µ": "#f39c12", "ë‚®ìŒ": "#e74c3c", "ì—†ìŒ": "#95a5a6"}
        )
        fig.update_layout(yaxis_title="ê¸°ì—¬ë„ (ë†’ìŒ=3, ë³´í†µ=2, ë‚®ìŒ=1, ì—†ìŒ=0)")
        st.plotly_chart(fig, width='stretch')
    except ImportError:
        st.info("Plotlyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì°¨íŠ¸ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def render_chain_analysis(chain_info: Dict, recommendation: Dict):
    """ì²´ì¸ ì •ë³´ ìƒì„¸ ë¶„ì„"""
    chains = chain_info.get("chains", [])
    if not chains:
        st.info("ì²´ì¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.write(f"**ë°œê²¬ëœ ê´€ê³„ ì²´ì¸: {len(chains)}ê°œ**")
    
    # ì²´ì¸ ìš”ì•½ ì •ë³´
    summary = chain_info.get("summary", {})
    if summary:
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("í‰ê·  ì²´ì¸ ê¸¸ì´", f"{summary.get('avg_depth', 0):.1f}")
        with col2:
            st.metric("í‰ê·  ì²´ì¸ ì ìˆ˜", f"{summary.get('avg_score', 0):.3f}")
        with col3:
            st.metric("ìµœê³  ì²´ì¸ ì ìˆ˜", f"{summary.get('max_score', 0):.3f}")
    
    # ìƒìœ„ ì²´ì¸ í‘œì‹œ
    st.markdown("#### ìƒìœ„ ê´€ê³„ ì²´ì¸")
    
    for i, chain in enumerate(chains[:5], 1):  # ìƒìœ„ 5ê°œë§Œ
        with st.container():
            path = chain.get("path", [])
            predicates = chain.get("predicates", [])
            score = chain.get("score", 0)
            
            # ì²´ì¸ ê²½ë¡œ í‘œì‹œ
            chain_text = " â†’ ".join([
                f"{_extract_entity_name(path[j])} ({_extract_predicate_name(predicates[j]) if j < len(predicates) else ''})"
                for j in range(len(path))
            ])
            
            st.markdown(f"**{i}. ì²´ì¸ ì ìˆ˜: {score:.3f}**")
            st.write(chain_text)
            
            if i < min(5, len(chains)):
                st.divider()
    
    # ì²´ì¸ ì ìˆ˜ê°€ ì¢…í•© ì ìˆ˜ì— ë¯¸ì¹œ ì˜í–¥
    chain_score = recommendation.get("score_breakdown", {}).get("chain", 0)
    chain_weight = 0.10  # ì²´ì¸ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’)
    
    st.markdown("#### ì²´ì¸ ì ìˆ˜ì˜ ì¢…í•© ì ìˆ˜ ê¸°ì—¬ë„")
    st.write(f"- ì²´ì¸ ì ìˆ˜: {chain_score:.3f}")
    st.write(f"- ì²´ì¸ ê°€ì¤‘ì¹˜: {chain_weight:.2f}")
    st.write(f"- ê¸°ì—¬ë„: {chain_score * chain_weight:.4f} (ì¢…í•© ì ìˆ˜ì— ê¸°ì—¬)")


def render_rag_results_detail(rag_results: List[Dict], score_breakdown: Dict):
    """RAG ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ í‘œì‹œ"""
    historical_score = score_breakdown.get("historical", 0)
    
    st.write(f"**ê³¼ê±° íš¨ê³¼ì„± ì ìˆ˜: {historical_score:.3f}**")
    st.write(f"**ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(rag_results)}ê°œ**")
    
    # ì„±ê³µ í‚¤ì›Œë“œ ë¶„ì„
    success_keywords = ['ì„±ê³µ', 'íš¨ê³¼ì ', 'ìŠ¹ë¦¬', 'ì™„ë£Œ', 'ë‹¬ì„±']
    success_count = 0
    
    for result in rag_results:
        text = result.get("text", "")
        if any(keyword in text for keyword in success_keywords):
            success_count += 1
    
    st.write(f"**ì„±ê³µ ì‚¬ë¡€ í¬í•¨ ë¬¸ì„œ: {success_count}ê°œ**")
    st.write(f"**ì„±ê³µë¥ : {success_count / len(rag_results) * 100:.1f}%** (ê³¼ê±° íš¨ê³¼ì„± ì ìˆ˜ ê³„ì‚°ì— ì‚¬ìš©)")
    
    st.markdown("---")
    st.markdown("#### ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸")
    
    for i, result in enumerate(rag_results[:5], 1):  # ìƒìœ„ 5ê°œë§Œ
        with st.container():
            text = result.get("text", "")
            score = result.get("score", 0)
            metadata = result.get("metadata", {})
            
            st.markdown(f"**{i}. ê´€ë ¨ë„: {score:.3f}**")
            
            # ì„±ê³µ í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸
            highlighted_text = text
            for keyword in success_keywords:
                if keyword in highlighted_text:
                    highlighted_text = highlighted_text.replace(
                        keyword, f"**{keyword}**"
                    )
            
            st.write(highlighted_text[:500] + ("..." if len(text) > 500 else ""))
            
            if metadata:
                st.caption(f"ì¶œì²˜: {metadata.get('source', 'N/A')}")
            
            if i < min(5, len(rag_results)):
                st.divider()


def render_related_entities_detail(related_entities: List[Dict], core=None):
    """ê´€ë ¨ ì—”í‹°í‹° ìƒì„¸ í‘œì‹œ"""
    st.write(f"**ë°œê²¬ëœ ê´€ë ¨ ì—”í‹°í‹°: {len(related_entities)}ê°œ**")
    
    # ì—”í‹°í‹° íƒ€ì…ë³„ ê·¸ë£¹í™”
    entity_types = {}
    for entity in related_entities:
        entity_type = entity.get("type", "ê¸°íƒ€")
        if entity_type not in entity_types:
            entity_types[entity_type] = []
        entity_types[entity_type].append(entity)
    
    # íƒ€ì…ë³„ í‘œì‹œ
    for entity_type, entities in entity_types.items():
        with st.expander(f"ğŸ“Œ {entity_type} ({len(entities)}ê°œ)", expanded=False):
            for entity in entities[:10]:  # íƒ€ì…ë³„ ìµœëŒ€ 10ê°œ
                entity_id = entity.get("id", entity.get("label", "Unknown"))
                entity_label = entity.get("label", entity_id)
                relations = entity.get("relations", [])
                
                st.write(f"**{entity_label}** ({entity_id})")
                if relations:
                    st.caption(f"ê´€ê³„: {', '.join(relations[:3])}")
                st.divider()


def _analyze_ontology_contribution(factor_key: str, score: float, situation_analysis: Dict, recommendation: Dict) -> Dict:
    """ì˜¨í†¨ë¡œì§€ ê¸°ì—¬ë„ ë¶„ì„"""
    rag_results = situation_analysis.get("rag_results", [])
    chain_info = situation_analysis.get("chain_info", {})
    related_entities = situation_analysis.get("related_entities", [])
    
    if factor_key == "chain":
        chains = chain_info.get("chains", []) if chain_info else []
        if chains:
            return {
                "level": "ë†’ìŒ",
                "description": f"ì˜¨í†¨ë¡œì§€ì—ì„œ {len(chains)}ê°œì˜ ê´€ê³„ ì²´ì¸ì„ ë°œê²¬í•˜ì—¬ ì—°ê³„ì„± ì ìˆ˜ ê³„ì‚°ì— ì§ì ‘ ì‚¬ìš©",
                "source": "ì˜¨í†¨ë¡œì§€ ê´€ê³„ ì²´ì¸ íƒìƒ‰"
            }
        else:
            return {
                "level": "ì—†ìŒ",
                "description": "ê´€ê³„ ì²´ì¸ì„ ì°¾ì§€ ëª»í•˜ì—¬ ê¸°ë³¸ê°’(0.5) ì‚¬ìš©",
                "source": "ê¸°ë³¸ê°’"
            }
    
    elif factor_key == "historical":
        if rag_results:
            success_count = sum(1 for r in rag_results if any(kw in r.get("text", "") for kw in ['ì„±ê³µ', 'íš¨ê³¼ì ', 'ìŠ¹ë¦¬']))
            return {
                "level": "ë†’ìŒ",
                "description": f"RAG ê²€ìƒ‰ ê²°ê³¼ {len(rag_results)}ê°œ ì¤‘ {success_count}ê°œê°€ ì„±ê³µ ì‚¬ë¡€ í¬í•¨",
                "source": "RAG ê²€ìƒ‰ (ê³¼ê±° ë¬¸ì„œ)"
            }
        else:
            return {
                "level": "ì—†ìŒ",
                "description": "RAG ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ê°’(0.5) ì‚¬ìš©",
                "source": "ê¸°ë³¸ê°’"
            }
    
    elif factor_key == "resources":
        # ìì› ì •ë³´ëŠ” ì˜¨í†¨ë¡œì§€ì—ì„œ ì¡°íšŒí–ˆëŠ”ì§€ í™•ì¸
        return {
            "level": "ë³´í†µ",
            "description": "ì˜¨í†¨ë¡œì§€ì—ì„œ COA-ìì› ê´€ê³„ë¥¼ ì¡°íšŒí•˜ì—¬ ìì› ê°€ìš©ì„± ê³„ì‚°",
            "source": "ì˜¨í†¨ë¡œì§€ COA-ìì› ê´€ê³„"
        }
    
    elif factor_key == "environment":
        # í™˜ê²½ ì •ë³´ëŠ” ì˜¨í†¨ë¡œì§€ì—ì„œ ì¡°íšŒí–ˆëŠ”ì§€ í™•ì¸
        return {
            "level": "ë³´í†µ",
            "description": "ì˜¨í†¨ë¡œì§€ì—ì„œ ê¸°ìƒ-ìœ„í˜‘ ê´€ê³„ë¥¼ ì¡°íšŒí•˜ì—¬ í™˜ê²½ ì í•©ì„± ê³„ì‚°",
            "source": "ì˜¨í†¨ë¡œì§€ ê¸°ìƒ-ìœ„í˜‘ ê´€ê³„"
        }
    
    elif factor_key == "assets":
        if related_entities:
            asset_entities = [e for e in related_entities if "ìì‚°" in str(e.get("type", "")) or "asset" in str(e.get("type", "")).lower()]
            if asset_entities:
                return {
                    "level": "ë³´í†µ",
                    "description": f"ì˜¨í†¨ë¡œì§€ì—ì„œ {len(asset_entities)}ê°œì˜ ìì‚° ì—”í‹°í‹° ë°œê²¬",
                    "source": "ì˜¨í†¨ë¡œì§€ ìì‚° ì—”í‹°í‹°"
                }
        
        return {
            "level": "ë‚®ìŒ",
            "description": "ì£¼ë¡œ ì…ë ¥ ë°ì´í„°ì—ì„œ ìì‚° ì •ë³´ ì¶”ì¶œ",
            "source": "ì…ë ¥ ë°ì´í„°"
        }
    
    elif factor_key == "threat":
        return {
            "level": "ë‚®ìŒ",
            "description": "ì£¼ë¡œ ì…ë ¥ ë°ì´í„°ì—ì„œ ìœ„í˜‘ ìˆ˜ì¤€ ì¶”ì¶œ",
            "source": "ì…ë ¥ ë°ì´í„°"
        }
    
    return {
        "level": "ì—†ìŒ",
        "description": "ì˜¨í†¨ë¡œì§€ ì •ë³´ ë¯¸ì‚¬ìš©",
        "source": "ê¸°ë³¸ê°’"
    }


def _check_resource_matching(situation_analysis: Dict) -> bool:
    """ìì› ë§¤ì¹­ ì •ë³´ í™•ì¸"""
    # ìì› ê´€ë ¨ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
    related_entities = situation_analysis.get("related_entities", [])
    for entity in related_entities:
        if "ìì›" in str(entity.get("type", "")) or "resource" in str(entity.get("type", "")).lower():
            return True
    return False


def _check_environment_compatibility(situation_analysis: Dict) -> bool:
    """í™˜ê²½ í˜¸í™˜ì„± ì •ë³´ í™•ì¸"""
    # ê¸°ìƒ/í™˜ê²½ ê´€ë ¨ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
    related_entities = situation_analysis.get("related_entities", [])
    for entity in related_entities:
        entity_type = str(entity.get("type", "")).lower()
        if "ê¸°ìƒ" in entity_type or "weather" in entity_type or "í™˜ê²½" in entity_type or "environment" in entity_type:
            return True
    return False


def _extract_entity_name(uri: str) -> str:
    """URIì—ì„œ ì—”í‹°í‹° ì´ë¦„ ì¶”ì¶œ"""
    if not uri:
        return "Unknown"
    if "#" in uri:
        return uri.split("#")[-1]
    if "/" in uri:
        return uri.split("/")[-1]
    return uri


def _extract_predicate_name(uri: str) -> str:
    """URIì—ì„œ í”„ë ˆë””ì¼€ì´íŠ¸ ì´ë¦„ ì¶”ì¶œ"""
    return _extract_entity_name(uri)


def render_debug_info(agent_result: Dict, core, situation_analysis: Dict, score_breakdown: Dict):
    """ë””ë²„ê¹… ì •ë³´ í‘œì‹œ"""
    st.write("**Agent ì‹¤í–‰ ê²°ê³¼ êµ¬ì¡°:**")
    
    debug_data = {
        "situation_analysis ì¡´ì¬": situation_analysis is not None and len(situation_analysis) > 0,
        "situation_analysis í‚¤": list(situation_analysis.keys()) if situation_analysis else [],
        "score_breakdown ì¡´ì¬": score_breakdown is not None and len(score_breakdown) > 0,
        "score_breakdown í‚¤": list(score_breakdown.keys()) if score_breakdown else [],
        "palantir_mode": agent_result.get("palantir_mode", False),
    }
    
    if core:
        debug_data.update({
            "RAG ë§¤ë‹ˆì € ì‚¬ìš© ê°€ëŠ¥": core.rag_manager.is_available() if hasattr(core, 'rag_manager') else "N/A",
            "ì„ë² ë”© ëª¨ë¸ ë¡œë“œ": core.rag_manager.embedding_model is not None if hasattr(core, 'rag_manager') else "N/A",
            "ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ ì¡´ì¬": core.ontology_manager.graph is not None if hasattr(core, 'ontology_manager') else "N/A",
        })
        if hasattr(core, 'ontology_manager') and core.ontology_manager.graph:
            triples_count = len(list(core.ontology_manager.graph.triples((None, None, None))))
            debug_data["ì˜¨í†¨ë¡œì§€ Triples ìˆ˜"] = triples_count
    
    st.json(debug_data)
    
    # ìƒì„¸ ì •ë³´
    st.markdown("---")
    st.write("**ìƒí™© ë¶„ì„ ìƒì„¸:**")
    if situation_analysis:
        st.json({
            "rag_results ê°œìˆ˜": len(situation_analysis.get("rag_results", [])),
            "related_entities ê°œìˆ˜": len(situation_analysis.get("related_entities", [])),
            "chain_info ì¡´ì¬": situation_analysis.get("chain_info") is not None,
            "chain_info chains ê°œìˆ˜": len(situation_analysis.get("chain_info", {}).get("chains", [])) if situation_analysis.get("chain_info") else 0,
        })
    else:
        st.warning("situation_analysisê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")


def _get_usage_reason(item: str, is_used: bool, situation_analysis: Dict, agent_result: Dict, core) -> str:
    """í•­ëª©ë³„ ì‚¬ìš© ì—¬ë¶€ ì›ì¸ ë°˜í™˜"""
    if is_used:
        return "ì •ìƒ ì‚¬ìš© ì¤‘"
    
    if item == "RAG ê²€ìƒ‰":
        if not core:
            return "Core ì •ë³´ ì—†ìŒ"
        if not hasattr(core, 'rag_manager'):
            return "RAG ë§¤ë‹ˆì € ì—†ìŒ"
        if not core.rag_manager.is_available():
            return "RAG ë§¤ë‹ˆì € ë¯¸ì‚¬ìš©"
        if not core.rag_manager.embedding_model:
            return "ì„ë² ë”© ëª¨ë¸ ë¯¸ë¡œë“œ"
        return "RAG ê²€ìƒ‰ ë¯¸ìˆ˜í–‰"
    
    elif item == "ê´€ê³„ ì²´ì¸":
        palantir_mode = agent_result.get("palantir_mode", False)
        if not palantir_mode:
            return "íŒ”ë€í‹°ì–´ ëª¨ë“œ ë¯¸í™œì„±í™”"
        chain_info = situation_analysis.get("chain_info", {})
        if not chain_info:
            return "ì²´ì¸ ì •ë³´ ë¯¸ìƒì„±"
        return "ì²´ì¸ ë¯¸ë°œê²¬"
    
    elif item == "ê´€ë ¨ ì—”í‹°í‹°":
        if not core:
            return "Core ì •ë³´ ì—†ìŒ"
        if not hasattr(core, 'ontology_manager'):
            return "ì˜¨í†¨ë¡œì§€ ë§¤ë‹ˆì € ì—†ìŒ"
        if not core.ontology_manager.graph:
            return "ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ ì—†ìŒ"
        return "ê´€ë ¨ ì—”í‹°í‹° ë¯¸íƒìƒ‰"
    
    elif item == "ìì› ë§¤ì¹­":
        palantir_mode = agent_result.get("palantir_mode", False)
        if not palantir_mode:
            return "íŒ”ë€í‹°ì–´ ëª¨ë“œ í•„ìš”"
        resource_info = situation_analysis.get("resource_info", {})
        if resource_info:
            return f"ìì› ì •ë³´ ì¡°íšŒë¨ (ì ìˆ˜: {resource_info.get('score', 0):.3f})"
        return "ìì› ì •ë³´ ë¯¸ì¡°íšŒ"
    
    elif item == "í™˜ê²½ í˜¸í™˜ì„±":
        palantir_mode = agent_result.get("palantir_mode", False)
        if not palantir_mode:
            return "íŒ”ë€í‹°ì–´ ëª¨ë“œ í•„ìš”"
        environment_info = situation_analysis.get("environment_info", {})
        if environment_info:
            return f"í™˜ê²½ ì •ë³´ ì¡°íšŒë¨ (ì ìˆ˜: {environment_info.get('score', 0):.3f})"
        return "í™˜ê²½ ì •ë³´ ë¯¸ì¡°íšŒ"
    
    return "ì•Œ ìˆ˜ ì—†ìŒ"


def _get_solution_for_item(item: str, core, agent_result: Dict) -> str:
    """í•­ëª©ë³„ í•´ê²° ë°©ë²• ë°˜í™˜"""
    if item == "RAG ê²€ìƒ‰":
        if not core or not hasattr(core, 'rag_manager'):
            return "ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë¬¸ì œì…ë‹ˆë‹¤. ì‹œìŠ¤í…œì„ ì¬ì‹œì‘í•˜ì„¸ìš”."
        if not core.rag_manager.is_available():
            return """
            **RAG ë§¤ë‹ˆì €ê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.**
            
            1. RAG ì¸ë±ìŠ¤ êµ¬ì¶• í™•ì¸:
               - "4ë‹¨ê³„: RAG ì¸ë±ìŠ¤ êµ¬ì„±" í˜ì´ì§€ë¡œ ì´ë™
               - ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”
            
            2. ì„ë² ë”© ëª¨ë¸ í™•ì¸:
               - ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
               - ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜ê°€ ìˆëŠ”ì§€ í™•ì¸
            """
        return """
        **RAG ê²€ìƒ‰ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.**
        
        - Agent ì‹¤í–‰ ì‹œ `enable_rag_search=True` ë˜ëŠ” `use_embedding=True`ë¡œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        - RAG ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        """
    
    elif item == "ê´€ê³„ ì²´ì¸":
        return """
        **ê´€ê³„ ì²´ì¸ì„ ì‚¬ìš©í•˜ë ¤ë©´:**
        
        1. íŒ”ë€í‹°ì–´ ëª¨ë“œ í™œì„±í™”:
           - "ìƒí™© ì…ë ¥ ë° ì¶”ì²œ" íƒ­ ë˜ëŠ” "Agent ì‹¤í–‰" í˜ì´ì§€ì—ì„œ
           - "íŒ”ë€í‹°ì–´ ëª¨ë“œ" ì²´í¬ë°•ìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”
        
        2. Agent ì¬ì‹¤í–‰:
           - íŒ”ë€í‹°ì–´ ëª¨ë“œê°€ í™œì„±í™”ëœ ìƒíƒœì—ì„œ Agentë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”
        
        3. ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ í™•ì¸:
           - ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ê°€ êµ¬ì¶•ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
           - ê´€ê³„ ì •ë³´ê°€ ì˜¨í†¨ë¡œì§€ì— í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
        """
    
    elif item == "ê´€ë ¨ ì—”í‹°í‹°":
        if not core or not hasattr(core, 'ontology_manager'):
            return "ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë¬¸ì œì…ë‹ˆë‹¤. ì‹œìŠ¤í…œì„ ì¬ì‹œì‘í•˜ì„¸ìš”."
        if not core.ontology_manager.graph:
            return """
            **ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ê°€ ì—†ìŠµë‹ˆë‹¤.**
            
            1. ë°ì´í„° ë¡œë“œ:
               - Excel ë°ì´í„° íŒŒì¼ì´ `data_lake/` í´ë”ì— ìˆëŠ”ì§€ í™•ì¸
            
            2. ì˜¨í†¨ë¡œì§€ êµ¬ì¶•:
               - Agentë¥¼ ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ê°€ êµ¬ì¶•ë©ë‹ˆë‹¤
               - ë˜ëŠ” "3ë‹¨ê³„: ì§€ì‹ê·¸ë˜í”„ ì¡°íšŒ" í˜ì´ì§€ì—ì„œ ê·¸ë˜í”„ ìƒì„± ë²„íŠ¼ í´ë¦­
            """
        return """
        **ê´€ë ¨ ì—”í‹°í‹° íƒìƒ‰ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.**
        
        - Agent ì‹¤í–‰ ì‹œ `use_reasoned_graph=True`ë¡œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        - ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ì— ê´€ë ¨ ì—”í‹°í‹° ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
        """
    
    elif item == "ìì› ë§¤ì¹­":
        return """
        **ìì› ë§¤ì¹­ì„ ì‚¬ìš©í•˜ë ¤ë©´:**
        
        1. íŒ”ë€í‹°ì–´ ëª¨ë“œ í™œì„±í™”:
           - íŒ”ë€í‹°ì–´ ëª¨ë“œì—ì„œë§Œ ìì› ë§¤ì¹­ ì ìˆ˜ê°€ ê³„ì‚°ë©ë‹ˆë‹¤
        
        2. ì˜¨í†¨ë¡œì§€ ê´€ê³„ í™•ì¸:
           - COA-ìì› ê´€ê³„ê°€ ì˜¨í†¨ë¡œì§€ì— í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
           - FK ê¸°ë°˜ ê´€ê³„ê°€ í…Œì´ë¸”ì •ì˜ì„œì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
        """
    
    elif item == "í™˜ê²½ í˜¸í™˜ì„±":
        return """
        **í™˜ê²½ í˜¸í™˜ì„±ì„ ì‚¬ìš©í•˜ë ¤ë©´:**
        
        1. íŒ”ë€í‹°ì–´ ëª¨ë“œ í™œì„±í™”:
           - íŒ”ë€í‹°ì–´ ëª¨ë“œì—ì„œë§Œ í™˜ê²½ ì í•©ì„± ì ìˆ˜ê°€ ê³„ì‚°ë©ë‹ˆë‹¤
        
        2. ì˜¨í†¨ë¡œì§€ ê´€ê³„ í™•ì¸:
           - ê¸°ìƒ-ìœ„í˜‘ ê´€ê³„ê°€ ì˜¨í†¨ë¡œì§€ì— í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
           - FK ê¸°ë°˜ ê´€ê³„ê°€ í…Œì´ë¸”ì •ì˜ì„œì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
        """
    
    return "í•´ê²° ë°©ë²•ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

