# ui/components/data_quality_checker.py
# -*- coding: utf-8 -*-
"""
ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì»´í¬ë„ŒíŠ¸
ì…ë ¥ ë°ì´í„°ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ë¬¸ì œì ì„ ì‹ë³„
ì—‘ì…€ íŒŒì¼ì˜ ë‘ ë²ˆì§¸ ì‹œíŠ¸(í…Œì´ë¸”ì •ì˜ì„œ)ë¥¼ ì°¸ì¡°í•˜ì—¬ ë™ì ìœ¼ë¡œ ê²€ì¦ ê¸°ì¤€ ìƒì„±
"""
import streamlit as st
import pandas as pd
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import re


def render_data_quality_checker(data_manager, config: Optional[Dict] = None):
    """
    ë°ì´í„° í’ˆì§ˆ ê²€ì¦ íŒ¨ë„ ë Œë”ë§
    
    Args:
        data_manager: DataManager ì¸ìŠ¤í„´ìŠ¤
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    st.subheader("ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì¦")
    
    if st.button("ğŸ”„ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹¤í–‰", type="primary"):
        with st.spinner("ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì¤‘..."):
            quality_results = perform_quality_checks(data_manager, config)
            render_quality_results(quality_results)


def perform_quality_checks(data_manager, config: Optional[Dict] = None) -> Dict:
    """
    ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ìˆ˜í–‰
    
    Args:
        data_manager: DataManager ì¸ìŠ¤í„´ìŠ¤
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ê²€ì‚¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    results = {
        "overall_status": "pass",
        "checks": [],
        "summary": {
            "total_tables": 0,
            "passed_tables": 0,
            "failed_tables": 0,
            "total_issues": 0
        }
    }
    
    try:
        # ëª¨ë“  í…Œì´ë¸” ë¡œë“œ
        all_data = data_manager.load_all()
        results["summary"]["total_tables"] = len(all_data)
        
        # ê° í…Œì´ë¸”ë³„ ê²€ì‚¬
        for table_name, df in all_data.items():
            if df is None or df.empty:
                results["checks"].append({
                    "table": table_name,
                    "status": "error",
                    "message": "í…Œì´ë¸”ì´ ë¹„ì–´ìˆê±°ë‚˜ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "issues": []
                })
                results["summary"]["failed_tables"] += 1
                continue
            
            table_results = check_table_quality(table_name, df, config, data_manager)
            results["checks"].append(table_results)
            
            if table_results["status"] == "pass":
                results["summary"]["passed_tables"] += 1
            else:
                results["summary"]["failed_tables"] += 1
                results["summary"]["total_issues"] += len(table_results.get("issues", []))
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        if results["summary"]["failed_tables"] > 0:
            results["overall_status"] = "warning"
        if results["summary"]["total_issues"] > 10:
            results["overall_status"] = "error"
            
    except Exception as e:
        results["overall_status"] = "error"
        results["error"] = str(e)
    
    return results


def load_table_schema(table_name: str, config: Optional[Dict] = None) -> Optional[Dict]:
    """
    ì—‘ì…€ íŒŒì¼ì˜ ë‘ ë²ˆì§¸ ì‹œíŠ¸(í…Œì´ë¸”ì •ì˜ì„œ)ë¥¼ ì½ì–´ì„œ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ
    
    Args:
        table_name: í…Œì´ë¸”ëª…
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ìŠ¤í‚¤ë§ˆ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
    """
    try:
        # data_lake ê²½ë¡œ ì°¾ê¸°
        data_lake_path = "./data_lake"
        if config:
            data_lake_path = config.get("data_lake_path", data_lake_path)
        
        base_dir = Path(__file__).parent.parent.parent
        excel_file = base_dir / data_lake_path / f"{table_name}.xlsx"
        
        if not excel_file.exists():
            return None
        
        # ì—‘ì…€ íŒŒì¼ì˜ ì‹œíŠ¸ ëª©ë¡ í™•ì¸
        excel_file_obj = pd.ExcelFile(excel_file)
        sheet_names = excel_file_obj.sheet_names
        
        # í…Œì´ë¸”ì •ì˜ì„œ ì‹œíŠ¸ ì°¾ê¸°
        schema_sheet = None
        for sheet in sheet_names:
            if "ì •ì˜ì„œ" in sheet or "schema" in sheet.lower() or "ì •ì˜" in sheet:
                schema_sheet = sheet
                break
        
        if not schema_sheet:
            return None
        
        # í…Œì´ë¸”ì •ì˜ì„œ ì½ê¸°
        schema_df = pd.read_excel(excel_file, sheet_name=schema_sheet)
        
        # ì»¬ëŸ¼ëª… ì •ê·œí™” (ìƒˆë¡œìš´ êµ¬ì¡°: í•„ë“œëª…, íƒ€ì…, PK, FK, ë°ì´í„°ëª©ë¡, ê´€ê³„)
        field_col = None
        type_col = None
        pk_col = None
        fk_col = None
        data_list_col = None
        relation_col = None
        
        for col in schema_df.columns:
            col_lower = str(col).lower()
            col_str = str(col)
            if "í•„ë“œ" in col_str or "field" in col_lower or "ì»¬ëŸ¼" in col_str:
                field_col = col
            elif "íƒ€ì…" in col_str or "type" in col_lower:
                type_col = col
            elif col_str == "PK" or col_lower == "pk":
                pk_col = col
            elif col_str == "FK" or col_lower == "fk":
                fk_col = col
            elif "ë°ì´í„°ëª©ë¡" in col_str or "ë°ì´í„° ëª©ë¡" in col_str or "data" in col_lower and "list" in col_lower:
                data_list_col = col
            elif "ê´€ê³„" in col_str or "relation" in col_lower:
                relation_col = col
        
        if not field_col:
            return None
        
        # ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ì¶œ
        schema = {
            "primary_keys": [],
            "foreign_keys": [],
            "required_columns": [],
            "column_types": {},
            "value_ranges": {},
            "constraints": {},
            "enums": {}
        }
        
        for idx, row in schema_df.iterrows():
            field_name = str(row[field_col]).strip()
            if pd.isna(field_name) or field_name == "":
                continue
            
            # 1. PK í™•ì¸ (PK ì»¬ëŸ¼ì—ì„œ Y ê°’ í™•ì¸)
            if pk_col and pk_col in row:
                pk_value = row[pk_col]
                if not pd.isna(pk_value) and str(pk_value).upper() in ['Y', 'YES', 'TRUE', '1', 'ì˜ˆ', 'O']:
                    schema["primary_keys"].append(field_name)
                    schema["required_columns"].append(field_name)
            
            # 2. FK í™•ì¸ (FK ì»¬ëŸ¼ì—ì„œ Y ê°’ í™•ì¸, ê´€ê³„ ì»¬ëŸ¼ì—ì„œ ê´€ê³„ ì •ë³´ ì¶”ì¶œ)
            if fk_col and fk_col in row:
                fk_value = row[fk_col]
                if not pd.isna(fk_value) and str(fk_value).upper() in ['Y', 'YES', 'TRUE', '1', 'ì˜ˆ', 'O']:
                    # ê´€ê³„ ì»¬ëŸ¼ì—ì„œ FK ê´€ê³„ ì •ë³´ ì¶”ì¶œ
                    if relation_col and relation_col in row:
                        relation = str(row[relation_col]) if not pd.isna(row[relation_col]) else ""
                        if relation and relation.strip():
                            # í˜•ì‹: í…Œì´ë¸”ëª…:ì»¬ëŸ¼ëª… ë˜ëŠ” í…Œì´ë¸”ëª….ì»¬ëŸ¼ëª…
                            # ì˜ˆ: ì „ì¥ì¶•ì„ :ì¶•ì„ ID, ì§€í˜•ì…€:ì§€í˜•ì…€ID
                            fk_match = re.search(r'([^:.,]+)[:.,]\s*([^\s,]+)', relation)
                            if fk_match:
                                target_table = fk_match.group(1).strip()
                                target_column = fk_match.group(2).strip()
                                schema["foreign_keys"].append({
                                    "column": field_name,
                                    "target_table": target_table,
                                    "target_column": target_column
                                })
            
            # 3. ë°ì´í„° íƒ€ì… ì¶”ì¶œ
            if type_col and type_col in row:
                col_type = str(row[type_col]).strip().lower() if not pd.isna(row[type_col]) else ""
                if col_type:
                    schema["column_types"][field_name] = col_type
            
            # 4. ë°ì´í„°ëª©ë¡ ì»¬ëŸ¼ì—ì„œ ê°’ ë²”ìœ„ ë° ì—´ê±°í˜• ì¶”ì¶œ
            if data_list_col and data_list_col in row:
                data_list = str(row[data_list_col]) if not pd.isna(row[data_list_col]) else ""
                if data_list and data_list.strip():
                    # ê°’ ë²”ìœ„ ì¶”ì¶œ (ì˜ˆ: 0~100, 1-5)
                    range_match = re.search(r'(\d+)\s*[~-]\s*(\d+)', data_list)
                    if range_match:
                        min_val = int(range_match.group(1))
                        max_val = int(range_match.group(2))
                        schema["value_ranges"][field_name] = {"min": min_val, "max": max_val}
                    else:
                        # ì—´ê±°í˜• ê°’ ì¶”ì¶œ (ì˜ˆ: ê°€ìš©/ì†ì‹¤/ì´ë™ì¤‘, High/Medium/Low)
                        # ìŠ¬ë˜ì‹œ ë˜ëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ê°’ë“¤
                        enum_values = []
                        # ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±° (ë“±, etc, ê¸°íƒ€ ë“±)
                        data_list_clean = re.sub(r'\s*(ë“±|etc|ê¸°íƒ€|ì™¸|ì´ìƒ|ì´í•˜).*$', '', data_list, flags=re.IGNORECASE)
                        data_list_clean = data_list_clean.strip()
                        
                        # ìŠ¬ë˜ì‹œ êµ¬ë¶„
                        if '/' in data_list_clean:
                            enum_values = [v.strip() for v in data_list_clean.split('/') if v.strip()]
                        # ì‰¼í‘œ êµ¬ë¶„
                        elif ',' in data_list_clean:
                            enum_values = [v.strip() for v in data_list_clean.split(',') if v.strip()]
                        
                        if enum_values:
                            schema["enums"][field_name] = enum_values
        
        return schema
        
    except Exception as e:
        print(f"[WARN] í…Œì´ë¸”ì •ì˜ì„œ ë¡œë“œ ì‹¤íŒ¨ ({table_name}): {e}")
        return None


def check_table_quality(table_name: str, df: pd.DataFrame, config: Optional[Dict] = None, data_manager=None) -> Dict:
    """
    ê°œë³„ í…Œì´ë¸” í’ˆì§ˆ ê²€ì‚¬
    
    Args:
        table_name: í…Œì´ë¸” ì´ë¦„
        df: DataFrame
        
    Returns:
        ê²€ì‚¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    issues = []
    status = "pass"
    
    # í…Œì´ë¸”ì •ì˜ì„œì—ì„œ ìŠ¤í‚¤ë§ˆ ì •ë³´ ë¡œë“œ
    schema = load_table_schema(table_name, config)
    
    # 1. í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸ (í…Œì´ë¸”ì •ì˜ì„œ ìš°ì„ )
    required_columns = []
    if schema and schema.get("required_columns"):
        required_columns = schema["required_columns"]
    else:
        # í´ë°±: ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
        required_columns = get_required_columns(table_name, config=config)
    
    if required_columns:
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            issues.append({
                "type": "missing_columns",
                "severity": "error",
                "message": f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {', '.join(missing_columns)}"
            })
            status = "error"
    
    # 2. PK ì»¬ëŸ¼ ì¡´ì¬ ë° ê³ ìœ ì„± í™•ì¸ (í…Œì´ë¸”ì •ì˜ì„œ ê¸°ë°˜)
    if schema and schema.get("primary_keys"):
        pk_columns = [col for col in schema["primary_keys"] if col in df.columns]
        
        if pk_columns:
            for pk_col in pk_columns:
                # NULL ê°’ í™•ì¸
                if df[pk_col].isna().any():
                    issues.append({
                        "type": "null_primary_key",
                        "severity": "error",
                        "message": f"PK ì»¬ëŸ¼ '{pk_col}'ì— NULL ê°’ì´ ìˆìŠµë‹ˆë‹¤."
                    })
                    status = "error"
                
                # ì¤‘ë³µ í™•ì¸
                if df[pk_col].duplicated().any():
                    issues.append({
                        "type": "duplicate_primary_key",
                        "severity": "error",
                        "message": f"PK ì»¬ëŸ¼ '{pk_col}'ì— ì¤‘ë³µ ê°’ì´ ìˆìŠµë‹ˆë‹¤."
                    })
                    status = "error"
        else:
            # PKê°€ ì •ì˜ë˜ì–´ ìˆì§€ë§Œ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°
            missing_pk = [col for col in schema["primary_keys"] if col not in df.columns]
            if missing_pk:
                issues.append({
                    "type": "missing_primary_key",
                    "severity": "error",
                    "message": f"PK ì»¬ëŸ¼ ëˆ„ë½: {', '.join(missing_pk)}"
                })
                status = "error"
    else:
        # í…Œì´ë¸”ì •ì˜ì„œê°€ ì—†ê±°ë‚˜ PK ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
        id_columns = [col for col in df.columns if 'ID' in col.upper() or col.upper() == 'ID']
        
        if id_columns:
            id_col = id_columns[0]
            if df[id_col].isna().any():
                issues.append({
                    "type": "null_id",
                    "severity": "error",
                    "message": f"{id_col} ì»¬ëŸ¼ì— NULL ê°’ì´ ìˆìŠµë‹ˆë‹¤."
                })
                status = "error"
            
            if df[id_col].duplicated().any():
                issues.append({
                    "type": "duplicate_id",
                    "severity": "error",
                    "message": f"{id_col} ì»¬ëŸ¼ì— ì¤‘ë³µ ê°’ì´ ìˆìŠµë‹ˆë‹¤."
                })
                status = "error"
        else:
            # ID ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° - ì„¤ì • í…Œì´ë¸”ì€ ì˜ˆì™¸ ì²˜ë¦¬
            id_optional_tables = ["í‰ê°€ê¸°ì¤€_ê°€ì¤‘ì¹˜"]
            if table_name not in id_optional_tables:
                issues.append({
                    "type": "no_id_column",
                    "severity": "warning",
                    "message": "ID ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì˜¨í†¨ë¡œì§€ êµ¬ì¶•ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                })
                if status == "pass":
                    status = "warning"
    
    # 3. ë°ì´í„° íƒ€ì… í™•ì¸ (í…Œì´ë¸”ì •ì˜ì„œ ê¸°ë°˜)
    type_issues = check_data_types(df, schema)
    if type_issues:
        issues.extend(type_issues)
        if any(issue["severity"] == "error" for issue in type_issues):
            status = "error"
        elif status == "pass":
            status = "warning"
    
    # 4. NULL ê°’ í™•ì¸
    null_counts = df.isnull().sum()
    high_null_columns = null_counts[null_counts > len(df) * 0.5]
    if not high_null_columns.empty:
        issues.append({
            "type": "high_null_ratio",
            "severity": "warning",
            "message": f"50% ì´ìƒ NULLì¸ ì»¬ëŸ¼: {', '.join(high_null_columns.index.tolist())}"
        })
        if status == "pass":
            status = "warning"
    
    # 5. ì¤‘ë³µ í–‰ í™•ì¸
    duplicate_rows = df.duplicated().sum()
    if duplicate_rows > 0:
        issues.append({
            "type": "duplicate_rows",
            "severity": "warning",
            "message": f"ì¤‘ë³µ í–‰ {duplicate_rows}ê°œ ë°œê²¬"
        })
        if status == "pass":
            status = "warning"
    
    # 6. ê°’ ë²”ìœ„ ê²€ì¦ (í…Œì´ë¸”ì •ì˜ì„œ ê¸°ë°˜)
    range_issues = check_value_ranges(table_name, df, schema)
    if range_issues:
        issues.extend(range_issues)
        if any(issue["severity"] == "error" for issue in range_issues):
            status = "error"
        elif status == "pass":
            status = "warning"
    
    # 7. ì—´ê±°í˜• ê°’ ê²€ì¦ (í…Œì´ë¸”ì •ì˜ì„œ ê¸°ë°˜)
    enum_issues = check_enum_values(df, schema)
    if enum_issues:
        issues.extend(enum_issues)
        if any(issue["severity"] == "error" for issue in enum_issues):
            status = "error"
        elif status == "pass":
            status = "warning"
    
    # 8. FK ì°¸ì¡° ë¬´ê²°ì„± ê²€ì¦ (í…Œì´ë¸”ì •ì˜ì„œ ê¸°ë°˜)
    fk_issues = check_foreign_key_integrity(table_name, df, schema, data_manager, config)
    if fk_issues:
        issues.extend(fk_issues)
        if any(issue["severity"] == "error" for issue in fk_issues):
            status = "error"
        elif status == "pass":
            status = "warning"
    
    return {
        "table": table_name,
        "status": status,
        "row_count": len(df),
        "column_count": len(df.columns),
        "issues": issues
    }


def get_required_columns(table_name: str, metadata_path: str = "./metadata", config: Optional[Dict] = None) -> List[str]:
    """
    í…Œì´ë¸”ë³„ í•„ìˆ˜ ì»¬ëŸ¼ ëª©ë¡ ë°˜í™˜ (ì—‘ì…€ì˜ "í…Œì´ë¸”ì •ì˜ì„œ" ì‹œíŠ¸ ìš°ì„  ì‚¬ìš©)
    
    Args:
        table_name: í…Œì´ë¸”ëª…
        metadata_path: ë©”íƒ€ë°ì´í„° ê²½ë¡œ (í•˜ìœ„ í˜¸í™˜ì„±)
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (data_paths í¬í•¨ ê°€ëŠ¥)
    
    Returns:
        í•„ìˆ˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
    """
    # 1. ì—‘ì…€ì˜ "í…Œì´ë¸”ì •ì˜ì„œ" ì‹œíŠ¸ì—ì„œ PK í•„ë“œ ì¶”ì¶œ (ìš°ì„ )
    try:
        schema = load_table_schema(table_name, config)
        if schema:
            # PKë¡œ í‘œì‹œëœ í•„ë“œë“¤ì„ í•„ìˆ˜ ì»¬ëŸ¼ìœ¼ë¡œ ë°˜í™˜
            required = []
            for field_info in schema.get('fields', []):
                field_name = field_info.get('name', '')
                description = field_info.get('description', '').upper()
                
                # PK í‘œì‹œê°€ ìˆê±°ë‚˜ ì„¤ëª…ì— "í•„ìˆ˜"ê°€ ìˆìœ¼ë©´ í•„ìˆ˜ ì»¬ëŸ¼
                if 'PK' in description or 'í•„ìˆ˜' in description or 'NOT NULL' in description:
                    required.append(field_name)
            
            if required:
                return required
    except Exception as e:
        # ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ë°©ë²• ì‹œë„
        pass
    
    # 2. DataManagerë¥¼ í†µí•´ ë¡œë” ì‚¬ìš© ì‹œë„
    try:
        if config:
            from core_pipeline.data_manager import DataManager
            data_manager = DataManager(config)
            loader = data_manager.get_loader(table_name)
            if loader:
                schema = loader.load_schema()
                pk_columns = loader.get_primary_keys()
                if pk_columns:
                    return pk_columns
    except Exception as e:
        # ë¡œë” ì‚¬ìš© ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ë°©ë²• ì‹œë„
        pass
    
    # 3. ë©”íƒ€ë°ì´í„° íŒŒì¼ì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ì½ê¸° (í•˜ìœ„ í˜¸í™˜ì„±)
    if config:
        metadata_path = config.get("metadata_path", metadata_path)
    
    metadata_file = Path(metadata_path) / "table_metadata.json"
    if metadata_file.exists():
        try:
            import json
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                table_meta = metadata.get("tables", {}).get(table_name, {})
                required = table_meta.get("required_columns", [])
                if required:
                    return required
        except Exception as e:
            # ë©”íƒ€ë°ì´í„° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì‹œ í´ë°±
            pass
    
    # 4. ë ˆê±°ì‹œ í•˜ë“œì½”ë”© (í•˜ìœ„ í˜¸í™˜ì„±, ì ì§„ì  ì œê±° ì˜ˆì •)
    # í‘œì¤€ í…Œì´ë¸”ëª…ìœ¼ë¡œ ë§¤í•‘
    standard_table_mapping = {
        "ìœ„í˜‘ìƒí™©": ["ìœ„í˜‘ID"],
        "ì•„êµ°ë¶€ëŒ€í˜„í™©": ["ì•„êµ°ë¶€ëŒ€ID"],
        "ì êµ°ë¶€ëŒ€í˜„í™©": ["ì êµ°ë¶€ëŒ€ID"],
        "ì„ë¬´ì •ë³´": ["ì„ë¬´ID"],
        "ì „ì¥ì¶•ì„ ": ["ì¶•ì„ ID"],
        "ì§€í˜•ì…€": ["ì§€í˜•ì…€ID"],
        "ì œì•½ì¡°ê±´": ["ì œì•½ID"],
    }
    
    # í‘œì¤€ í…Œì´ë¸”ëª… ë§¤í•‘ì—ì„œ ì°¾ê¸°
    if table_name in standard_table_mapping:
        return standard_table_mapping[table_name]
    
    # ë ˆê±°ì‹œ ë§¤í•‘ (í•˜ìœ„ í˜¸í™˜ì„±)
    legacy_required_columns = {
        "ìœ„í˜‘ìƒí™©": ["ID", "ìœ„í˜‘ìœ í˜•", "ì‹¬ê°ë„", "ë°œìƒì¥ì†Œ"],
        "ì•„êµ°ë¶€ëŒ€": ["ë¶€ëŒ€ëª…"],
        "ì êµ°ë¶€ëŒ€": ["ID", "ë¶€ëŒ€ëª…", "ë¶€ëŒ€ìœ í˜•"],
        "ì•„êµ°ê°€ìš©ìì‚°": ["ìì‚°ID", "ìì‚°ëª…", "ìì‚°ì¢…ë¥˜"],
        "ê¸°ìƒìƒí™©": ["ID", "ì¥ì†Œ", "ìƒíƒœ"],
        "ë³´ê¸‰ìƒíƒœ": ["ID", "ì¥ì†Œ", "ì¬ê³ ìˆ˜ì¤€"]
    }
    
    # ë ˆê±°ì‹œ ë§¤í•‘ì—ì„œ ì°¾ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ - ëª¨ë“  ì»¬ëŸ¼ ì„ íƒì )
    return legacy_required_columns.get(table_name, [])


def check_data_types(df: pd.DataFrame, schema: Optional[Dict] = None) -> List[Dict]:
    """ë°ì´í„° íƒ€ì… ê²€ì¦ (í…Œì´ë¸”ì •ì˜ì„œ ê¸°ë°˜)"""
    issues = []
    
    if schema and schema.get("column_types"):
        # í…Œì´ë¸”ì •ì˜ì„œì˜ íƒ€ì… ì •ë³´ë¡œ ê²€ì¦
        for col_name, expected_type in schema["column_types"].items():
            if col_name not in df.columns:
                continue
            
            expected_type_lower = expected_type.lower()
            
            # ìˆ«ì íƒ€ì… ê²€ì¦
            if "int" in expected_type_lower or "number" in expected_type_lower:
                non_numeric = pd.to_numeric(df[col_name], errors='coerce').isna().sum()
                if non_numeric > 0:
                    issues.append({
                        "type": "invalid_data_type",
                        "severity": "error",
                        "message": f"{col_name} ì»¬ëŸ¼ì€ {expected_type} íƒ€ì…ì´ì–´ì•¼ í•˜ì§€ë§Œ ìˆ«ìê°€ ì•„ë‹Œ ê°’ {non_numeric}ê°œ ë°œê²¬"
                    })
            
            # ë‚ ì§œ íƒ€ì… ê²€ì¦
            elif "date" in expected_type_lower or "datetime" in expected_type_lower:
                non_date = pd.to_datetime(df[col_name], errors='coerce').isna().sum()
                if non_date > 0:
                    issues.append({
                        "type": "invalid_data_type",
                        "severity": "warning",
                        "message": f"{col_name} ì»¬ëŸ¼ì€ ë‚ ì§œ íƒ€ì…ì´ì–´ì•¼ í•˜ì§€ë§Œ ë‚ ì§œê°€ ì•„ë‹Œ ê°’ {non_date}ê°œ ë°œê²¬"
                    })
            
            # ë¬¸ìì—´ íƒ€ì… ê²€ì¦ (ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì œ ì—†ì§€ë§Œ í™•ì¸)
            elif "string" in expected_type_lower or "text" in expected_type_lower:
                # ë¬¸ìì—´ íƒ€ì…ì€ íŠ¹ë³„í•œ ê²€ì¦ ë¶ˆí•„ìš”
                pass
    else:
        # í´ë°±: ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
        numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns
        for col in numeric_columns:
            non_numeric = pd.to_numeric(df[col], errors='coerce').isna().sum()
            if non_numeric > 0:
                issues.append({
                    "type": "invalid_data_type",
                    "severity": "warning",
                    "message": f"{col} ì»¬ëŸ¼ì— ìˆ«ìê°€ ì•„ë‹Œ ê°’ {non_numeric}ê°œ ë°œê²¬"
                })
    
    return issues


def check_value_ranges(table_name: str, df: pd.DataFrame, schema: Optional[Dict] = None) -> List[Dict]:
    """ê°’ ë²”ìœ„ ê²€ì¦ (í…Œì´ë¸”ì •ì˜ì„œ ê¸°ë°˜)"""
    issues = []
    
    if schema and schema.get("value_ranges"):
        # í…Œì´ë¸”ì •ì˜ì„œì˜ ë²”ìœ„ ì •ë³´ë¡œ ê²€ì¦
        for col_name, range_info in schema["value_ranges"].items():
            if col_name not in df.columns:
                continue
            
            min_val = range_info.get("min")
            max_val = range_info.get("max")
            
            col_values = pd.to_numeric(df[col_name], errors='coerce')
            valid_values = col_values.notna()
            
            if valid_values.any():
                out_of_range = ((col_values < min_val) | (col_values > max_val)).sum()
                if out_of_range > 0:
                    issues.append({
                        "type": "out_of_range",
                        "severity": "error",
                        "message": f"{col_name} ì»¬ëŸ¼ì— {min_val}~{max_val} ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê°’ {out_of_range}ê°œ ë°œê²¬"
                    })
    else:
        # í´ë°±: ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
        if "ì‹¬ê°ë„" in df.columns:
            severity = pd.to_numeric(df["ì‹¬ê°ë„"], errors='coerce')
            if severity.notna().any():
                out_of_range = ((severity < 0) | (severity > 100)).sum()
                if out_of_range > 0:
                    issues.append({
                        "type": "out_of_range",
                        "severity": "error",
                        "message": f"ì‹¬ê°ë„ ì»¬ëŸ¼ì— 0-100 ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê°’ {out_of_range}ê°œ ë°œê²¬"
                    })
        
        if "ì¬ê³ ìˆ˜ì¤€" in df.columns:
            stock_level = df["ì¬ê³ ìˆ˜ì¤€"].str.replace('%', '').astype(str)
            stock_level_numeric = pd.to_numeric(stock_level, errors='coerce')
            if stock_level_numeric.notna().any():
                out_of_range = ((stock_level_numeric < 0) | (stock_level_numeric > 100)).sum()
                if out_of_range > 0:
                    issues.append({
                        "type": "out_of_range",
                        "severity": "warning",
                        "message": f"ì¬ê³ ìˆ˜ì¤€ ì»¬ëŸ¼ì— 0-100 ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê°’ {out_of_range}ê°œ ë°œê²¬"
                    })
    
    return issues


def check_enum_values(df: pd.DataFrame, schema: Optional[Dict] = None) -> List[Dict]:
    """ì—´ê±°í˜• ê°’ ê²€ì¦ (í…Œì´ë¸”ì •ì˜ì„œ ê¸°ë°˜)"""
    issues = []
    
    if schema and schema.get("enums"):
        for col_name, allowed_values in schema["enums"].items():
            if col_name not in df.columns:
                continue
            
            # NULLì´ ì•„ë‹Œ ê°’ë“¤ ì¤‘ í—ˆìš©ë˜ì§€ ì•Šì€ ê°’ ì°¾ê¸°
            non_null_values = df[col_name].dropna()
            invalid_values = non_null_values[~non_null_values.isin(allowed_values)]
            
            if len(invalid_values) > 0:
                unique_invalid = invalid_values.unique()[:5]  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                issues.append({
                    "type": "invalid_enum_value",
                    "severity": "error",
                    "message": f"{col_name} ì»¬ëŸ¼ì— í—ˆìš©ë˜ì§€ ì•Šì€ ê°’ ë°œê²¬: {', '.join(map(str, unique_invalid))} (í—ˆìš© ê°’: {', '.join(allowed_values)})"
                })
    
    return issues


def check_foreign_key_integrity(table_name: str, df: pd.DataFrame, schema: Optional[Dict], 
                                data_manager, config: Optional[Dict] = None) -> List[Dict]:
    """FK ì°¸ì¡° ë¬´ê²°ì„± ê²€ì¦"""
    issues = []
    
    if not schema or not schema.get("foreign_keys"):
        return issues
    
    try:
        # ëª¨ë“  í…Œì´ë¸” ë¡œë“œ (FK ì°¸ì¡° í™•ì¸ìš©)
        all_tables = data_manager.load_all()
        
        for fk_info in schema["foreign_keys"]:
            fk_column = fk_info["column"]
            target_table = fk_info["target_table"]
            target_column = fk_info["target_column"]
            
            if fk_column not in df.columns:
                continue
            
            # FK ê°’ ì¶”ì¶œ (NULL ì œì™¸)
            fk_values = df[fk_column].dropna().unique()
            
            if len(fk_values) == 0:
                continue
            
            # ì°¸ì¡° ëŒ€ìƒ í…Œì´ë¸” ì°¾ê¸°
            target_df = None
            for t_name, t_df in all_tables.items():
                # í…Œì´ë¸”ëª… ë§¤ì¹­ (ë¶€ë¶„ ì¼ì¹˜ í—ˆìš©)
                if target_table in t_name or t_name in target_table:
                    if target_column in t_df.columns:
                        target_df = t_df
                        break
            
            if target_df is None:
                issues.append({
                    "type": "foreign_key_reference_not_found",
                    "severity": "warning",
                    "message": f"FK '{fk_column}'ì˜ ì°¸ì¡° ëŒ€ìƒ í…Œì´ë¸” '{target_table}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                })
                continue
            
            # ì°¸ì¡° ë¬´ê²°ì„± í™•ì¸
            invalid_fk = fk_values[~pd.Series(fk_values).isin(target_df[target_column].values)]
            
            if len(invalid_fk) > 0:
                unique_invalid = invalid_fk[:5]  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                issues.append({
                    "type": "foreign_key_integrity_violation",
                    "severity": "error",
                    "message": f"FK '{fk_column}'ì— ì°¸ì¡° ëŒ€ìƒ í…Œì´ë¸” '{target_table}.{target_column}'ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê°’ {len(invalid_fk)}ê°œ ë°œê²¬ (ì˜ˆ: {', '.join(map(str, unique_invalid))})"
                })
    
    except Exception as e:
        issues.append({
            "type": "foreign_key_check_error",
            "severity": "warning",
            "message": f"FK ì°¸ì¡° ë¬´ê²°ì„± ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)[:100]}"
        })
    
    return issues


def render_quality_results(results: Dict):
    """í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼ ë Œë”ë§"""
    # ì „ì²´ ìƒíƒœ í‘œì‹œ
    overall_status = results.get("overall_status", "unknown")
    
    if overall_status == "pass":
        st.success("âœ… ëª¨ë“  ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ë¥¼ í†µê³¼í–ˆìŠµë‹ˆë‹¤!")
    elif overall_status == "warning":
        st.warning("âš ï¸ ì¼ë¶€ ë°ì´í„°ì— ê²½ê³ ê°€ ìˆìŠµë‹ˆë‹¤. í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        st.error("âŒ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ì—ì„œ ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ìš”ì•½ ì •ë³´
    summary = results.get("summary", {})
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì „ì²´ í…Œì´ë¸”", summary.get("total_tables", 0))
    with col2:
        st.metric("í†µê³¼", summary.get("passed_tables", 0), 
                 delta=f"{summary.get('passed_tables', 0)}/{summary.get('total_tables', 1)}")
    with col3:
        st.metric("ì‹¤íŒ¨", summary.get("failed_tables", 0))
    with col4:
        st.metric("ë°œê²¬ëœ ë¬¸ì œ", summary.get("total_issues", 0))
    
    st.divider()
    
    # ê° í…Œì´ë¸”ë³„ ìƒì„¸ ê²°ê³¼
    st.subheader("ğŸ“Š í…Œì´ë¸”ë³„ ìƒì„¸ ê²°ê³¼")
    
    checks = results.get("checks", [])
    for check in checks:
        table_name = check.get("table", "Unknown")
        status = check.get("status", "unknown")
        issues = check.get("issues", [])
        
        # ìƒíƒœì— ë”°ë¥¸ ì•„ì´ì½˜
        if status == "pass":
            status_icon = "âœ…"
            status_color = "success"
        elif status == "warning":
            status_icon = "âš ï¸"
            status_color = "warning"
        else:
            status_icon = "âŒ"
            status_color = "error"
        
        with st.expander(f"{status_icon} {table_name} ({check.get('row_count', 0)}í–‰, {check.get('column_count', 0)}ì—´)", 
                        expanded=(status != "pass")):
            if status == "pass":
                st.success("âœ… ëª¨ë“  ê²€ì‚¬ë¥¼ í†µê³¼í–ˆìŠµë‹ˆë‹¤.")
            else:
                # ë¬¸ì œì  í‘œì‹œ
                for issue in issues:
                    severity = issue.get("severity", "info")
                    message = issue.get("message", "N/A")
                    
                    if severity == "error":
                        st.error(f"âŒ {message}")
                    else:
                        st.warning(f"âš ï¸ {message}")
    
    # ê°œì„  ì œì•ˆ
    if summary.get("total_issues", 0) > 0:
        st.divider()
        st.subheader("ğŸ’¡ ê°œì„  ì œì•ˆ")
        
        error_issues = [issue for check in checks for issue in check.get("issues", []) 
                       if issue.get("severity") == "error"]
        warning_issues = [issue for check in checks for issue in check.get("issues", []) 
                         if issue.get("severity") == "warning"]
        
        if error_issues:
            st.error("**ì¦‰ì‹œ ìˆ˜ì • í•„ìš”:**")
            for issue in error_issues[:5]:  # ìƒìœ„ 5ê°œë§Œ
                st.write(f"- {issue.get('message', 'N/A')}")
        
        if warning_issues:
            st.warning("**ê°œì„  ê¶Œì¥:**")
            for issue in warning_issues[:5]:  # ìƒìœ„ 5ê°œë§Œ
                st.write(f"- {issue.get('message', 'N/A')}")


