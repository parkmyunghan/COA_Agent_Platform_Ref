# ui/components/ontology_studio/inference_manager.py
# -*- coding: utf-8 -*-
"""
ì¶”ë¡  ê´€ë¦¬ ì»´í¬ë„ŒíŠ¸
ì¶”ë¡  ì—”ì§„ ê´€ë¦¬ ë° ë¶„ì„
"""
import streamlit as st
import pandas as pd
from ui.components.ontology_dashboard_panel import render_ontology_dashboard_panel

def render_inference_manager(orchestrator):
    """ì¶”ë¡  ê´€ë¦¬ ë Œë”ë§"""
    st.markdown("### ğŸ§  ì¶”ë¡  ë° ë¶„ì„ (Inference & Analysis)")
    
    ontology_manager = orchestrator.core.enhanced_ontology_manager
    if not ontology_manager or not ontology_manager.graph:
        st.warning("âš ï¸ ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì„œë¸Œíƒ­ êµ¬ì„±
    sub_tab1, sub_tab2, sub_tab3 = st.tabs([
        "ì¶”ë¡  ì „/í›„ ë¹„êµ",
        "ì¶”ë¡  ê·œì¹™ ê´€ë¦¬",
        "ì¶”ë¡  ê²°ê³¼ ë¶„ì„"
    ])
    
    with sub_tab1:
        _render_inference_comparison(orchestrator)
    
    with sub_tab2:
        _render_inference_rules(orchestrator)
    
    with sub_tab3:
        _render_inference_results(orchestrator)

def _render_inference_comparison(orchestrator):
    """ì¶”ë¡  ì „/í›„ ë¹„êµ"""
    st.markdown("#### ğŸ”„ ì¶”ë¡  ì „/í›„ ë¹„êµ")
    st.markdown("ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ì—”ì§„ì´ ë„ì¶œí•œ **ìƒˆë¡œìš´ ì§€ì‹(Implicit Knowledge)**ì„ í™•ì¸í•©ë‹ˆë‹¤.")
    
    col1, col2 = st.columns(2)
    with col1:
        st.info("Input Graph (ì‚¬ëŒì´ ì…ë ¥í•œ ë°ì´í„°)")
        st.code("\n".join([
            "# A unit is on high ground",
            ":Unit_A :locatedIn :HighGround .",
            ":HighGround :type :Mountain ."
        ]), language="turtle")
    
    with col2:
        st.success("Reasoned Graph (AIê°€ ì¶”ë¡ í•œ ì‚¬ì‹¤)")
        st.code("\n".join([
            "# AI infers advantage",
            ':Unit_A :hasAdvantage "True" .',
            ':Unit_A :movementSpeed "Slow" .'
        ]), language="turtle")
    
    st.divider()
    st.caption("ì‹¤ì œ ë°ì´í„° ì¶”ë¡  ê²°ê³¼ (Sample)")
    
    # ì‹¤ì œ ì¶”ë¡ ëœ íŠ¸ë¦¬í”Œ ìƒ˜í”Œ ì¡°íšŒ
    query_inferred_sample = "\n".join([
        "SELECT ?s ?p ?o WHERE {",
        "    ?s <http://coa-agent-platform.org/ontology#hasAdvantage> ?o .",
        "} LIMIT 5"
    ])
    
    try:
        ontology_manager = orchestrator.core.enhanced_ontology_manager
        res = ontology_manager.graph.query(query_inferred_sample)
        data = []
        for row in res:
            data.append({"Subject": row.s, "Predicate": "hasAdvantage", "Object": row.o})
        
        if data:
            st.dataframe(pd.DataFrame(data), width="stretch")
        else:
            st.warning("í˜„ì¬ ì¶”ë¡ ëœ 'ì „ìˆ ì  ì´ì (hasAdvantage)' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"ì¶”ë¡  ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")

def _render_inference_rules(orchestrator):
    """ì¶”ë¡  ê·œì¹™ ê´€ë¦¬"""
    st.markdown("#### ğŸ“‹ ì¶”ë¡  ê·œì¹™ ê´€ë¦¬")
    st.info("ğŸ’¡ ì¶”ë¡  ê·œì¹™ì„ ê´€ë¦¬í•˜ê³  í™œì„±í™”/ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    st.info("ì¶”ë¡  ê·œì¹™ ê´€ë¦¬ ê¸°ëŠ¥ì€ êµ¬í˜„ ì˜ˆì •ì…ë‹ˆë‹¤.")

def _render_inference_results(orchestrator):
    """ì¶”ë¡  ê²°ê³¼ ë¶„ì„"""
    st.markdown("#### ğŸ“Š ì¶”ë¡  ê²°ê³¼ ë¶„ì„")
    st.info("ğŸ’¡ ì¶”ë¡ ëœ ê´€ê³„ì˜ í†µê³„ ë° íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    st.info("ì¶”ë¡  ê²°ê³¼ ë¶„ì„ ê¸°ëŠ¥ì€ êµ¬í˜„ ì˜ˆì •ì…ë‹ˆë‹¤.")

