# ui/components/ontology_studio/schema_manager.py
# -*- coding: utf-8 -*-
"""
ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ ì»´í¬ë„ŒíŠ¸
ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ(T-Box) ì •ì˜ ë° ê´€ë¦¬
"""
import streamlit as st
import pandas as pd
from pathlib import Path
import json
from collections import defaultdict
from datetime import datetime
from typing import Dict, List
from rdflib import RDF, RDFS, OWL

def render_schema_manager(orchestrator):
    """ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ ë Œë”ë§"""
    st.markdown("### ğŸ“ ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ (Schema Management)")
    st.info("ğŸ’¡ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ(T-Box)ë¥¼ ì •ì˜í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.")
    
    ontology_manager = orchestrator.core.enhanced_ontology_manager
    if not ontology_manager or not ontology_manager.graph:
        st.warning("âš ï¸ ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì„œë¸Œíƒ­ êµ¬ì„±
    sub_tab1, sub_tab2, sub_tab3, sub_tab4 = st.tabs([
        "í´ë˜ìŠ¤ ì •ì˜",
        "ì†ì„± ì •ì˜",
        "ê´€ê³„ ì •ì˜",
        "ìŠ¤í‚¤ë§ˆ ê²€ì¦"
    ])
    
    with sub_tab1:
        _render_class_definition(orchestrator)
    
    with sub_tab2:
        _render_property_definition(orchestrator)
    
    with sub_tab3:
        _render_relation_definition(orchestrator)
    
    with sub_tab4:
        _render_schema_validation(orchestrator)

def _render_class_definition(orchestrator):
    """í´ë˜ìŠ¤ ì •ì˜ - ê°œì„ ëœ ë²„ì „"""
    st.markdown("#### ğŸ“‹ í´ë˜ìŠ¤ ì •ì˜")
    st.info("ğŸ’¡ ì˜¨í†¨ë¡œì§€ì˜ í´ë˜ìŠ¤(Class)ë¥¼ ì •ì˜í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.")
    
    ontology_manager = orchestrator.core.enhanced_ontology_manager
    graph = ontology_manager.graph
    ns = ontology_manager.ns
    
    # í´ë˜ìŠ¤ ì¶”ì¶œ (OWL.Class í¬í•¨)
    classes = []
    class_details = {}
    
    # OWL.Class ì¶”ì¶œ
    for s, p, o in graph.triples((None, RDF.type, OWL.Class)):
        class_name = str(s).replace(str(ns), "")
        if class_name and class_name not in class_details:
            # ì¸ìŠ¤í„´ìŠ¤ ê°œìˆ˜ í™•ì¸
            instance_count = len(list(graph.triples((None, RDF.type, s))))
            classes.append(class_name)
            class_details[class_name] = {
                "name": class_name,
                "uri": str(s),
                "instance_count": instance_count,
                "type": "OWL.Class"
            }
    
    # RDFS.Classë„ í™•ì¸ (í•˜ìœ„ í˜¸í™˜ì„±)
    for s, p, o in graph.triples((None, RDF.type, RDFS.Class)):
        class_name = str(s).replace(str(ns), "")
        if class_name and class_name not in class_details:
            instance_count = len(list(graph.triples((None, RDF.type, s))))
            classes.append(class_name)
            class_details[class_name] = {
                "name": class_name,
                "uri": str(s),
                "instance_count": instance_count,
                "type": "RDFS.Class"
            }
    
    # Table, Column ë“± ë ˆê±°ì‹œ í´ë˜ìŠ¤ë„ í™•ì¸
    if hasattr(ns, 'Table'):
        for s, p, o in graph.triples((None, RDF.type, ns.Table)):
            class_name = str(s).replace(str(ns), "")
            if class_name and class_name not in class_details:
                instance_count = len(list(graph.triples((None, RDF.type, s))))
                classes.append(class_name)
                class_details[class_name] = {
                    "name": class_name,
                    "uri": str(s),
                    "instance_count": instance_count,
                    "type": "Legacy.Table"
                }
    
    if classes:
        st.success(f"âœ… ë“±ë¡ëœ í´ë˜ìŠ¤: **{len(classes)}ê°œ**")
        
        # ê²€ìƒ‰ ë° í•„í„°ë§
        col1, col2 = st.columns([3, 1])
        with col1:
            search_term = st.text_input("ğŸ” í´ë˜ìŠ¤ ê²€ìƒ‰", placeholder="í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ê²€ìƒ‰...", key="class_search")
        with col2:
            min_instances = st.number_input("ìµœì†Œ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜", min_value=0, value=0, step=1, key="min_instances")
        
        # í•„í„°ë§
        filtered_classes = []
        for cls_name in sorted(classes):
            if search_term and search_term.lower() not in cls_name.lower():
                continue
            if class_details[cls_name]["instance_count"] < min_instances:
                continue
            filtered_classes.append(cls_name)
        
        if filtered_classes:
            # í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
            class_data = []
            for cls_name in filtered_classes:
                details = class_details[cls_name]
                class_data.append({
                    "í´ë˜ìŠ¤ëª…": cls_name,
                    "ì¸ìŠ¤í„´ìŠ¤ ìˆ˜": details["instance_count"],
                    "íƒ€ì…": details["type"]
                })
            
            df = pd.DataFrame(class_data)
            st.dataframe(df, use_container_width=True, hide_index=True)
            
            # í†µê³„ ì •ë³´
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ í´ë˜ìŠ¤ ìˆ˜", len(classes))
            with col2:
                total_instances = sum(d["instance_count"] for d in class_details.values())
                st.metric("ì´ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜", f"{total_instances:,}")
            with col3:
                avg_instances = total_instances / len(classes) if classes else 0
                st.metric("í‰ê·  ì¸ìŠ¤í„´ìŠ¤ ìˆ˜", f"{avg_instances:.1f}")
        else:
            st.info("ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” í´ë˜ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("âš ï¸ ë“±ë¡ëœ í´ë˜ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.info("ğŸ’¡ í´ë˜ìŠ¤ëŠ” ì˜¨í†¨ë¡œì§€ ìƒì„± ê³¼ì •ì—ì„œ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤. ì˜¨í†¨ë¡œì§€ ìƒì„± í˜ì´ì§€ì—ì„œ ì˜¨í†¨ë¡œì§€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")

def _render_property_definition(orchestrator):
    """ì†ì„± ì •ì˜ - ê°œì„ ëœ ë²„ì „"""
    st.markdown("#### ğŸ”§ ì†ì„± ì •ì˜")
    st.info("ğŸ’¡ ì˜¨í†¨ë¡œì§€ì˜ ì†ì„±(Property)ì„ ì •ì˜í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.")
    
    ontology_manager = orchestrator.core.enhanced_ontology_manager
    graph = ontology_manager.graph
    ns = ontology_manager.ns
    
    # ì†ì„± ì¶”ì¶œ ë° í†µê³„ ìˆ˜ì§‘
    properties = set()
    property_stats = defaultdict(int)
    property_details = {}
    
    for s, p, o in graph.triples((None, None, None)):
        if str(p).startswith(str(ns)) and str(p) != str(ns.type):
            prop_name = str(p).replace(str(ns), "")
            properties.add(prop_name)
            property_stats[prop_name] += 1
            
            if prop_name not in property_details:
                property_details[prop_name] = {
                    "name": prop_name,
                    "uri": str(p),
                    "usage_count": 0,
                    "domain_classes": set(),
                    "range_types": set()
                }
            property_details[prop_name]["usage_count"] = property_stats[prop_name]
    
    if properties:
        st.success(f"âœ… ë“±ë¡ëœ ì†ì„±: **{len(properties)}ê°œ**")
        
        # ê²€ìƒ‰ ë° í•„í„°ë§
        col1, col2, col3 = st.columns([2, 1, 1])
        with col1:
            search_term = st.text_input("ğŸ” ì†ì„± ê²€ìƒ‰", placeholder="ì†ì„±ëª…ìœ¼ë¡œ ê²€ìƒ‰...", key="prop_search")
        with col2:
            min_usage = st.number_input("ìµœì†Œ ì‚¬ìš© íšŸìˆ˜", min_value=0, value=0, step=1, key="min_usage")
        with col3:
            sort_by = st.selectbox("ì •ë ¬ ê¸°ì¤€", ["ì´ë¦„", "ì‚¬ìš© íšŸìˆ˜"], index=0, key="prop_sort")
        
        # í•„í„°ë§ ë° ì •ë ¬
        filtered_props = []
        for prop_name in properties:
            if search_term and search_term.lower() not in prop_name.lower():
                continue
            if property_details[prop_name]["usage_count"] < min_usage:
                continue
            filtered_props.append(prop_name)
        
        # ì •ë ¬
        if sort_by == "ì‚¬ìš© íšŸìˆ˜":
            filtered_props.sort(key=lambda x: property_details[x]["usage_count"], reverse=True)
        else:
            filtered_props.sort()
        
        if filtered_props:
            # í˜ì´ì§€ë„¤ì´ì…˜
            items_per_page = 20
            total_pages = (len(filtered_props) + items_per_page - 1) // items_per_page
            page = st.selectbox("í˜ì´ì§€", range(1, total_pages + 1), index=0, key="prop_page") if total_pages > 1 else 1
            
            start_idx = (page - 1) * items_per_page
            end_idx = start_idx + items_per_page
            page_props = filtered_props[start_idx:end_idx]
            
            # í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
            prop_data = []
            for prop_name in page_props:
                details = property_details[prop_name]
                prop_data.append({
                    "ì†ì„±ëª…": prop_name,
                    "ì‚¬ìš© íšŸìˆ˜": details["usage_count"],
                    "URI": details["uri"][:50] + "..." if len(details["uri"]) > 50 else details["uri"]
                })
            
            df = pd.DataFrame(prop_data)
            st.dataframe(df, use_container_width=True, hide_index=True)
            
            if total_pages > 1:
                st.caption(f"í˜ì´ì§€ {page}/{total_pages} (ì´ {len(filtered_props)}ê°œ ì†ì„±)")
            
            # í†µê³„ ì •ë³´
            st.divider()
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ì´ ì†ì„± ìˆ˜", len(properties))
            with col2:
                total_usage = sum(d["usage_count"] for d in property_details.values())
                st.metric("ì´ ì‚¬ìš© íšŸìˆ˜", f"{total_usage:,}")
            with col3:
                avg_usage = total_usage / len(properties) if properties else 0
                st.metric("í‰ê·  ì‚¬ìš© íšŸìˆ˜", f"{avg_usage:.1f}")
            with col4:
                max_usage_prop = max(property_details.items(), key=lambda x: x[1]["usage_count"])
                st.metric("ê°€ì¥ ë§ì´ ì‚¬ìš©", max_usage_prop[0][:20])
        else:
            st.info("ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("âš ï¸ ë“±ë¡ëœ ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")

def _render_relation_definition(orchestrator):
    """ê´€ê³„ ì •ì˜ - ê°œì„ ëœ ë²„ì „ (ëˆ„ë½ëœ í…Œì´ë¸” ê²½ê³  ì¶”ê°€)"""
    st.markdown("#### ğŸ”— ê´€ê³„ ì •ì˜")
    st.info("ğŸ’¡ ì˜¨í†¨ë¡œì§€ì˜ ê´€ê³„(Relation) ê·œì¹™ì„ ì •ì˜í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.")
    
    # relation_mappings.json ë¡œë“œ
    base_dir = Path(__file__).parent.parent.parent.parent
    relation_mapping_path = base_dir / "metadata" / "relation_mappings.json"
    
    if not relation_mapping_path.exists():
        st.error("ê´€ê³„ ë§¤í•‘ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    with open(relation_mapping_path, 'r', encoding='utf-8') as f:
        relation_mappings = json.load(f)
    
    # schema_registry.yamlê³¼ ë¹„êµí•˜ì—¬ ëˆ„ë½ëœ í…Œì´ë¸” í™•ì¸
    ontology_manager = orchestrator.core.enhanced_ontology_manager
    schema_registry = ontology_manager.schema_registry if hasattr(ontology_manager, 'schema_registry') else {}
    
    # data_lakeì˜ ì‹¤ì œ íŒŒì¼ í™•ì¸
    data_lake_path = base_dir / "data_lake"
    actual_tables = set()
    if data_lake_path.exists():
        for file in data_lake_path.glob("*.xlsx"):
            table_name = file.stem
            actual_tables.add(table_name)
    
    # relation_mappings.jsonì— ìˆëŠ” í…Œì´ë¸” (ì¤‘ë³µ ì œê±°)
    mapped_tables = set(relation_mappings.keys())
    
    # schema_registryì— ìˆëŠ” í…Œì´ë¸”
    registry_tables = set(schema_registry.keys()) if schema_registry else set()
    
    # ëˆ„ë½ëœ í…Œì´ë¸” í™•ì¸
    missing_from_mapping = registry_tables - mapped_tables
    missing_from_registry = mapped_tables - registry_tables
    
    # í†µê³„ ê³„ì‚°
    total_tables = len(relation_mappings)
    total_rules = sum(len(rules) for rules in relation_mappings.values())
    
    # ê´€ê³„ ìœ í˜•ë³„ ë¶„ë¥˜
    rule_types = {
        "ì¼ë°˜ ê´€ê³„": 0,
        "ë™ì  FK": 0,
        "ì¶”ë¡  ê´€ê³„": 0
    }
    
    relation_data = []
    for table_name, rules in relation_mappings.items():
        if not rules:  # ë¹ˆ ê·œì¹™ì€ ìŠ¤í‚µ
            continue
        for col_name, rule_config in rules.items():
            rule_type = "ì¼ë°˜ ê´€ê³„"
            if isinstance(rule_config, dict):
                if rule_config.get("dynamic"):
                    rule_type = "ë™ì  FK"
                elif col_name.startswith("ì¶”ë¡ :"):
                    rule_type = "ì¶”ë¡  ê´€ê³„"
            
            rule_types[rule_type] += 1
            
            target = rule_config.get("target", "") if isinstance(rule_config, dict) else str(rule_config)
            relation = rule_config.get("relation", "") if isinstance(rule_config, dict) else ""
            
            relation_data.append({
                "ì†ŒìŠ¤ í…Œì´ë¸”": table_name,
                "ì†ŒìŠ¤ ì»¬ëŸ¼": col_name.replace("ì¶”ë¡ :", "").replace("ë™ì FK:", ""),
                "ê´€ê³„ ìœ í˜•": rule_type,
                "íƒ€ê²Ÿ": target,
                "ê´€ê³„ëª…": relation,
                "ì„¤ì •": json.dumps(rule_config, ensure_ascii=False) if isinstance(rule_config, dict) else str(rule_config)
            })
    
    # ê²½ê³  í‘œì‹œ
    if missing_from_mapping:
        st.warning(f"âš ï¸ **{len(missing_from_mapping)}ê°œ í…Œì´ë¸”ì´ ê´€ê³„ ë§¤í•‘ì— ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤:** {', '.join(sorted(missing_from_mapping))}")
        st.info("ğŸ’¡ `schema_registry.yaml`ì—ëŠ” ì •ì˜ë˜ì–´ ìˆì§€ë§Œ `relation_mappings.json`ì— ê´€ê³„ ê·œì¹™ì´ ì—†ìŠµë‹ˆë‹¤. ì˜¨í†¨ë¡œì§€ ìƒì„± ì‹œ í•´ë‹¹ í…Œì´ë¸”ì˜ ê´€ê³„ê°€ ìƒì„±ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    if missing_from_registry:
        st.info(f"â„¹ï¸ **{len(missing_from_registry)}ê°œ í…Œì´ë¸”ì´ schema_registryì— ì—†ìŠµë‹ˆë‹¤:** {', '.join(sorted(missing_from_registry))}")
    
    st.success(f"âœ… ë“±ë¡ëœ ê´€ê³„ ê·œì¹™: **{len(mapped_tables)}ê°œ í…Œì´ë¸”**, **{total_rules}ê°œ ê·œì¹™**")
    
    # ë™ê¸°í™” ìƒíƒœ í‘œì‹œ
    if schema_registry:
        sync_status = len(mapped_tables) / len(registry_tables) * 100 if registry_tables else 0
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("í…Œì´ë¸” ìˆ˜", len(mapped_tables))
        with col2:
            st.metric("ì´ ê·œì¹™ ìˆ˜", total_rules)
        with col3:
            st.metric("ì¼ë°˜ ê´€ê³„", rule_types["ì¼ë°˜ ê´€ê³„"])
        with col4:
            st.metric("ë™ì /ì¶”ë¡ ", rule_types["ë™ì  FK"] + rule_types["ì¶”ë¡  ê´€ê³„"])
        with col5:
            delta_color = "normal" if sync_status >= 90 else "inverse"
            st.metric("ë™ê¸°í™”ìœ¨", f"{sync_status:.1f}%", 
                     delta="ì™„ì „" if sync_status >= 90 else "ë¶€ì¡±",
                     delta_color=delta_color)
    else:
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("í…Œì´ë¸” ìˆ˜", len(mapped_tables))
        with col2:
            st.metric("ì´ ê·œì¹™ ìˆ˜", total_rules)
        with col3:
            st.metric("ì¼ë°˜ ê´€ê³„", rule_types["ì¼ë°˜ ê´€ê³„"])
        with col4:
            st.metric("ë™ì /ì¶”ë¡ ", rule_types["ë™ì  FK"] + rule_types["ì¶”ë¡  ê´€ê³„"])
    
    st.divider()
    
    # í•„í„°ë§ ì˜µì…˜
    col1, col2 = st.columns([2, 1])
    with col1:
        search_term = st.text_input("ğŸ” ê²€ìƒ‰", placeholder="í…Œì´ë¸”ëª…, ì»¬ëŸ¼ëª…, ê´€ê³„ëª…ìœ¼ë¡œ ê²€ìƒ‰...", key="rel_search")
    with col2:
        filter_type = st.selectbox("ê´€ê³„ ìœ í˜• í•„í„°", ["ì „ì²´", "ì¼ë°˜ ê´€ê³„", "ë™ì  FK", "ì¶”ë¡  ê´€ê³„"], index=0, key="rel_filter")
    
    # í•„í„°ë§
    filtered_data = relation_data
    if search_term:
        filtered_data = [
            r for r in filtered_data
            if search_term.lower() in r["ì†ŒìŠ¤ í…Œì´ë¸”"].lower() or
               search_term.lower() in r["ì†ŒìŠ¤ ì»¬ëŸ¼"].lower() or
               search_term.lower() in r["ê´€ê³„ëª…"].lower()
        ]
    if filter_type != "ì „ì²´":
        filtered_data = [r for r in filtered_data if r["ê´€ê³„ ìœ í˜•"] == filter_type]
    
    if filtered_data:
        # í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ í‘œì‹œ (ì„¤ì • ì»¬ëŸ¼ ì œì™¸)
        display_data = [
            {
                "ì†ŒìŠ¤ í…Œì´ë¸”": r["ì†ŒìŠ¤ í…Œì´ë¸”"],
                "ì†ŒìŠ¤ ì»¬ëŸ¼": r["ì†ŒìŠ¤ ì»¬ëŸ¼"],
                "ê´€ê³„ ìœ í˜•": r["ê´€ê³„ ìœ í˜•"],
                "íƒ€ê²Ÿ": r["íƒ€ê²Ÿ"],
                "ê´€ê³„ëª…": r["ê´€ê³„ëª…"]
            }
            for r in filtered_data
        ]
        
        df = pd.DataFrame(display_data)
        st.dataframe(df, use_container_width=True, hide_index=True)
        
        # ìƒì„¸ ì •ë³´ëŠ” expanderë¡œ í‘œì‹œ
        with st.expander("ğŸ“‹ ìƒì„¸ ì„¤ì • ë³´ê¸° (JSON)", expanded=False):
            if len(filtered_data) > 0:
                selected_idx = st.selectbox("ê·œì¹™ ì„ íƒ", range(len(filtered_data)), 
                                           format_func=lambda x: f"{filtered_data[x]['ì†ŒìŠ¤ í…Œì´ë¸”']}.{filtered_data[x]['ì†ŒìŠ¤ ì»¬ëŸ¼']}",
                                           key="rel_detail_select")
                try:
                    st.json(json.loads(filtered_data[selected_idx]["ì„¤ì •"]))
                except:
                    st.code(filtered_data[selected_idx]["ì„¤ì •"])
    else:
        st.info("ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ê´€ê³„ ê·œì¹™ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ëˆ„ë½ëœ í…Œì´ë¸” ìƒì„¸ ì •ë³´
    if missing_from_mapping:
        st.divider()
        st.markdown("#### âš ï¸ ëˆ„ë½ëœ í…Œì´ë¸” ìƒì„¸ ì •ë³´")
        missing_data = []
        for table_name in sorted(missing_from_mapping):
            table_info = schema_registry.get(table_name, {})
            relations = table_info.get('relations', [])
            missing_data.append({
                "í…Œì´ë¸”ëª…": table_name,
                "ì„¤ëª…": table_info.get('description', ''),
                "ì •ì˜ëœ ê´€ê³„ ìˆ˜": len(relations),
                "íŒŒì¼ëª…": table_info.get('file_name', '')
            })
        
        if missing_data:
            df_missing = pd.DataFrame(missing_data)
            st.dataframe(df_missing, use_container_width=True, hide_index=True)
            st.info("ğŸ’¡ ì´ í…Œì´ë¸”ë“¤ì€ `schema_registry.yaml`ì— ê´€ê³„ê°€ ì •ì˜ë˜ì–´ ìˆì§€ë§Œ `relation_mappings.json`ì— ë°˜ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def _render_schema_validation(orchestrator):
    """ìŠ¤í‚¤ë§ˆ ê²€ì¦ - ê°œì„ ëœ ë²„ì „ (ê°œì„ ë°©ì•ˆ ì œì‹œ ì¶”ê°€)"""
    st.markdown("#### âœ… ìŠ¤í‚¤ë§ˆ ê²€ì¦")
    st.info("ğŸ’¡ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆì˜ ì¼ê´€ì„± ë° ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.")
    
    # ê²€ì¦ ì‹¤í–‰ ë²„íŠ¼
    if st.button("ğŸš€ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤í–‰", type="primary", key="schema_validate_btn"):
        st.session_state.schema_validation_running = True
        st.rerun()
    
    # ê²€ì¦ ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥
    if st.session_state.get('schema_validation_running', False):
        st.session_state.schema_validation_running = False  # í”Œë˜ê·¸ ì´ˆê¸°í™”
        
        with st.spinner("ê²€ì¦ ì¤‘..."):
            from core_pipeline.ontology_validator import OntologyValidator
            validator = OntologyValidator(orchestrator.core.enhanced_ontology_manager)
            report = validator.validate_schema_compliance()
            
            st.session_state.schema_validation_report = report
            
            # ê¶Œì¥ì‚¬í•­ ì¶”ì¶œ ë° ì €ì¥
            recommendations = _extract_recommendations(report)
            if recommendations:
                st.session_state.validation_recommendations = recommendations
                st.session_state.validation_recommendations_timestamp = datetime.now()
                # navigate_to_tabì€ ì œê±° (ìë™ íƒ­ ì´ë™ ë°©ì§€)
            else:
                # ê¶Œì¥ì‚¬í•­ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ê¶Œì¥ì‚¬í•­ ì œê±°
                if 'validation_recommendations' in st.session_state:
                    del st.session_state.validation_recommendations
                if 'validation_recommendations_timestamp' in st.session_state:
                    del st.session_state.validation_recommendations_timestamp
        
        st.rerun()  # ê²°ê³¼ í‘œì‹œë¥¼ ìœ„í•œ ì¬ë Œë”ë§
    
    # ê²€ì¦ ê²°ê³¼ í‘œì‹œ
    if 'schema_validation_report' in st.session_state:
        report = st.session_state.schema_validation_report
        
        # ê²€ì¦ ê²°ê³¼ í‘œì‹œ
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            score = report['overall_score']
            delta_color = "normal" if score >= 80 else "inverse"
            st.metric("ì¢…í•© ì ìˆ˜", f"{score}%", 
                     delta="í†µê³¼" if score >= 80 else "ë¯¸í†µê³¼",
                     delta_color=delta_color)
        
        # ìƒì„¸ ê²€ì¦ ê²°ê³¼
        st.divider()
        
        # Axis ê²€ì¦
        axis_res = report.get('axis_compliance', {})
        with st.expander("1. ì „ì¥ì¶•ì„ (Axis) ê°ì²´í™” ê²€ì¦", expanded=True):
            for check in axis_res.get('checks', []):
                status_icon = "ğŸŸ¢" if check.get('status') == 'PASS' else "ğŸ”´"
                st.markdown(f"**{status_icon} {check.get('name', '')}**: {check.get('message', '')}")
            
            # ê°œì„ ë°©ì•ˆ ì œì‹œ
            if axis_res.get('score', 0) < 100:
                st.warning("âš ï¸ ê°œì„  í•„ìš”")
                st.markdown("**ê°œì„  ë°©ì•ˆ:**")
                st.markdown("""
                - ì „ì¥ì¶•ì„ ì´ ê°ì²´ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°:
                  1. ì˜¨í†¨ë¡œì§€ ìƒì„± í˜ì´ì§€ì—ì„œ ì „ì¥ì¶•ì„  ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”
                  2. `schema_registry.yaml`ì—ì„œ ì „ì¥ì¶•ì„  í…Œì´ë¸”ì˜ ê´€ê³„ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”
                  3. ì˜¨í†¨ë¡œì§€ë¥¼ ì¬ìƒì„±í•˜ì—¬ ì „ì¥ì¶•ì„ ì´ ê°ì²´ë¡œ ë³€í™˜ë˜ë„ë¡ í•˜ì„¸ìš”
                - ì¶•ì„ -ì§€í˜• ì—°ê²°ì´ ì—†ëŠ” ê²½ìš°:
                  1. `relation_mappings.json`ì—ì„œ `ì „ì¥ì¶•ì„ ` í…Œì´ë¸”ì˜ ê´€ê³„ ê·œì¹™ì„ í™•ì¸í•˜ì„¸ìš”
                  2. ì§€í˜•ì…€ê³¼ì˜ ì—°ê²° ê´€ê³„(`locatedIn` ë“±)ê°€ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
                """)
        
        # ì—°ê²°ì„± ê²€ì¦
        conn_res = report.get('connectivity_health', {})
        with st.expander("2. ë°ì´í„° ì—°ê²°ì„± ê²€ì¦", expanded=True):
            for check in conn_res.get('checks', []):
                status_icon = "ğŸŸ¢" if check.get('status') == 'PASS' else "ğŸŸ¡"
                st.markdown(f"**{status_icon} {check.get('name', '')}**: {check.get('message', '')}")
            
            # ê°œì„ ë°©ì•ˆ ì œì‹œ
            failed_checks = [c for c in conn_res.get('checks', []) if c.get('status') != 'PASS']
            if failed_checks:
                st.warning("âš ï¸ ê°œì„  í•„ìš”")
                st.markdown("**ê°œì„  ë°©ì•ˆ:**")
                for check in failed_checks:
                    check_name = check.get('name', '')
                    if "ê³ ë¦½ëœ ë…¸ë“œ" in check_name:
                        st.markdown(f"""
                        - **{check_name}**:
                          1. ê´€ê³„ ê´€ë¦¬ íƒ­ì—ì„œ ê³ ë¦½ëœ ë…¸ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”
                          2. í•´ë‹¹ ë…¸ë“œì— ëŒ€í•œ ê´€ê³„ ê·œì¹™ì´ `relation_mappings.json`ì— ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
                          3. í•„ìš”ì‹œ ê´€ê³„ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€í•˜ì„¸ìš”
                        """)
                    elif "ìˆœí™˜ ì°¸ì¡°" in check_name:
                        st.markdown(f"""
                        - **{check_name}**:
                          1. ê´€ê³„ ê´€ë¦¬ íƒ­ì—ì„œ ìˆœí™˜ ì°¸ì¡° ê´€ê³„ë¥¼ í™•ì¸í•˜ì„¸ìš”
                          2. ìˆœí™˜ ì°¸ì¡°ê°€ ì˜ë„ëœ ê²ƒì¸ì§€ ê²€í† í•˜ì„¸ìš”
                          3. ì˜ë„ë˜ì§€ ì•Šì€ ê²½ìš° ê´€ê³„ ê·œì¹™ì„ ìˆ˜ì •í•˜ì„¸ìš”
                        """)
        
        # ì¶”ë¡  ì—”ì§„ ìƒíƒœ (ìˆëŠ” ê²½ìš°)
        if 'reasoning_status' in report:
            reason_res = report.get('reasoning_status', {})
            with st.expander("3. ì¶”ë¡  ì—”ì§„ ìƒíƒœ", expanded=False):
                for check in reason_res.get('checks', []):
                    status_icon = "ğŸŸ¢" if check.get('status') == 'PASS' else "âšª"
                    st.markdown(f"**{status_icon} {check.get('name', '')}**: {check.get('message', '')}")
        
        # ì¢…í•© ê°œì„ ë°©ì•ˆ
        st.divider()
        if report['overall_score'] >= 80:
            st.success("âœ… ìŠ¤í‚¤ë§ˆ ê²€ì¦ í†µê³¼!")
            # í†µê³¼ ì‹œ ê¶Œì¥ì‚¬í•­ ì œê±°
            if 'validation_recommendations' in st.session_state:
                del st.session_state.validation_recommendations
        else:
            st.warning(f"âš ï¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì ìˆ˜: {report['overall_score']}% (80% ì´ìƒ ê¶Œì¥)")
            
            st.markdown("#### ğŸ’¡ ì¢…í•© ê°œì„  ê¶Œì¥ì‚¬í•­")
            
            # ê¶Œì¥ì‚¬í•­ì´ ì´ë¯¸ ì¶”ì¶œë˜ì–´ ìˆìœ¼ë©´ í‘œì‹œ
            if 'validation_recommendations' in st.session_state:
                recommendations = st.session_state.validation_recommendations
                
                # ê°„ë‹¨í•œ ìš”ì•½ í…Œì´ë¸”
                summary_data = []
                for rec in recommendations:
                    summary_data.append({
                        "ìš°ì„ ìˆœìœ„": rec.get('ìš°ì„ ìˆœìœ„', ''),
                        "í•­ëª©": rec.get('í•­ëª©', ''),
                        "ì¡°ì¹˜": rec.get('ì¡°ì¹˜', ''),
                        "ëŒ€ìƒ": rec.get('ëŒ€ìƒ', '')
                    })
                
                if summary_data:
                    df_rec = pd.DataFrame(summary_data)
                    st.dataframe(df_rec, use_container_width=True, hide_index=True)
                    
                    # ê´€ê³„ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ì´ë™ ì•ˆë‚´
                    st.info("ğŸ’¡ **ê°œì„  ê¶Œì¥ì‚¬í•­ì„ ì¡°ì¹˜í•˜ë ¤ë©´:** ìœ„ì˜ ê¶Œì¥ì‚¬í•­ì„ í™•ì¸í•˜ê³ , í•„ìš”ì‹œ **ê´€ê³„ ê´€ë¦¬** íƒ­ìœ¼ë¡œ ì´ë™í•˜ì—¬ ê´€ê³„ë¥¼ ì¶”ê°€/ìˆ˜ì •í•˜ì„¸ìš”.")
                    
                    # ê´€ê³„ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ì´ë™ ë²„íŠ¼ (ì•ˆë‚´ìš©)
                    col1, col2 = st.columns([1, 4])
                    with col1:
                        if st.button("ğŸ”— ê´€ê³„ ê´€ë¦¬ íƒ­ìœ¼ë¡œ ì´ë™", type="primary", use_container_width=True, key="nav_to_rel_mgmt_from_validation"):
                            st.session_state.navigate_to_tab = "ê´€ê³„ ê´€ë¦¬"
                            if recommendations:
                                st.session_state.navigate_to_subtab = recommendations[0].get('ê´€ë ¨_ì„œë¸Œíƒ­', 'ê´€ê³„ ì¡°íšŒ')
                            st.info("ğŸ‘‰ ìƒë‹¨ì˜ **ê´€ê³„ ê´€ë¦¬** íƒ­ì„ í´ë¦­í•˜ì„¸ìš”. ê¶Œì¥ì‚¬í•­ì´ ìë™ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
                    with col2:
                        st.caption("ğŸ’¡ ê¶Œì¥ì‚¬í•­ì€ ê´€ê³„ ê´€ë¦¬ íƒ­ ìƒë‹¨ì— ë°°ë„ˆë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
            else:
                st.info("ğŸ’¡ ìƒì„¸í•œ ê´€ê³„ ê´€ë¦¬ëŠ” **ê´€ê³„ ê´€ë¦¬** íƒ­ì—ì„œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

def _extract_recommendations(report: Dict) -> List[Dict]:
    """ê²€ì¦ ê²°ê³¼ì—ì„œ ê¶Œì¥ì‚¬í•­ ì¶”ì¶œ"""
    recommendations = []
    
    # Axis ê²€ì¦ ê¶Œì¥ì‚¬í•­
    axis_res = report.get('axis_compliance', {})
    if axis_res.get('score', 0) < 100:
        failed_checks = [c for c in axis_res.get('checks', []) if c.get('status') != 'PASS']
        for check in failed_checks:
            check_name = check.get('name', '')
            check_message = check.get('message', '')
            
            if "ì¶•ì„ -ì§€í˜• ì—°ê²°ì„±" in check_name:
                recommendations.append({
                    "id": "axis_terrain_connectivity",
                    "ìš°ì„ ìˆœìœ„": "ë†’ìŒ",
                    "í•­ëª©": "ì¶•ì„ -ì§€í˜• ì—°ê²°ì„±",
                    "ë¬¸ì œ": check_message,
                    "ì¡°ì¹˜": "ê´€ê³„ ê·œì¹™ í™•ì¸ ë° ì¶”ê°€",
                    "ëŒ€ìƒ": "ì „ì¥ì¶•ì„ ",
                    "ê´€ë ¨_íƒ­": "ê´€ê³„ ê´€ë¦¬",
                    "ê´€ë ¨_ì„œë¸Œíƒ­": "ê´€ê³„ ìƒì„± ê·œì¹™",
                    "ìƒì„¸_ì¡°ì¹˜": [
                        "1. ê´€ê³„ ê´€ë¦¬ íƒ­ì˜ 'ê´€ê³„ ìƒì„± ê·œì¹™' ì„œë¸Œíƒ­ìœ¼ë¡œ ì´ë™",
                        "2. 'ì „ì¥ì¶•ì„ ' í…Œì´ë¸” ì„ íƒ",
                        "3. ì‹œì‘ì§€í˜•ì…€ID, ì¢…ë‹¨ì§€í˜•ì…€ID ê´€ê³„ ê·œì¹™ í™•ì¸",
                        "4. ê´€ê³„ ê·œì¹™ì´ ì—†ìœ¼ë©´ ì¶”ê°€ (ì§€í˜•ì…€ íƒ€ê²Ÿ, hasì§€í˜•ì…€ ê´€ê³„)",
                        "5. ì˜¨í†¨ë¡œì§€ ì¬ìƒì„±í•˜ì—¬ ê´€ê³„ ì ìš©"
                    ],
                    "ê´€ë ¨_í…Œì´ë¸”": ["ì „ì¥ì¶•ì„ ", "ì§€í˜•ì…€"],
                    "ê´€ë ¨_ê´€ê³„": ["hasì§€í˜•ì…€"]
                })
            elif "ì „ì¥ì¶•ì„  ê°ì²´í™”" in check_name:
                recommendations.append({
                    "id": "axis_objectification",
                    "ìš°ì„ ìˆœìœ„": "ë†’ìŒ",
                    "í•­ëª©": "ì „ì¥ì¶•ì„  ê°ì²´í™”",
                    "ë¬¸ì œ": check_message,
                    "ì¡°ì¹˜": "ì˜¨í†¨ë¡œì§€ ì¬ìƒì„±",
                    "ëŒ€ìƒ": "ì „ì¥ì¶•ì„ ",
                    "ê´€ë ¨_íƒ­": "ì˜¨í†¨ë¡œì§€ ìƒì„±",
                    "ê´€ë ¨_ì„œë¸Œíƒ­": None,
                    "ìƒì„¸_ì¡°ì¹˜": [
                        "1. ì˜¨í†¨ë¡œì§€ ìƒì„± í˜ì´ì§€ë¡œ ì´ë™",
                        "2. ì „ì¥ì¶•ì„  ë°ì´í„° í™•ì¸",
                        "3. ì˜¨í†¨ë¡œì§€ ì¬ìƒì„± ì‹¤í–‰"
                    ]
                })
    
    # ì—°ê²°ì„± ê²€ì¦ ê¶Œì¥ì‚¬í•­
    conn_res = report.get('connectivity_health', {})
    failed_conn = [c for c in conn_res.get('checks', []) if c.get('status') != 'PASS']
    for check in failed_conn:
        check_name = check.get('name', '')
        check_message = check.get('message', '')
        
        if "ê³ ë¦½ëœ ë…¸ë“œ" in check_name:
            recommendations.append({
                "id": "orphan_nodes",
                "ìš°ì„ ìˆœìœ„": "ì¤‘ê°„",
                "í•­ëª©": "ê³ ë¦½ëœ ë…¸ë“œ",
                "ë¬¸ì œ": check_message,
                "ì¡°ì¹˜": "ê³ ë¦½ëœ ë…¸ë“œ í™•ì¸ ë° ê´€ê³„ ì¶”ê°€",
                "ëŒ€ìƒ": "ì „ì²´",
                "ê´€ë ¨_íƒ­": "ê´€ê³„ ê´€ë¦¬",
                "ê´€ë ¨_ì„œë¸Œíƒ­": "ê´€ê³„ ì¡°íšŒ",
                "ìƒì„¸_ì¡°ì¹˜": [
                    "1. ê´€ê³„ ê´€ë¦¬ íƒ­ì˜ 'ê´€ê³„ ì¡°íšŒ' ì„œë¸Œíƒ­ìœ¼ë¡œ ì´ë™",
                    "2. ê³ ë¦½ëœ ë…¸ë“œ ê²€ìƒ‰ ë° í™•ì¸",
                    "3. í•„ìš”í•œ ê´€ê³„ë¥¼ 'ê´€ê³„ í¸ì§‘' ì„œë¸Œíƒ­ì—ì„œ ì¶”ê°€",
                    "4. ë˜ëŠ” 'ê´€ê³„ ìƒì„± ê·œì¹™' ì„œë¸Œíƒ­ì—ì„œ ê´€ê³„ ê·œì¹™ ì¶”ê°€"
                ]
            })
        elif "ìˆœí™˜ ì°¸ì¡°" in check_name:
            recommendations.append({
                "id": "circular_reference",
                "ìš°ì„ ìˆœìœ„": "ì¤‘ê°„",
                "í•­ëª©": "ìˆœí™˜ ì°¸ì¡°",
                "ë¬¸ì œ": check_message,
                "ì¡°ì¹˜": "ìˆœí™˜ ì°¸ì¡° ê´€ê³„ í™•ì¸ ë° ìˆ˜ì •",
                "ëŒ€ìƒ": "ì „ì²´",
                "ê´€ë ¨_íƒ­": "ê´€ê³„ ê´€ë¦¬",
                "ê´€ë ¨_ì„œë¸Œíƒ­": "ê´€ê³„ ì¡°íšŒ",
                "ìƒì„¸_ì¡°ì¹˜": [
                    "1. ê´€ê³„ ê´€ë¦¬ íƒ­ì˜ 'ê´€ê³„ ì¡°íšŒ' ì„œë¸Œíƒ­ìœ¼ë¡œ ì´ë™",
                    "2. ìˆœí™˜ ì°¸ì¡° ê´€ê³„ ê²€ìƒ‰ ë° í™•ì¸",
                    "3. ì˜ë„ëœ ìˆœí™˜ ì°¸ì¡°ì¸ì§€ ê²€í† ",
                    "4. ì˜ë„ë˜ì§€ ì•Šì€ ê²½ìš° 'ê´€ê³„ í¸ì§‘' ì„œë¸Œíƒ­ì—ì„œ ìˆ˜ì •"
                ]
            })
    
    return recommendations
