# ui/components/graph_analysis_panel.py
# -*- coding: utf-8 -*-
"""
ê·¸ë˜í”„ ë¶„ì„ ë° í•´ì„ íŒ¨ë„
ì§€ì‹ê·¸ë˜í”„ì˜ êµ¬ì¡°, í’ˆì§ˆ, ê°œì„ ì ì„ ë¶„ì„í•˜ê³  ì œì•ˆ
"""
import streamlit as st
from typing import Dict, List, Optional, Tuple, Set
from collections import defaultdict, Counter, deque
import pandas as pd
import math

try:
    from rdflib import Graph, URIRef, RDF, RDFS, OWL, Namespace
    RDFLIB_AVAILABLE = True
except ImportError:
    RDFLIB_AVAILABLE = False


def render_graph_analysis(core, graph_data: Dict, graph_mode: str):
    """
    ê·¸ë˜í”„ ë¶„ì„ ë° í•´ì„ íŒ¨ë„ ë Œë”ë§
    
    Args:
        core: CorePipeline ì¸ìŠ¤í„´ìŠ¤
        graph_data: ê·¸ë˜í”„ ë°ì´í„° {"instances": {...}, "schema": {...}}
        graph_mode: "ì¸ìŠ¤í„´ìŠ¤ ê·¸ë˜í”„ (ABox)" ë˜ëŠ” "ìŠ¤í‚¤ë§ˆ ê·¸ë˜í”„ (TBox)"
    """
    st.markdown("---")
    st.markdown("### ğŸ“Š ê·¸ë˜í”„ ë¶„ì„ ë° í•´ì„")
    
    use_instances = graph_mode == "ì¸ìŠ¤í„´ìŠ¤ ê·¸ë˜í”„ (ABox)"
    data = graph_data["instances"] if use_instances else graph_data["schema"]
    
    # íƒ­ìœ¼ë¡œ ë¶„ì„ ê¸°ëŠ¥ ë¶„ë¦¬
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ“ˆ êµ¬ì¡° ë¶„ì„",
        "ğŸ” ëˆ„ë½ ê´€ê³„ íƒì§€",
        "âœ… í’ˆì§ˆ í‰ê°€",
        "ğŸ’¡ ê°œì„  ì œì•ˆ"
    ])
    
    with tab1:
        _render_structure_analysis(data, use_instances)
    
    with tab2:
        if use_instances:
            _render_missing_relations_analysis(core, graph_data)
        else:
            st.info("ìŠ¤í‚¤ë§ˆ ê·¸ë˜í”„ì—ì„œëŠ” ëˆ„ë½ ê´€ê³„ íƒì§€ê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    with tab3:
        _render_quality_assessment(core, graph_data, use_instances)
    
    with tab4:
        _render_improvement_suggestions(core, graph_data, use_instances)


def _render_structure_analysis(data: Dict, is_instance: bool):
    """ê·¸ë˜í”„ êµ¬ì¡° ë¶„ì„"""
    st.markdown("#### ğŸ“ˆ ê·¸ë˜í”„ êµ¬ì¡° ë¶„ì„")
    
    nodes = data.get("nodes", [])
    links = data.get("links", [])
    
    if not nodes:
        st.warning("ë¶„ì„í•  ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ë³¸ í†µê³„
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ì´ ë…¸ë“œ ìˆ˜", len(nodes))
    with col2:
        st.metric("ì´ ê´€ê³„ ìˆ˜", len(links))
    with col3:
        # í‰ê·  ì—°ê²°ë„ ê³„ì‚°
        node_degree = defaultdict(int)
        for link in links:
            node_degree[link.get("source")] += 1
            node_degree[link.get("target")] += 1
        avg_degree = sum(node_degree.values()) / len(nodes) if nodes else 0
        st.metric("í‰ê·  ì—°ê²°ë„", f"{avg_degree:.2f}")
    with col4:
        # ê·¸ë£¹ ìˆ˜
        groups = set(n.get("group", "") for n in nodes)
        st.metric("ê·¸ë£¹ ìˆ˜", len(groups))
    
    st.divider()
    
    # ë…¸ë“œë³„ ì—°ê²°ë„ ë¶„ì„
    st.markdown("##### ğŸ”— ë…¸ë“œ ì—°ê²°ë„ ë¶„ì„")
    node_degree = defaultdict(int)
    for link in links:
        node_degree[link.get("source")] += 1
        node_degree[link.get("target")] += 1
    
    # ì—°ê²°ë„ê°€ ë†’ì€ ë…¸ë“œ (í—ˆë¸Œ ë…¸ë“œ)
    if node_degree:
        sorted_nodes = sorted(node_degree.items(), key=lambda x: x[1], reverse=True)
        st.markdown("**ì—°ê²°ë„ê°€ ë†’ì€ ë…¸ë“œ (Top 10)**")
        hub_data = []
        for node_id, degree in sorted_nodes[:10]:
            node = next((n for n in nodes if n.get("id") == node_id), None)
            if node:
                hub_data.append({
                    "ë…¸ë“œ": node.get("label", node_id),
                    "ê·¸ë£¹": node.get("group", ""),
                    "ì—°ê²°ë„": degree
                })
        
        if hub_data:
            st.dataframe(pd.DataFrame(hub_data), width='stretch')
    
    # ê³ ë¦½ëœ ë…¸ë“œ (ì—°ê²°ë„ê°€ 0ì¸ ë…¸ë“œ)
    isolated_nodes = [n for n in nodes if node_degree.get(n.get("id"), 0) == 0]
    if isolated_nodes:
        st.warning(f"âš ï¸ ê³ ë¦½ëœ ë…¸ë“œ {len(isolated_nodes)}ê°œ ë°œê²¬")
        with st.expander("ê³ ë¦½ëœ ë…¸ë“œ ëª©ë¡ ë³´ê¸°"):
            isolated_data = []
            for node in isolated_nodes[:20]:  # ìµœëŒ€ 20ê°œë§Œ í‘œì‹œ
                isolated_data.append({
                    "ë…¸ë“œ": node.get("label", node.get("id")),
                    "ê·¸ë£¹": node.get("group", "")
                })
            if isolated_data:
                st.dataframe(pd.DataFrame(isolated_data), width='stretch')
            if len(isolated_nodes) > 20:
                st.caption(f"... ì™¸ {len(isolated_nodes) - 20}ê°œ")
    
    # ê´€ê³„ ìœ í˜• ë¶„ì„
    st.divider()
    st.markdown("##### ğŸ”— ê´€ê³„ ìœ í˜• ë¶„ì„")
    relation_types = Counter(link.get("relation", "Unknown") for link in links)
    if relation_types:
        st.markdown("**ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ê´€ê³„ ìœ í˜• (Top 10)**")
        rel_data = []
        for rel_type, count in relation_types.most_common(10):
            rel_data.append({
                "ê´€ê³„ ìœ í˜•": rel_type,
                "ì‚¬ìš© íšŸìˆ˜": count,
                "ë¹„ìœ¨": f"{count / len(links) * 100:.1f}%"
            })
        st.dataframe(pd.DataFrame(rel_data), width='stretch')
    
    # ê·¸ë£¹ë³„ ë¶„í¬
    st.divider()
    st.markdown("##### ğŸ“Š ê·¸ë£¹ë³„ ë…¸ë“œ ë¶„í¬")
    group_counts = Counter(n.get("group", "Unknown") for n in nodes)
    if group_counts:
        group_data = []
        for group, count in group_counts.most_common():
            group_data.append({
                "ê·¸ë£¹": group,
                "ë…¸ë“œ ìˆ˜": count,
                "ë¹„ìœ¨": f"{count / len(nodes) * 100:.1f}%"
            })
        st.dataframe(pd.DataFrame(group_data), width='stretch')


def _render_missing_relations_analysis(core, graph_data: Dict):
    """ëˆ„ë½ëœ ê´€ê³„ íƒì§€"""
    st.markdown("#### ğŸ” ëˆ„ë½ëœ ê´€ê³„ íƒì§€")
    
    if not RDFLIB_AVAILABLE or not core.ontology_manager or not core.ontology_manager.graph:
        st.warning("ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ê°€ ì—†ì–´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    graph = core.ontology_manager.graph
    instances = graph_data.get("instances", {})
    nodes = instances.get("nodes", [])
    links = instances.get("links", [])
    
    if not nodes:
        st.warning("ë¶„ì„í•  ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ì¡´ ê´€ê³„ ë§µ ìƒì„±
    existing_relations = defaultdict(set)
    for link in links:
        source = link.get("source")
        target = link.get("target")
        relation = link.get("relation", "")
        if source and target:
            existing_relations[source].add((target, relation))
    
    # ëˆ„ë½ëœ ê´€ê³„ íƒì§€
    st.markdown("##### ğŸ”— ì ì¬ì  ê´€ê³„ ë¶„ì„")
    
    # 1. ì™¸ë˜í‚¤ ê¸°ë°˜ ëˆ„ë½ ê´€ê³„ íƒì§€
    missing_fk_relations = _detect_missing_fk_relations(core, graph, nodes, existing_relations)
    
    if missing_fk_relations:
        st.markdown("**ì™¸ë˜í‚¤ ê¸°ë°˜ ëˆ„ë½ ê´€ê³„**")
        st.info(f"ì™¸ë˜í‚¤ ì»¬ëŸ¼ì´ ìˆì§€ë§Œ ê´€ê³„ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° {len(missing_fk_relations)}ê±´ ë°œê²¬")
        with st.expander("ëˆ„ë½ëœ ê´€ê³„ ëª©ë¡ ë³´ê¸°"):
            fk_data = []
            for item in missing_fk_relations[:20]:  # ìµœëŒ€ 20ê°œ
                fk_data.append({
                    "ì†ŒìŠ¤ ë…¸ë“œ": item.get("source_label", item.get("source")),
                    "íƒ€ê²Ÿ ë…¸ë“œ": item.get("target_label", item.get("target")),
                    "ì œì•ˆ ê´€ê³„": item.get("relation", ""),
                    "ì‹ ë¢°ë„": f"{item.get('confidence', 0):.2f}"
                })
            if fk_data:
                st.dataframe(pd.DataFrame(fk_data), width='stretch')
            if len(missing_fk_relations) > 20:
                st.caption(f"... ì™¸ {len(missing_fk_relations) - 20}ê±´")
    else:
        st.success("âœ… ì™¸ë˜í‚¤ ê¸°ë°˜ ëˆ„ë½ ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # 2. ìœ ì‚¬ì„± ê¸°ë°˜ ê´€ê³„ ì œì•ˆ
    st.divider()
    
    # ë°©ë²• ì„ íƒ
    similarity_method = st.radio(
        "ìœ ì‚¬ì„± ë¶„ì„ ë°©ë²•",
        ["ê¸°ë³¸ (ê·¸ë£¹ ê¸°ë°˜)", "êµ¬ì¡°ì  ìœ ì‚¬ì„± (ê¶Œì¥)", "í•˜ì´ë¸Œë¦¬ë“œ (LLM ê²€ì¦)"],
        horizontal=True,
        help="ê¸°ë³¸: ê°™ì€ ê·¸ë£¹ ë‚´ ë…¸ë“œ ìŒ ì œì•ˆ\nêµ¬ì¡°ì  ìœ ì‚¬ì„±: ê·¸ë˜í”„ êµ¬ì¡° ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°\ní•˜ì´ë¸Œë¦¬ë“œ: êµ¬ì¡°ì  ìœ ì‚¬ì„± + LLM ê²€ì¦"
    )
    
    with st.spinner("ìœ ì‚¬ì„± ë¶„ì„ ì¤‘..."):
        if similarity_method == "ê¸°ë³¸ (ê·¸ë£¹ ê¸°ë°˜)":
            similarity_relations = _suggest_similarity_based_relations(nodes, links, existing_relations)
        elif similarity_method == "êµ¬ì¡°ì  ìœ ì‚¬ì„± (ê¶Œì¥)":
            similarity_relations = _suggest_similarity_based_relations_structural(
                nodes, links, existing_relations, max_suggestions=20
            )
        else:  # í•˜ì´ë¸Œë¦¬ë“œ
            similarity_relations = _suggest_similarity_based_relations_hybrid(
                core, nodes, links, existing_relations, use_llm=True, max_suggestions=20
            )
    
    if similarity_relations:
        st.markdown("**ìœ ì‚¬ì„± ê¸°ë°˜ ê´€ê³„ ì œì•ˆ**")
        st.info(f"ìœ ì‚¬í•œ ì†ì„±ì„ ê°€ì§„ ë…¸ë“œ ê°„ ì ì¬ì  ê´€ê³„ {len(similarity_relations)}ê±´ ì œì•ˆ")
        
        with st.expander("ì œì•ˆëœ ê´€ê³„ ëª©ë¡ ë³´ê¸°", expanded=True):
            # ì„ íƒëœ ê´€ê³„ë¥¼ ì €ì¥í•  ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            if "selected_relations" not in st.session_state:
                st.session_state.selected_relations = set()
            
            # í‘œ ë°ì´í„° ì¤€ë¹„
            sim_data = []
            selected_indices = []
            
            for idx, item in enumerate(similarity_relations[:20]):  # ìµœëŒ€ 20ê°œ
                # ì„¸ì…˜ ìƒíƒœì—ì„œ ì„ íƒ ì—¬ë¶€ í™•ì¸
                is_selected = idx in st.session_state.selected_relations
                
                row = {
                    "ì„ íƒ": "âœ“" if is_selected else "",
                    "ë…¸ë“œ 1": item.get("node1_label", item.get("node1")),
                    "ë…¸ë“œ 2": item.get("node2_label", item.get("node2")),
                    "ì œì•ˆ ê´€ê³„": item.get("relation", "relatedTo"),
                    "ìœ ì‚¬ë„": f"{item.get('similarity', 0):.2f}"
                }
                
                # êµ¬ì¡°ì  ìœ ì‚¬ì„± ë°©ë²•ì¸ ê²½ìš° ì¶”ê°€ ì •ë³´ í‘œì‹œ
                if similarity_method == "êµ¬ì¡°ì  ìœ ì‚¬ì„± (ê¶Œì¥)":
                    if item.get('common_neighbors') is not None:
                        row["ê³µí†µ ì´ì›ƒ"] = item.get('common_neighbors', 0)
                    if item.get('path_length') is not None:
                        row["ê²½ë¡œ ê¸¸ì´"] = item.get('path_length', 'N/A')
                
                # í•˜ì´ë¸Œë¦¬ë“œ ë°©ë²•ì¸ ê²½ìš° LLM ê²€ì¦ ì—¬ë¶€ í‘œì‹œ
                if similarity_method == "í•˜ì´ë¸Œë¦¬ë“œ (LLM ê²€ì¦)":
                    row["LLM ê²€ì¦"] = "âœ“" if item.get('llm_validated', False) else "-"
                
                sim_data.append(row)
                if is_selected:
                    selected_indices.append(idx)
            
            # í‘œ í‘œì‹œ
            if sim_data:
                df = pd.DataFrame(sim_data)
                st.dataframe(df, width="stretch", hide_index=True)
            
            st.divider()
            
            # ê° í–‰ì— ëŒ€í•œ ì²´í¬ë°•ìŠ¤ (í‘œ ì•„ë˜ì— ê·¸ë¦¬ë“œ í˜•íƒœë¡œ ë°°ì¹˜)
            st.markdown("**ê´€ê³„ ì„ íƒ (í‘œì˜ í–‰ ë²ˆí˜¸ì™€ ì¼ì¹˜):**")
            num_cols = 5
            num_items = len(similarity_relations[:20])
            num_rows = (num_items + num_cols - 1) // num_cols
            
            for row_idx in range(num_rows):
                cols = st.columns(num_cols)
                for col_idx in range(num_cols):
                    idx = row_idx * num_cols + col_idx
                    if idx < num_items:
                        with cols[col_idx]:
                            item = similarity_relations[idx]
                            # ì§§ì€ ë¼ë²¨ ìƒì„±
                            node1_label = item.get('node1_label', item.get('node1', ''))[:12]
                            checkbox_label = f"#{idx+1}: {node1_label}..."
                            
                            is_selected = st.checkbox(
                                checkbox_label,
                                value=idx in st.session_state.selected_relations,
                                key=f"relation_select_{idx}"
                            )
                            
                            if is_selected:
                                st.session_state.selected_relations.add(idx)
                                if idx not in selected_indices:
                                    selected_indices.append(idx)
                            else:
                                st.session_state.selected_relations.discard(idx)
                                if idx in selected_indices:
                                    selected_indices.remove(idx)
            
            if len(similarity_relations) > 20:
                st.caption(f"... ì™¸ {len(similarity_relations) - 20}ê±´")
            
            st.divider()
            
            # ì„ íƒëœ ê´€ê³„ ì¶”ê°€ ë²„íŠ¼
            # ì„¸ì…˜ ìƒíƒœì—ì„œ ì„ íƒëœ í•­ëª© ë‹¤ì‹œ ì½ê¸°
            current_selected = list(st.session_state.selected_relations)
            
            if current_selected:
                col1, col2 = st.columns([1, 3])
                with col1:
                    if st.button("âœ… ì„ íƒëœ ê´€ê³„ ì¶”ê°€", type="primary"):
                        with st.spinner("ê´€ê³„ ì¶”ê°€ ì¤‘..."):
                            try:
                                relationships_to_add = []
                                for idx in current_selected:
                                    if idx < len(similarity_relations):
                                        item = similarity_relations[idx]
                                        relationships_to_add.append({
                                            "source": item.get("node1"),
                                            "target": item.get("node2"),
                                            "relation": item.get("relation", "relatedTo")
                                        })
                                
                                # ê·¸ë˜í”„ì— ê´€ê³„ ì¶”ê°€
                                if hasattr(core, 'ontology_manager') and core.ontology_manager:
                                    result = core.ontology_manager.add_relationships_batch(relationships_to_add)
                                    
                                    if result["success"] > 0:
                                        # ê·¸ë˜í”„ ì €ì¥
                                        core.ontology_manager.save_graph()
                                        
                                        st.success(f"âœ… {result['success']}ê°œ ê´€ê³„ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                                        if result["failed"] > 0:
                                            st.warning(f"âš ï¸ {result['failed']}ê°œ ê´€ê³„ ì¶”ê°€ ì‹¤íŒ¨")
                                        
                                        # ì„ íƒ ì´ˆê¸°í™”
                                        st.session_state.selected_relations = set()
                                        
                                        # ê·¸ë˜í”„ ìƒˆë¡œê³ ì¹¨
                                        st.rerun()
                                    else:
                                        st.error("ê´€ê³„ ì¶”ê°€ ì‹¤íŒ¨")
                                else:
                                    st.error("Ontology Managerë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            except Exception as e:
                                st.error(f"ê´€ê³„ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                                import traceback
                                st.code(traceback.format_exc())
                
                with col2:
                    if st.button("ğŸ”„ ì„ íƒ ì´ˆê¸°í™”"):
                        st.session_state.selected_relations = set()
                        st.rerun()
    else:
        st.info("ìœ ì‚¬ì„± ê¸°ë°˜ ê´€ê³„ ì œì•ˆì´ ì—†ìŠµë‹ˆë‹¤.")


def _detect_missing_fk_relations(core, graph: Graph, nodes: List[Dict], 
                                 existing_relations: Dict) -> List[Dict]:
    """ì™¸ë˜í‚¤ ê¸°ë°˜ ëˆ„ë½ ê´€ê³„ íƒì§€"""
    missing_relations = []
    
    if not hasattr(core, 'data_manager') or not core.data_manager:
        return missing_relations
    
    try:
        # ë°ì´í„° ë¡œë“œ
        data = core.data_manager.load_all()
        
        # ê´€ê³„ ë§¤í•‘ ë¡œë“œ (Enhanced Ontology Managerê°€ ìˆìœ¼ë©´)
        relation_mappings = []
        if hasattr(core, 'enhanced_ontology_manager') and core.enhanced_ontology_manager:
            relation_mappings = core.enhanced_ontology_manager.load_relation_mappings()
        
        # ê° ë…¸ë“œì— ëŒ€í•´ ì™¸ë˜í‚¤ í™•ì¸
        for node in nodes[:50]:  # ì„±ëŠ¥ì„ ìœ„í•´ ìµœëŒ€ 50ê°œë§Œ í™•ì¸
            node_id = node.get("id", "")
            node_label = node.get("label", node_id)
            node_group = node.get("group", "")
            
            # ë…¸ë“œ IDì—ì„œ í…Œì´ë¸”ëª…ê³¼ í–‰ ID ì¶”ì¶œ
            if "_" in node_id:
                parts = node_id.split("_", 1)
                if len(parts) == 2:
                    table_name = parts[0]
                    row_id = parts[1]
                    
                    # í•´ë‹¹ í…Œì´ë¸”ì˜ ë°ì´í„° í™•ì¸
                    if table_name in data:
                        df = data[table_name]
                        # ID ì»¬ëŸ¼ ì°¾ê¸°
                        id_col = None
                        for col in df.columns:
                            if col.upper() == 'ID' or col.endswith('ID') or col.endswith('_id'):
                                id_col = col
                                break
                        
                        if id_col:
                            # í•´ë‹¹ í–‰ ì°¾ê¸°
                            matching_rows = df[df[id_col].astype(str).str.strip() == row_id]
                            if not matching_rows.empty:
                                row = matching_rows.iloc[0]
                                
                                # ê´€ê³„ ë§¤í•‘ í™•ì¸
                                for rel_map in relation_mappings:
                                    if rel_map.get('src_table') == table_name:
                                        src_col = rel_map.get('src_col')
                                        if src_col and src_col in row:
                                            fk_val = str(row[src_col]).strip()
                                            if fk_val and fk_val != 'nan':
                                                tgt_table = rel_map.get('tgt_table')
                                                relation_name = rel_map.get('relation', f"has{tgt_table}")
                                                
                                                # íƒ€ê²Ÿ ë…¸ë“œ ì°¾ê¸°
                                                target_node_id = f"{tgt_table}_{fk_val}"
                                                
                                                # ì´ë¯¸ ê´€ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸
                                                if target_node_id not in [r[0] for r in existing_relations.get(node_id, set())]:
                                                    missing_relations.append({
                                                        "source": node_id,
                                                        "source_label": node_label,
                                                        "target": target_node_id,
                                                        "target_label": f"{tgt_table}_{fk_val}",
                                                        "relation": relation_name,
                                                        "confidence": 0.9  # ì™¸ë˜í‚¤ ê¸°ë°˜ì´ë¯€ë¡œ ë†’ì€ ì‹ ë¢°ë„
                                                    })
    except Exception as e:
        st.warning(f"ëˆ„ë½ ê´€ê³„ íƒì§€ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return missing_relations


def _find_shortest_path(neighbors_map: Dict, start: str, end: str, max_depth: int = 3) -> Optional[List[str]]:
    """BFSë¡œ ìµœë‹¨ ê²½ë¡œ ì°¾ê¸°"""
    if start == end:
        return [start]
    
    queue = deque([(start, [start])])
    visited = {start}
    
    while queue and len(queue[0][1]) <= max_depth:
        current, path = queue.popleft()
        
        for neighbor in neighbors_map.get(current, set()):
            if neighbor == end:
                return path + [neighbor]
            
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append((neighbor, path + [neighbor]))
    
    return None


def _suggest_similarity_based_relations_structural(
    nodes: List[Dict], 
    links: List[Dict],
    existing_relations: Dict,
    max_suggestions: int = 20
) -> List[Dict]:
    """
    êµ¬ì¡°ì  ìœ ì‚¬ì„± ê¸°ë°˜ ê´€ê³„ ì œì•ˆ (Palantir ìŠ¤íƒ€ì¼)
    
    ë°©ë²•:
    1. ê³µí†µ ì´ì›ƒ (Common Neighbors): ë‘ ë…¸ë“œê°€ ê³µìœ í•˜ëŠ” ì´ì›ƒ ë…¸ë“œ ìˆ˜
    2. Jaccard ìœ ì‚¬ë„: ê³µí†µ ì´ì›ƒ / ì „ì²´ ì´ì›ƒ
    3. Adamic-Adar ì ìˆ˜: ê³µí†µ ì´ì›ƒì˜ ì—°ê²°ë„ ì—­ê°€ì¤‘
    4. ê²½ë¡œ ê¸°ë°˜ ìœ ì‚¬ë„: ìµœë‹¨ ê²½ë¡œ ê±°ë¦¬
    """
    suggestions = []
    
    # ë…¸ë“œë³„ ì´ì›ƒ ë§µ êµ¬ì„±
    neighbors_map = defaultdict(set)
    for link in links:
        source = link.get("source", "")
        target = link.get("target", "")
        if source and target:
            neighbors_map[source].add(target)
            neighbors_map[target].add(source)
    
    # ë…¸ë“œ ìŒë³„ ìœ ì‚¬ë„ ê³„ì‚°
    node_pairs = []
    for i, node1 in enumerate(nodes):
        for node2 in nodes[i+1:]:
            node1_id = node1.get("id")
            node2_id = node2.get("id")
            
            # ì´ë¯¸ ê´€ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸
            has_relation = False
            if node1_id in existing_relations:
                if any(target == node2_id for target, _ in existing_relations[node1_id]):
                    has_relation = True
            
            if has_relation:
                continue
            
            # ê³µí†µ ì´ì›ƒ ê³„ì‚°
            neighbors1 = neighbors_map.get(node1_id, set())
            neighbors2 = neighbors_map.get(node2_id, set())
            common_neighbors = neighbors1 & neighbors2
            
            # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
            similarity_scores = {}
            
            # 1. ê³µí†µ ì´ì›ƒ ìˆ˜
            similarity_scores['common_neighbors'] = len(common_neighbors)
            
            # 2. Jaccard ìœ ì‚¬ë„
            union_neighbors = neighbors1 | neighbors2
            if union_neighbors:
                similarity_scores['jaccard'] = len(common_neighbors) / len(union_neighbors)
            else:
                similarity_scores['jaccard'] = 0.0
            
            # 3. Adamic-Adar ì ìˆ˜ (ê³µí†µ ì´ì›ƒì˜ ì—°ê²°ë„ ì—­ê°€ì¤‘)
            adamic_adar = 0.0
            for neighbor in common_neighbors:
                neighbor_degree = len(neighbors_map.get(neighbor, set()))
                if neighbor_degree > 1:
                    adamic_adar += 1.0 / math.log(neighbor_degree)
            similarity_scores['adamic_adar'] = adamic_adar
            
            # 4. ìµœë‹¨ ê²½ë¡œ ê±°ë¦¬ (BFS)
            shortest_path = _find_shortest_path(neighbors_map, node1_id, node2_id, max_depth=3)
            if shortest_path:
                similarity_scores['path_similarity'] = 1.0 / (len(shortest_path) + 1)
            else:
                similarity_scores['path_similarity'] = 0.0
            
            # ì¢…í•© ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
            final_score = (
                similarity_scores['jaccard'] * 0.4 +
                min(similarity_scores['adamic_adar'] / 10.0, 1.0) * 0.3 +
                similarity_scores['path_similarity'] * 0.3
            )
            
            if final_score > 0.1:  # ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš°ë§Œ ì œì•ˆ
                node_pairs.append({
                    "node1": node1_id,
                    "node1_label": node1.get("label", node1_id),
                    "node2": node2_id,
                    "node2_label": node2.get("label", node2_id),
                    "similarity": final_score,
                    "common_neighbors": len(common_neighbors),
                    "jaccard": similarity_scores['jaccard'],
                    "adamic_adar": similarity_scores['adamic_adar'],
                    "path_length": len(shortest_path) if shortest_path else None,
                    "relation": "relatedTo"  # ê¸°ë³¸ ê´€ê³„ëª…
                })
    
    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    node_pairs.sort(key=lambda x: x.get("similarity", 0), reverse=True)
    return node_pairs[:max_suggestions]


def _suggest_similarity_based_relations_hybrid(
    core,
    nodes: List[Dict], 
    links: List[Dict],
    existing_relations: Dict,
    use_llm: bool = True,
    max_suggestions: int = 20
) -> List[Dict]:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼: êµ¬ì¡°ì  ìœ ì‚¬ì„± + LLM ê²€ì¦ (Palantir ìŠ¤íƒ€ì¼)
    
    ë°©ë²•:
    1. êµ¬ì¡°ì  ìœ ì‚¬ì„±ìœ¼ë¡œ í›„ë³´ ì„ ì •
    2. LLMìœ¼ë¡œ ìƒìœ„ ì œì•ˆ ê²€ì¦ ë° ê´€ê³„ëª… ì œì•ˆ
    """
    # 1. êµ¬ì¡°ì  ìœ ì‚¬ì„± ê¸°ë°˜ ì œì•ˆ
    structural_suggestions = _suggest_similarity_based_relations_structural(
        nodes, links, existing_relations, max_suggestions * 2
    )
    
    if not structural_suggestions:
        return []
    
    # 2. LLMìœ¼ë¡œ ìƒìœ„ ì œì•ˆ ê²€ì¦ ë° ê´€ê³„ëª… ì œì•ˆ (ì„ íƒì )
    if use_llm and core and hasattr(core, 'llm_manager') and core.llm_manager and core.llm_manager.is_available():
        # ìƒìœ„ 10ê°œë§Œ LLMìœ¼ë¡œ ê²€ì¦ (ì„±ëŠ¥ ìµœì í™”)
        top_suggestions = structural_suggestions[:10]
        
        for suggestion in top_suggestions:
            # LLMìœ¼ë¡œ ê´€ê³„ëª… ì œì•ˆ
            prompt = f"""ë‹¤ìŒ ë‘ ë…¸ë“œ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

ë…¸ë“œ 1: {suggestion['node1_label']} (ID: {suggestion['node1']})
ë…¸ë“œ 2: {suggestion['node2_label']} (ID: {suggestion['node2']})
êµ¬ì¡°ì  ìœ ì‚¬ë„: {suggestion.get('similarity', 0):.2f}
ê³µí†µ ì´ì›ƒ: {suggestion.get('common_neighbors', 0)}ê°œ

ì´ ë‘ ë…¸ë“œ ê°„ì— ì˜ë¯¸ ìˆëŠ” ê´€ê³„ê°€ ìˆë‹¤ë©´ ê´€ê³„ëª…ì„ ì œì•ˆí•˜ì„¸ìš”. ì—†ìœ¼ë©´ "ì—†ìŒ"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.
ê´€ê³„ëª…ë§Œ ê°„ë‹¨íˆ ë‹µí•˜ì„¸ìš” (ì˜ˆ: relatedTo, partOf, locatedIn, hasMission ë“±)"""
            
            try:
                response = core.llm_manager.generate(prompt, max_tokens=20)
                relation_name = response.strip()
                if relation_name and relation_name != "ì—†ìŒ" and len(relation_name) < 50:
                    suggestion['relation'] = relation_name
                    suggestion['llm_validated'] = True
                else:
                    suggestion['llm_validated'] = False
            except Exception as e:
                suggestion['llm_validated'] = False
                st.warning(f"LLM ê²€ì¦ ì‹¤íŒ¨: {e}")
    else:
        # LLM ì—†ìœ¼ë©´ ëª¨ë‘ ê²€ì¦ë˜ì§€ ì•ŠìŒìœ¼ë¡œ í‘œì‹œ
        for suggestion in structural_suggestions:
            suggestion['llm_validated'] = False
    
    # LLM ê²€ì¦ëœ ê²ƒ ìš°ì„ , ê·¸ ë‹¤ìŒ ìœ ì‚¬ë„ ìˆœ
    validated = [s for s in structural_suggestions if s.get('llm_validated', False)]
    not_validated = [s for s in structural_suggestions if not s.get('llm_validated', False)]
    
    final_suggestions = validated + not_validated
    return final_suggestions[:max_suggestions]


def _suggest_similarity_based_relations(nodes: List[Dict], links: List[Dict],
                                       existing_relations: Dict) -> List[Dict]:
    """ìœ ì‚¬ì„± ê¸°ë°˜ ê´€ê³„ ì œì•ˆ (ê¸°ë³¸ ë°©ë²•: ê·¸ë£¹ ê¸°ë°˜)"""
    suggestions = []
    
    # ê°„ë‹¨í•œ ìœ ì‚¬ì„± ê¸°ë°˜ ì œì•ˆ (ê·¸ë£¹ì´ ê°™ê³  ì—°ê²°ì´ ì—†ëŠ” ê²½ìš°)
    node_by_group = defaultdict(list)
    for node in nodes:
        group = node.get("group", "")
        if group:
            node_by_group[group].append(node)
    
    # ê°™ì€ ê·¸ë£¹ ë‚´ì—ì„œ ì—°ê²°ì´ ì—†ëŠ” ë…¸ë“œ ìŒ ì°¾ê¸°
    for group, group_nodes in node_by_group.items():
        if len(group_nodes) >= 2:
            # ì´ë¯¸ ì—°ê²°ëœ ë…¸ë“œ ìŒ ì œì™¸
            for i, node1 in enumerate(group_nodes[:10]):  # ì„±ëŠ¥ ì œí•œ
                for node2 in group_nodes[i+1:11]:
                    node1_id = node1.get("id")
                    node2_id = node2.get("id")
                    
                    # ì´ë¯¸ ê´€ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸
                    has_relation = False
                    if node1_id in existing_relations:
                        if any(target == node2_id for target, _ in existing_relations[node1_id]):
                            has_relation = True
                    
                    if not has_relation:
                        suggestions.append({
                            "node1": node1_id,
                            "node1_label": node1.get("label", node1_id),
                            "node2": node2_id,
                            "node2_label": node2.get("label", node2_id),
                            "relation": "relatedTo",
                            "similarity": 0.5  # ê¸°ë³¸ ìœ ì‚¬ë„
                        })
    
    return suggestions[:50]  # ìµœëŒ€ 50ê°œë§Œ ë°˜í™˜


def _render_quality_assessment(core, graph_data: Dict, is_instance: bool):
    """í’ˆì§ˆ í‰ê°€"""
    st.markdown("#### âœ… ê·¸ë˜í”„ í’ˆì§ˆ í‰ê°€")
    
    if is_instance:
        _assess_instance_quality(core, graph_data)
    else:
        _assess_schema_quality(core, graph_data)


def _assess_instance_quality(core, graph_data: Dict):
    """ì¸ìŠ¤í„´ìŠ¤ ê·¸ë˜í”„ í’ˆì§ˆ í‰ê°€"""
    instances = graph_data.get("instances", {})
    nodes = instances.get("nodes", [])
    links = instances.get("links", [])
    
    quality_scores = {}
    issues = []
    
    # 1. ì—°ê²°ì„± í‰ê°€
    node_degree = defaultdict(int)
    for link in links:
        node_degree[link.get("source")] += 1
        node_degree[link.get("target")] += 1
    
    isolated_count = sum(1 for n in nodes if node_degree.get(n.get("id"), 0) == 0)
    connectivity_score = 1.0 - (isolated_count / len(nodes)) if nodes else 0.0
    quality_scores["ì—°ê²°ì„±"] = connectivity_score
    
    if isolated_count > 0:
        issues.append({
            "í•­ëª©": "ì—°ê²°ì„±",
            "ë¬¸ì œ": f"ê³ ë¦½ëœ ë…¸ë“œ {isolated_count}ê°œ ë°œê²¬",
            "ì‹¬ê°ë„": "ì¤‘ê°„" if isolated_count < len(nodes) * 0.1 else "ë†’ìŒ",
            "ê¶Œì¥ ì¡°ì¹˜": "ì™¸ë˜í‚¤ ê´€ê³„ë¥¼ í™•ì¸í•˜ê³  ëˆ„ë½ëœ ê´€ê³„ë¥¼ ì¶”ê°€í•˜ì„¸ìš”"
        })
    
    # 2. ê´€ê³„ ë°€ë„ í‰ê°€
    if nodes:
        max_possible_links = len(nodes) * (len(nodes) - 1) / 2
        actual_density = len(links) / max_possible_links if max_possible_links > 0 else 0
        quality_scores["ê´€ê³„ ë°€ë„"] = min(actual_density * 10, 1.0)  # ì •ê·œí™”
    
    # 3. ê·¸ë£¹ ë¶„í¬ í‰ê°€
    groups = set(n.get("group", "") for n in nodes)
    if groups:
        group_distribution = Counter(n.get("group", "") for n in nodes)
        max_group_count = max(group_distribution.values())
        min_group_count = min(group_distribution.values())
        balance_score = min_group_count / max_group_count if max_group_count > 0 else 0
        quality_scores["ê·¸ë£¹ ê· í˜•"] = balance_score
        
        if balance_score < 0.1:
            issues.append({
                "í•­ëª©": "ê·¸ë£¹ ê· í˜•",
                "ë¬¸ì œ": "ê·¸ë£¹ ê°„ ë…¸ë“œ ìˆ˜ì˜ ë¶ˆê· í˜•ì´ í½ë‹ˆë‹¤",
                "ì‹¬ê°ë„": "ë‚®ìŒ",
                "ê¶Œì¥ ì¡°ì¹˜": "ë°ì´í„° ìˆ˜ì§‘ ì‹œ ê·¸ë£¹ë³„ ê· í˜•ì„ ê³ ë ¤í•˜ì„¸ìš”"
            })
    
    # í’ˆì§ˆ ì ìˆ˜ í‘œì‹œ
    st.markdown("##### ğŸ“Š í’ˆì§ˆ ì ìˆ˜")
    cols = st.columns(len(quality_scores))
    for idx, (metric, score) in enumerate(quality_scores.items()):
        with cols[idx]:
            st.metric(metric, f"{score:.2f}")
    
    # ë¬¸ì œì  í‘œì‹œ
    if issues:
        st.divider()
        st.markdown("##### âš ï¸ ë°œê²¬ëœ ë¬¸ì œì ")
        issues_df = pd.DataFrame(issues)
        st.dataframe(issues_df, width='stretch')
    else:
        st.success("âœ… ì‹¬ê°í•œ í’ˆì§ˆ ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


def _assess_schema_quality(core, graph_data: Dict):
    """ìŠ¤í‚¤ë§ˆ ê·¸ë˜í”„ í’ˆì§ˆ í‰ê°€"""
    schema = graph_data.get("schema", {})
    nodes = schema.get("nodes", [])
    links = schema.get("links", [])
    
    if not RDFLIB_AVAILABLE or not core.ontology_manager or not core.ontology_manager.graph:
        st.warning("ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ê°€ ì—†ì–´ ìŠ¤í‚¤ë§ˆ í’ˆì§ˆì„ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    graph = core.ontology_manager.graph
    quality_scores = {}
    issues = []
    
    # 1. í´ë˜ìŠ¤ ì •ì˜ ì™„ì„±ë„
    classes = [n for n in nodes if n.get("group") == "Class"]
    properties = [n for n in nodes if n.get("group") == "Property"]
    
    quality_scores["í´ë˜ìŠ¤ ìˆ˜"] = len(classes)
    quality_scores["ì†ì„± ìˆ˜"] = len(properties)
    
    # 2. Domain/Range ì •ì˜ í™•ì¸
    domain_links = [l for l in links if l.get("relation") == "domain"]
    range_links = [l for l in links if l.get("relation") == "range"]
    
    if properties:
        domain_completeness = len(domain_links) / len(properties)
        range_completeness = len(range_links) / len(properties)
        quality_scores["Domain ì •ì˜ìœ¨"] = domain_completeness
        quality_scores["Range ì •ì˜ìœ¨"] = range_completeness
        
        if domain_completeness < 0.8:
            issues.append({
                "í•­ëª©": "Domain ì •ì˜",
                "ë¬¸ì œ": f"ì†ì„±ì˜ {int((1-domain_completeness)*100)}%ê°€ domainì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤",
                "ì‹¬ê°ë„": "ì¤‘ê°„",
                "ê¶Œì¥ ì¡°ì¹˜": "ì†ì„±ì— domainì„ ëª…ì‹œí•˜ì—¬ ìŠ¤í‚¤ë§ˆë¥¼ ì™„ì„±í•˜ì„¸ìš”"
            })
        
        if range_completeness < 0.8:
            issues.append({
                "í•­ëª©": "Range ì •ì˜",
                "ë¬¸ì œ": f"ì†ì„±ì˜ {int((1-range_completeness)*100)}%ê°€ rangeê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤",
                "ì‹¬ê°ë„": "ì¤‘ê°„",
                "ê¶Œì¥ ì¡°ì¹˜": "ì†ì„±ì— rangeë¥¼ ëª…ì‹œí•˜ì—¬ ìŠ¤í‚¤ë§ˆë¥¼ ì™„ì„±í•˜ì„¸ìš”"
            })
    
    # 3. ê³„ì¸µ êµ¬ì¡° í™•ì¸
    subclass_links = [l for l in links if l.get("relation") == "subClassOf"]
    if classes:
        hierarchy_score = len(subclass_links) / len(classes) if classes else 0
        quality_scores["ê³„ì¸µ êµ¬ì¡° ì™„ì„±ë„"] = min(hierarchy_score, 1.0)
    
    # í’ˆì§ˆ ì ìˆ˜ í‘œì‹œ
    st.markdown("##### ğŸ“Š ìŠ¤í‚¤ë§ˆ í’ˆì§ˆ ì ìˆ˜")
    cols = st.columns(len(quality_scores))
    for idx, (metric, score) in enumerate(quality_scores.items()):
        with cols[idx]:
            if isinstance(score, float):
                st.metric(metric, f"{score:.2f}")
            else:
                st.metric(metric, score)
    
    # ë¬¸ì œì  í‘œì‹œ
    if issues:
        st.divider()
        st.markdown("##### âš ï¸ ë°œê²¬ëœ ë¬¸ì œì ")
        issues_df = pd.DataFrame(issues)
        st.dataframe(issues_df, width='stretch')
    else:
        st.success("âœ… ìŠ¤í‚¤ë§ˆ í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")


def _render_improvement_suggestions(core, graph_data: Dict, is_instance: bool):
    """ê°œì„  ì œì•ˆ"""
    st.markdown("#### ğŸ’¡ ê°œì„  ì œì•ˆ")
    
    suggestions = []
    
    if is_instance:
        # ì¸ìŠ¤í„´ìŠ¤ ê·¸ë˜í”„ ê°œì„  ì œì•ˆ
        instances = graph_data.get("instances", {})
        nodes = instances.get("nodes", [])
        links = instances.get("links", [])
        
        # ê³ ë¦½ëœ ë…¸ë“œê°€ ë§ìœ¼ë©´
        node_degree = defaultdict(int)
        for link in links:
            node_degree[link.get("source")] += 1
            node_degree[link.get("target")] += 1
        
        isolated_count = sum(1 for n in nodes if node_degree.get(n.get("id"), 0) == 0)
        if isolated_count > len(nodes) * 0.1:
            suggestions.append({
                "ìš°ì„ ìˆœìœ„": "ë†’ìŒ",
                "ì œì•ˆ": "ê³ ë¦½ëœ ë…¸ë“œ ì—°ê²°",
                "ì„¤ëª…": f"ì „ì²´ ë…¸ë“œì˜ {isolated_count/len(nodes)*100:.1f}%ê°€ ê³ ë¦½ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì™¸ë˜í‚¤ ê´€ê³„ë¥¼ í™•ì¸í•˜ê³  ëˆ„ë½ëœ ê´€ê³„ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.",
                "ì‹¤í–‰ ë°©ë²•": "ê´€ê³„ ë§¤í•‘ íŒŒì¼(metadata/relation_mappings.json)ì„ í™•ì¸í•˜ê³  í•„ìš”í•œ ê´€ê³„ë¥¼ ì¶”ê°€í•˜ì„¸ìš”. í¸ì§‘ ê°€ì´ë“œëŠ” metadata/RELATION_MAPPINGS_GUIDE.mdë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
            })
        
        # ê´€ê³„ ë°€ë„ê°€ ë‚®ìœ¼ë©´
        if nodes:
            max_possible = len(nodes) * (len(nodes) - 1) / 2
            actual_density = len(links) / max_possible if max_possible > 0 else 0
            if actual_density < 0.01:
                suggestions.append({
                    "ìš°ì„ ìˆœìœ„": "ì¤‘ê°„",
                    "ì œì•ˆ": "ê´€ê³„ ë°€ë„ í–¥ìƒ",
                    "ì„¤ëª…": f"í˜„ì¬ ê´€ê³„ ë°€ë„ê°€ {actual_density*100:.2f}%ë¡œ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤. ë” ë§ì€ ê´€ê³„ë¥¼ ì¶”ê°€í•˜ë©´ ê·¸ë˜í”„ì˜ ìœ ìš©ì„±ì´ í–¥ìƒë©ë‹ˆë‹¤.",
                    "ì‹¤í–‰ ë°©ë²•": "í…Œì´ë¸”ì •ì˜ì„œì—ì„œ FK ê´€ê³„ë¥¼ ì •ì˜í•˜ë©´ ìë™ìœ¼ë¡œ ì˜¨í†¨ë¡œì§€ ê´€ê³„ê°€ ìƒì„±ë©ë‹ˆë‹¤."
                })
    else:
        # ìŠ¤í‚¤ë§ˆ ê·¸ë˜í”„ ê°œì„  ì œì•ˆ
        schema = graph_data.get("schema", {})
        nodes = schema.get("nodes", [])
        links = schema.get("links", [])
        
        properties = [n for n in nodes if n.get("group") == "Property"]
        domain_links = [l for l in links if l.get("relation") == "domain"]
        range_links = [l for l in links if l.get("relation") == "range"]
        
        if properties:
            domain_completeness = len(domain_links) / len(properties)
            if domain_completeness < 0.8:
                suggestions.append({
                    "ìš°ì„ ìˆœìœ„": "ì¤‘ê°„",
                    "ì œì•ˆ": "Domain ì •ì˜ ë³´ì™„",
                    "ì„¤ëª…": f"ì†ì„±ì˜ {int((1-domain_completeness)*100)}%ê°€ domainì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    "ì‹¤í–‰ ë°©ë²•": "Enhanced Ontology Managerë¥¼ ì‚¬ìš©í•˜ë©´ ìë™ìœ¼ë¡œ domain/rangeê°€ ì„¤ì •ë©ë‹ˆë‹¤."
                })
            
            range_completeness = len(range_links) / len(properties)
            if range_completeness < 0.8:
                suggestions.append({
                    "ìš°ì„ ìˆœìœ„": "ì¤‘ê°„",
                    "ì œì•ˆ": "Range ì •ì˜ ë³´ì™„",
                    "ì„¤ëª…": f"ì†ì„±ì˜ {int((1-range_completeness)*100)}%ê°€ rangeê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    "ì‹¤í–‰ ë°©ë²•": "Enhanced Ontology Managerë¥¼ ì‚¬ìš©í•˜ë©´ ìë™ìœ¼ë¡œ domain/rangeê°€ ì„¤ì •ë©ë‹ˆë‹¤."
                })
    
    # ì œì•ˆ í‘œì‹œ
    if suggestions:
        for idx, suggestion in enumerate(suggestions, 1):
            priority_color = {
                "ë†’ìŒ": "ğŸ”´",
                "ì¤‘ê°„": "ğŸŸ¡",
                "ë‚®ìŒ": "ğŸŸ¢"
            }.get(suggestion.get("ìš°ì„ ìˆœìœ„", ""), "âšª")
            
            with st.expander(f"{priority_color} [{suggestion.get('ìš°ì„ ìˆœìœ„', '')}] {suggestion.get('ì œì•ˆ', '')}"):
                st.markdown(f"**ì„¤ëª…:** {suggestion.get('ì„¤ëª…', '')}")
                st.markdown(f"**ì‹¤í–‰ ë°©ë²•:** {suggestion.get('ì‹¤í–‰ ë°©ë²•', '')}")
    else:
        st.success("âœ… í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœê°€ ì–‘í˜¸í•©ë‹ˆë‹¤. íŠ¹ë³„í•œ ê°œì„  ì œì•ˆì´ ì—†ìŠµë‹ˆë‹¤.")

