# core_pipeline/ontology_manager_enhanced.py
# -*- coding: utf-8 -*-
"""
Enhanced Ontology Manager
í˜„ì¬ ì‹œìŠ¤í…œì˜ ì˜¨í†¨ë¡œì§€ ìƒì„± ë° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë¡œì§ í†µí•©
"""
import os
import sys
import json
import re
import hashlib
import shutil
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Union, Any
import pandas as pd
from pathlib import Path

# Windows ì½˜ì†” ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except Exception:
        pass

try:
    from rdflib import Graph, URIRef, Literal, Namespace, RDF, RDFS, OWL, XSD, BNode
    from rdflib.plugins.sparql import prepareQuery
    RDFLIB_AVAILABLE = True
except ImportError:
    RDFLIB_AVAILABLE = False
    from common.logger import get_logger
    logger = get_logger("OntologyManager")
    logger.warning("rdflib not installed. Ontology features will be limited.")


def _localname(u) -> str:
    """URIì—ì„œ ë¡œì»¬ ì´ë¦„ ì¶”ì¶œ (Enhanced)"""
    s = str(u)
    if '#' in s:
        return s.split('#')[-1]
    return s.split('/')[-1]


def _make_uri_safe(s: str) -> str:
    """ë¬¸ìì—´ì„ URI ì•ˆì „í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not s:
        return ""
    # ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ (ê³µë°±ì„ ì–¸ë”ë°”ë¡œ)
    s = str(s).strip().replace(" ", "_").replace("\t", "_").replace("\n", "_")
    # URIì— ë¶€ì í•©í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ì–¸ë”ë°”, ëŒ€ì‹œë§Œ í—ˆìš©)
    import re
    s = re.sub(r'[^\w\d_ê°€-í£\-]', '', s)
    return s


def _get_label(g: Graph, ns: Namespace, u) -> str:
    """
    ê·¸ë˜í”„ì—ì„œ ë…¸ë“œì˜ ë¼ë²¨ ê°€ì ¸ì˜¤ê¸°
    
    ì£¼ì˜: ì´ í•¨ìˆ˜ëŠ” rdfs:labelì„ ìš°ì„  ë°˜í™˜í•˜ì§€ë§Œ,
    to_json()ì—ì„œ IDì™€ ì¡°í•©í•˜ì—¬ í‘œì‹œí•˜ë¯€ë¡œ ì‹¤ì œ í‘œì‹œëŠ” ID ìš°ì„ ì´ ë©ë‹ˆë‹¤.
    """
    for _, _, lbl in g.triples((u, RDFS.label, None)):
        try:
            return str(lbl)
        except Exception:
            pass
    for _, _, lbl in g.triples((u, ns.name, None)):
        try:
            return str(lbl)
        except Exception:
            pass
    return _localname(u)


def safe_print(msg, also_log_file: bool = True):
    """ì•ˆì „í•œ ì¶œë ¥ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „ ì‚¬ìš©)"""
    from common.utils import safe_print as _safe_print
    _safe_print(msg, also_log_file=also_log_file, logger_name="OntologyManager")


# í…Œì´ë¸”ë³„ í‘œì¤€ ID ì»¬ëŸ¼ëª… ë§¤í•‘
STANDARD_ID_COLUMNS = {
    "ì„ë¬´ì •ë³´": ["ì„ë¬´ID", "mission_id", "ID"],
    "ì „ì¥ì¶•ì„ ": ["ì¶•ì„ ID", "axis_id", "ID"],
    "ì§€í˜•ì…€": ["ì§€í˜•ì…€ID", "terrain_cell_id", "ID"],
    "ì•„êµ°ë¶€ëŒ€í˜„í™©": ["ì•„êµ°ë¶€ëŒ€ID", "friendly_unit_id", "ID"],
    "ì êµ°ë¶€ëŒ€í˜„í™©": ["ì êµ°ë¶€ëŒ€ID", "enemy_unit_id", "ID"],
    "ìœ„í˜‘ìƒí™©": ["ìœ„í˜‘ID", "threat_id", "ID"],
    "ì œì•½ì¡°ê±´": ["ì œì•½ID", "constraint_id", "ID"],
    "COA_Library": ["COA_ID", "coa_id", "ID"],
    "ë°©ì±…ìœ í˜•_ìœ„í˜‘ìœ í˜•_ê´€ë ¨ì„±": ["coa_type"],
    "ì„ë¬´ë³„_ìì›í• ë‹¹": ["allocation_id", "ID"],
    "ê¸°ìƒìƒí™©": ["weather_id"],
}

# í…Œì´ë¸”ë³„ í‘œì¤€ ë¼ë²¨ ì»¬ëŸ¼ëª… ë§¤í•‘
STANDARD_LABEL_COLUMNS = {
    "ì„ë¬´ì •ë³´": ["ì„ë¬´ëª…", "mission_name", "name"],
    "ì „ì¥ì¶•ì„ ": ["ì¶•ì„ ëª…", "axis_name", "name"],
    "ì§€í˜•ì…€": ["ì§€í˜•ëª…", "terrain_name", "name"],
    "ì•„êµ°ë¶€ëŒ€í˜„í™©": ["ì•„êµ°ë¶€ëŒ€ëª…", "ë¶€ëŒ€ëª…", "unit_name", "name"],
    "ì êµ°ë¶€ëŒ€í˜„í™©": ["ì êµ°ë¶€ëŒ€ëª…", "ë¶€ëŒ€ëª…", "unit_name", "name"],
    "ìœ„í˜‘ìƒí™©": [],  # ë¼ë²¨ ì»¬ëŸ¼ ì—†ìŒ (IDë§Œ ì‚¬ìš©)
    "ì œì•½ì¡°ê±´": [],  # ë¼ë²¨ ì»¬ëŸ¼ ì—†ìŒ (IDë§Œ ì‚¬ìš©)
    "COA_Library": ["ëª…ì¹­", "name", "coa_name"],
    "ì„ë¬´ë³„_ìì›í• ë‹¹": ["resource_alias", "resource_name", "name"],
}


def suggest_id_column(table_name: str, columns: List[str]) -> str:
    """
    í…Œì´ë¸”ì˜ ì‹ë³„ì ì»¬ëŸ¼ì„ ìë™ ì œì•ˆ (ê°œì„  ë²„ì „)
    
    ìš°ì„ ìˆœìœ„:
    1. í…Œì´ë¸”ë³„ í‘œì¤€ ID ì»¬ëŸ¼ëª…
    2. íŒ¨í„´ ê¸°ë°˜ ê°ì§€
    3. ì²« ë²ˆì§¸ ì»¬ëŸ¼ (í´ë°±)
    """
    # 1. í…Œì´ë¸”ë³„ í‘œì¤€ ID ì»¬ëŸ¼ëª… ìš°ì„  í™•ì¸
    if table_name in STANDARD_ID_COLUMNS:
        for standard_col in STANDARD_ID_COLUMNS[table_name]:
            if standard_col in columns:
                # ğŸ”¥ ìµœì í™”: ë°˜ë³µ ë¡œê·¸ ì œê±° (200íšŒ ì´ìƒ ì¶œë ¥ ë°©ì§€)
                # safe_print(f"[DEBUG] í‘œì¤€ ID ì»¬ëŸ¼ ê°ì§€: {table_name}.{standard_col}")
                return standard_col
    
    # 2. íŒ¨í„´ ê¸°ë°˜ ê°ì§€ (ê¸°ì¡´ ë¡œì§ ê°œì„ )
    patterns = [
        r"^.*_id$|^.*_key$|^id$",  # ì˜ë¬¸ í˜•ì‹ (mission_id, axis_id ë“±)
        r"^.*id$|^.*ID$",  # ëŒ€ì†Œë¬¸ì ëª¨ë‘ ì¸ì‹ (ì„ë¬´ID, ì¶•ì„ ID ë“±)
        r"ì‹ë³„ì|í‚¤",
    ]
    for col in columns:
        col_l = col.lower()
        for p in patterns:
            if re.search(p, col_l):
                # ğŸ”¥ ìµœì í™”: ë°˜ë³µ ë¡œê·¸ ì œê±°
                # safe_print(f"[DEBUG] íŒ¨í„´ ê¸°ë°˜ ID ì»¬ëŸ¼ ê°ì§€: {table_name}.{col}")
                return col
    
    # 3. í´ë°±: ì²« ë²ˆì§¸ ì»¬ëŸ¼
    fallback_col = columns[0] if columns else ""
    if fallback_col:
        safe_print(f"[WARN] {table_name} í…Œì´ë¸”ì—ì„œ ID ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í•´ ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©: {fallback_col}")
    return fallback_col


def suggest_label_column(table_name: str, columns: List[str]) -> str:
    """
    í…Œì´ë¸”ì˜ ë¼ë²¨ ì»¬ëŸ¼ì„ ìë™ ì œì•ˆ (ID ì»¬ëŸ¼ ì œì™¸)
    
    Args:
        table_name: í…Œì´ë¸”ëª…
        columns: ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    """
    # ID ì»¬ëŸ¼ ê°ì§€ (ì œì™¸ ëŒ€ìƒ)
    id_col = suggest_id_column(table_name, columns)
    
    # ID ì»¬ëŸ¼ ì œì™¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    exclude_columns = set()
    if id_col:
        exclude_columns.add(id_col)
    
    # ID íŒ¨í„´ì„ í¬í•¨í•˜ëŠ” ì»¬ëŸ¼ë„ ì œì™¸
    id_patterns = [
        r"^.*_id$|^.*_key$|^id$",  # ì˜ë¬¸ í˜•ì‹
        r"^.*id$|^.*ID$",  # ëŒ€ì†Œë¬¸ì ëª¨ë‘
        r"ì‹ë³„ì|í‚¤",
    ]
    for col in columns:
        if col in exclude_columns:
            continue
        col_l = col.lower()
        for p in id_patterns:
            if re.search(p, col_l):
                exclude_columns.add(col)
                break
    
    # 1. í…Œì´ë¸”ë³„ í‘œì¤€ ë¼ë²¨ ì»¬ëŸ¼ëª… ìš°ì„  í™•ì¸
    if table_name in STANDARD_LABEL_COLUMNS:
        for standard_col in STANDARD_LABEL_COLUMNS[table_name]:
            if standard_col in columns and standard_col not in exclude_columns:
                safe_print(f"[DEBUG] í‘œì¤€ ë¼ë²¨ ì»¬ëŸ¼ ê°ì§€: {table_name}.{standard_col}")
                return standard_col
        # í‘œì¤€ ë¼ë²¨ ì»¬ëŸ¼ì´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ ë¼ë²¨ ì»¬ëŸ¼ ì—†ìŒ (IDë§Œ ì‚¬ìš©)
        if STANDARD_LABEL_COLUMNS[table_name] == []:
            safe_print(f"[DEBUG] {table_name} í…Œì´ë¸”ì€ ë¼ë²¨ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤ (IDë§Œ ì‚¬ìš©)")
            return ""
    
    # 2. íŒ¨í„´ ê¸°ë°˜ ê°ì§€ (ID ì»¬ëŸ¼ ì œì™¸)
    patterns = [
        r"name|label|title|ëª…|ì´ë¦„",
        r"description|ì„¤ëª…|ë‚´ìš©",
        r"^.*_name$|^.*_label$",
    ]
    for col in columns:
        if col in exclude_columns:
            continue
        col_l = col.lower()
        for p in patterns:
            if re.search(p, col_l):
                safe_print(f"[DEBUG] íŒ¨í„´ ê¸°ë°˜ ë¼ë²¨ ì»¬ëŸ¼ ê°ì§€: {table_name}.{col}")
                return col
    
    # 3. íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (IDë§Œ ì‚¬ìš©)
    safe_print(f"[DEBUG] {table_name} í…Œì´ë¸”ì—ì„œ ë¼ë²¨ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í•´ IDë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤")
    return ""


class EnhancedOntologyManager:
    """ê°•í™”ëœ ì˜¨í†¨ë¡œì§€ ê´€ë¦¬ì (í˜„ì¬ ì‹œìŠ¤í…œ ë¡œì§ í†µí•©)"""
    
    # ì˜ë¬¸ ê´€ê³„ëª… ë§¤í•‘ í…Œì´ë¸” (í•œê¸€ í…Œì´ë¸”ëª… -> ì˜ë¬¸ ê´€ê³„ëª…)
    RELATION_NAME_MAPPING = {
        "ì„ë¬´ì •ë³´": "hasMission",
        "ì§€í˜•ì…€": "locatedIn",
        "ì „ì¥ì¶•ì„ ": "hasAxis",
        "ì êµ°ë¶€ëŒ€í˜„í™©": "hasEnemyUnit",
        "ì•„êµ°ë¶€ëŒ€í˜„í™©": "hasFriendlyUnit",
        "ìœ„í˜‘ìƒí™©": "hasThreat",
        "ì œì•½ì¡°ê±´": "appliesTo"
    }
    
    # ì „ëµìœ í˜• ê°’ ë§¤í•‘ (ì˜ë¬¸ â†” í•œê¸€)
    STRATEGY_TYPE_MAPPING = {
        'offensive': ['ê³µê²©', 'offensive'],
        'defensive': ['ë°©ì–´', 'defensive'],
        'ê³µê²©': ['offensive', 'ê³µê²©'],
        'ë°©ì–´': ['defensive', 'ë°©ì–´']
    }
    
    def __init__(self, config: Dict):
        """
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        
        # [NEW] í†µê³„ìš© ì¹´ìš´í„°
        self.virtual_entities_count = 0
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì§ì ‘ ì´ˆê¸°í™” (base ì˜ì¡´ì„± ì œê±°)
        if RDFLIB_AVAILABLE:
            self.graph = Graph()
            # í†µì¼ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚¬ìš© (COA Agent Platform)
            self.ns = Namespace("http://coa-agent-platform.org/ontology#")
            self.ns_legacy = Namespace("http://coa-agent-platform.org/ontology#")  # Legacy alias updated to match standard
            # [NEW] ê°€ìƒ ì—”í‹°í‹° ì „ìš© ë„¤ì„ìŠ¤í˜ì´ìŠ¤
            self.virtual_ns = Namespace("http://coa-agent-platform.org/ontology/virtual#")
        else:
            self.graph = None
            self.ns = None
            self.ns_legacy = None
            self.virtual_ns = None # Ensure virtual_ns is also None if RDFLib is not available
        
        # [INFO] ì´ˆê¸°í™” ì™„ë£Œ ë©”ì‹œì§€
        # safe_print(f"[INFO] EnhancedOntologyManager ì´ˆê¸°í™” ì™„ë£Œ")
        
        # OntologyManagerì™€ ë™ì¼í•œ ì†ì„± ì¶”ê°€
        self.ontology_path = config.get("ontology_path", "./knowledge/ontology")
        # data_managerëŠ” ë‚˜ì¤‘ì— ì„¤ì • (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        self.data_manager = None
        
        # ë©”íƒ€ë°ì´í„° ê²½ë¡œ
        self.metadata_path = config.get("metadata_path", "./metadata")
        self.data_lake_path = config.get("data_lake_path", "./data_lake")
        self.output_path = config.get("output_path", "./outputs")
        
        # ì¶”ë¡  ì—¬ë¶€ íŒë‹¨ì„ ìœ„í•œ ì›ë³¸ ê·¸ë˜í”„ í¬ê¸° ì¶”ì 
        self._original_graph_size = None  # instances.ttl ë¡œë“œ ì§í›„ì˜ ì›ë³¸ í¬ê¸°
        self._inference_performed = False  # ì¶”ë¡  ì‹¤í–‰ ì—¬ë¶€ í”Œë˜ê·¸
        
        # ì¶”ë¡  ì‹¤í–‰ ì—¬ë¶€ í”Œë˜ê·¸ (ì¤‘ë³µ ì¶”ë¡  ë°©ì§€)
        self._inference_performed = False
        
        # ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë¡œë“œ
        self.schema_registry = self._load_schema_registry()
        
        # ê¸°ì¡´ ê·¸ë˜í”„ ìë™ ë¡œë“œ ì‹œë„
        self.try_load_existing_graph()
        
        # ê´€ê³„ ë§¤í•‘ ìºì‹œ
        self._relation_mappings = None
        self._relation_mappings_cache_time = {}  # íŒŒì¼ë³„ ìˆ˜ì • ì‹œê°„ ìºì‹œ
        
        # [NEW] JSON ì§ë ¬í™” ìºì‹œ
        self._json_cache = None
        self._last_graph_hash = None

    def _load_schema_registry(self) -> Dict:
        """Schema Registry ë¡œë“œ (YAML)"""
        import yaml
        registry_path = os.path.join(self.metadata_path, "schema_registry.yaml")
        if os.path.exists(registry_path):
            try:
                with open(registry_path, 'r', encoding='utf-8') as f:
                    return yaml.safe_load(f).get('tables', {})
            except Exception as e:
                safe_print(f"[WARN] Schema Registry ë¡œë“œ ì‹¤íŒ¨: {e}")
                return {}
        return {}

    def get_id_column(self, table_name: str, columns: List[str]) -> str:
        """ID ì»¬ëŸ¼ ì¡°íšŒ (Schema Registry ìš°ì„ )"""
        # 1. Schema Registry í™•ì¸
        if table_name in self.schema_registry:
            table_info = self.schema_registry[table_name]
            for col_name, col_info in table_info.get('columns', {}).items():
                if col_info.get('pk'):
                    if col_name in columns:
                        return col_name
                    # ëŒ€ì†Œë¬¸ì ì°¨ì´ ë“±ìœ¼ë¡œ ëª» ì°¾ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì»¬ëŸ¼ ëª©ë¡ì—ì„œ ê²€ìƒ‰
                    for c in columns:
                        if c.lower() == col_name.lower():
                            return c
        
        # 2. ê¸°ì¡´ ë¡œì§ í´ë°±
        return suggest_id_column(table_name, columns)

    def get_label_column(self, table_name: str, columns: List[str]) -> str:
        """Label ì»¬ëŸ¼ ì¡°íšŒ (Schema Registry ìš°ì„ )"""
        # 1. Schema Registry í™•ì¸
        if table_name in self.schema_registry:
            table_info = self.schema_registry[table_name]
            for col_name, col_info in table_info.get('columns', {}).items():
                if col_info.get('label'):
                    if col_name in columns:
                        return col_name
                    for c in columns:
                        if c.lower() == col_name.lower():
                            return c
        
        # 2. ê¸°ì¡´ ë¡œì§ í´ë°±
        return suggest_label_column(table_name, columns)
    
    
    def get_schema_summary(self) -> str:
        """
        [NEW] ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ìš”ì•½ ì •ë³´ ì¶”ì¶œ (LLM í”„ë¡¬í”„íŠ¸ìš©)
        ê·¸ë˜í”„ì— ì •ì˜ëœ í´ë˜ìŠ¤ì™€ ì£¼ìš” ì†ì„±ì„ ìš”ì•½í•˜ì—¬ ë¬¸ìì—´ë¡œ ë°˜í™˜
        """
        if not self.graph:
            return "Ontology graph is empty."
        
        summary = ["# Ontology Schema Summary"]
        
        try:
            # 1. ì£¼ìš” í´ë˜ìŠ¤ ì¶”ì¶œ
            query_classes = """
            SELECT DISTINCT ?type WHERE {
                ?s a ?type .
                FILTER(STRSTARTS(STR(?type), STR(def:)))
            }
            LIMIT 20
            """
            
            # 2. ì£¼ìš” ì†ì„± ì¶”ì¶œ (ObjectProperty & DatatypeProperty)
            query_props = """
            SELECT DISTINCT ?prop WHERE {
                ?s ?prop ?o .
                FILTER(STRSTARTS(STR(?prop), STR(def:)))
            }
            LIMIT 30
            """
            
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”© (ì¿¼ë¦¬ì— def: ì‚¬ìš©ì„ ìœ„í•´ í•„ìš”í•  ìˆ˜ ìˆìŒ, ì—¬ê¸°ì„  ì „ì²´ URI ì‚¬ìš©í•˜ê±°ë‚˜ bind í•„ìš”)
            # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ query ë©”ì„œë“œ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ê±°ë‚˜, í’€ URI ë§¤ì¹­ ë“± ê³ ë ¤
            # ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ graph.predicates(), graph.objects() ë“± í™œìš© ê°€ëŠ¥í•˜ë‚˜ SPARQLì´ í™•ì‹¤í•¨
            
            # í´ë˜ìŠ¤ ìˆ˜ì§‘ (ê°„ì†Œí™”)
            classes = set()
            for s, p, o in self.graph.triples((None, RDF.type, None)):
                if str(o).startswith(str(self.ns)):
                    classes.add(_localname(o))
            
            summary.append(f"## Classes ({len(classes)}):")
            summary.append(", ".join(sorted(list(classes))))
            
            # ì†ì„± ìˆ˜ì§‘ (Domain/Range í¬í•¨í•˜ë©´ ì¢‹ìœ¼ë‚˜ ì¼ë‹¨ ì´ë¦„ë§Œ)
            props = set()
            for s, p, o in self.graph:
                if str(p).startswith(str(self.ns)):
                    props.add(_localname(p))
            
            summary.append(f"\n## Properties ({len(props)}):")
            summary.append(", ".join(sorted(list(props))))
            
            # ì£¼ìš” ê´€ê³„ ìƒ˜í”Œ (Few-shot)
            summary.append("\n## Relationships:")
            relationships = [
                "def:OptimizationGoal -> def:hasMechanism -> def:Mechanism",
                "def:COA -> def:respondsTo -> def:ThreatEvent",
                "def:Unit -> def:hasType -> xsd:string",
                "def:Terrain -> def:hasEffect -> def:Effect"
            ]
            summary.extend([f"- {r}" for r in relationships])
            
        except Exception as e:
            summary.append(f"Error extracting schema: {e}")
            
        return "\n".join(summary)

    def load_relation_mappings(self, force_reload: bool = False) -> List[Dict]:
        """
        ê´€ê³„ ë§¤í•‘ ë¡œë“œ (Schema Registry í†µí•©)
        """
        # ê°•ì œ ì¬ë¡œë“œê°€ ì•„ë‹ˆê³  ìºì‹œê°€ ìˆìœ¼ë©´ ë°˜í™˜
        if not force_reload and self._relation_mappings is not None:
            return self._relation_mappings
        
        relation_mappings = []
        
        # 1. Schema Registryì—ì„œ ê´€ê³„ ë¡œë“œ (ìµœìš°ì„ )
        for table_name, table_info in self.schema_registry.items():
            relations = table_info.get('relations', [])
            for rel in relations:
                mapping = {
                    "src_table": table_name,
                    "src_col": rel.get('source_col'),
                    "tgt_table": rel.get('target_table'),
                    "relation": rel.get('name'),
                    "source": "schema_registry"
                }
                
                # ë™ì  ë§¤í•‘ ì²˜ë¦¬
                if rel.get('target_table') == 'dynamic':
                    mapping['dynamic'] = True
                    mapping['type_col'] = rel.get('type_col')
                    mapping['type_mapping'] = rel.get('type_mapping')
                
                # ì¶”ë¡  ê´€ê³„ ì²˜ë¦¬
                if rel.get('type') == 'inference':
                    mapping['inferred'] = True
                    mapping['confidence'] = rel.get('confidence', 0.8)
                
                relation_mappings.append(mapping)
        
        # 2. ê¸°ì¡´ relation_mappings.json ë¡œë“œ (í•˜ìœ„ í˜¸í™˜ì„±)
        # Schema Registryì— ì—†ëŠ” í…Œì´ë¸”ë§Œ ì¶”ê°€
        existing_tables = set(self.schema_registry.keys())
        
        rel_mapping_path = os.path.join(self.metadata_path, "relation_mappings.json")
        if os.path.exists(rel_mapping_path):
            try:
                with open(rel_mapping_path, 'r', encoding='utf-8') as f:
                    mapping_data = json.load(f)
                
                if isinstance(mapping_data, dict):
                    for src_table, col_mappings in mapping_data.items():
                        if src_table in existing_tables:
                            continue  # ì´ë¯¸ Registryì—ì„œ ë¡œë“œí•¨
                            
                        for src_col, mapping_value in col_mappings.items():
                            # ë™ì  FK ê´€ê³„ ì²˜ë¦¬
                            if isinstance(mapping_value, dict) and mapping_value.get("type_column"):
                                relation_mappings.append({
                                    "src_table": src_table,
                                    "src_col": src_col,
                                    "tgt_table": mapping_value.get("target", "ë™ì "),
                                    "tgt_col": None,
                                    "relation": mapping_value.get("relation", "appliesTo"),
                                    "type_column": mapping_value.get("type_column"),
                                    "type_mapping": mapping_value.get("type_mapping", {}),
                                    "dynamic": True,
                                    "inferred": False,
                                    "confidence": 1.0,
                                    "source": "relation_mappings.json"
                                })
                                continue
                            
                            # ê°ì²´ í˜•íƒœ FK ê´€ê³„ ì²˜ë¦¬ (ì‚¬ìš©ì ì§€ì • ê´€ê³„ëª…)
                            if isinstance(mapping_value, dict):
                                # relation í•„ë“œê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                                relation_name = mapping_value.get("relation")
                                if not relation_name:
                                    target_table = mapping_value.get("target")
                                    relation_name = f"has{target_table}"  # ê¸°ë³¸ê°’: has{í…Œì´ë¸”ëª…}
                                
                                relation_mappings.append({
                                    "src_table": src_table,
                                    "src_col": src_col,
                                    "tgt_table": mapping_value.get("target"),
                                    "tgt_col": None,
                                    "relation": relation_name,
                                    "inferred": False,
                                    "confidence": 1.0,
                                    "source": "relation_mappings.json"
                                })
                                continue
                            
                            # ì¼ë°˜ FK ê´€ê³„ ì²˜ë¦¬ (ë‹¨ìˆœ ë¬¸ìì—´)
                            if isinstance(mapping_value, str):
                                # ê¸°ë³¸ê°’: has{í…Œì´ë¸”ëª…} (ì‚¬ìš©ìê°€ relation_mappings.jsonì— ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´)
                                relation_mappings.append({
                                    "src_table": src_table,
                                    "src_col": src_col,
                                    "tgt_table": mapping_value,
                                    "tgt_col": None,
                                    "relation": f"has{mapping_value}",
                                    "inferred": False,
                                    "confidence": 1.0,
                                    "source": "relation_mappings.json"
                                })
                
                elif isinstance(mapping_data, list):
                    relation_mappings = mapping_data
                    for rel_map in relation_mappings:
                        rel_map["source"] = "relation_mappings.json"
            
            except Exception as e:
                safe_print(f"ê´€ê³„ ë§¤í•‘ ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        # 2. schema_registry.yamlì—ì„œ FK ì •ë³´ ë¡œë“œ ë° í†µí•© (í…Œì´ë¸”ì •ì˜ì„œ ëŒ€ì²´)
        try:
            # schema_registry.yamlì˜ relations ì„¹ì…˜ì—ì„œ FK ì •ë³´ ì¶”ì¶œ
            for table_name, table_info in self.schema_registry.items():
                if not isinstance(table_info, dict):
                    continue
                
                # relations ì„¹ì…˜ í™•ì¸
                relations = table_info.get('relations', [])
                if not relations:
                    continue
                
                for relation in relations:
                    if not isinstance(relation, dict):
                        continue
                    
                    source_col = relation.get('source_col')
                    target_table = relation.get('target_table')
                    relation_name = relation.get('name')
                    
                    if not source_col or not target_table:
                        continue
                    
                        # ì¤‘ë³µ ì²´í¬ (ì´ë¯¸ relation_mappings.jsonì— ìˆëŠ” ê´€ê³„ëŠ” ì œì™¸)
                        is_duplicate = False
                        for existing_rel in relation_mappings:
                            if (existing_rel.get('src_table') == table_name and 
                            existing_rel.get('src_col') == source_col):
                                is_duplicate = True
                                break
                        
                        if not is_duplicate:
                            # ê´€ê³„ëª…ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                            if not relation_name:
                                relation_name = f"has{target_table}"
                            
                            relation_mappings.append({
                                "src_table": table_name,
                            "src_col": source_col,
                            "tgt_table": target_table,
                            "tgt_col": None,  # schema_registry.yamlì—ëŠ” target_col ì •ë³´ê°€ ì—†ìŒ
                                "relation": relation_name,
                                "inferred": False,
                                "confidence": 1.0,
                            "source": "schema_registry.yaml"
                            })
                        safe_print(f"[INFO] schema_registry.yamlì—ì„œ FK ë°œê²¬: {table_name}.{source_col} -> {target_table} (ê´€ê³„ëª…: {relation_name})")
        
        except Exception as e:
            safe_print(f"[WARN] schema_registry.yaml FK ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        # ìºì‹œ ì‹œê°„ ì—…ë°ì´íŠ¸
        self._update_cache_times()
        
        self._relation_mappings = relation_mappings
        return relation_mappings
    
    def _check_files_changed(self) -> bool:
        """ê´€ê³„ ë§¤í•‘ ê´€ë ¨ íŒŒì¼ë“¤ì˜ ë³€ê²½ ì‹œê°„ í™•ì¸"""
        try:
            # 1. relation_mappings.json í™•ì¸
            rel_mapping_path = os.path.join(self.metadata_path, "relation_mappings.json")
            if os.path.exists(rel_mapping_path):
                current_mtime = os.path.getmtime(rel_mapping_path)
                cached_mtime = self._relation_mappings_cache_time.get(rel_mapping_path)
                if cached_mtime is None or current_mtime > cached_mtime:
                    return True
            
            # 2. schema_registry.yaml í™•ì¸
            schema_registry_path = os.path.join(self.metadata_path, "schema_registry.yaml")
            if os.path.exists(schema_registry_path):
                current_mtime = os.path.getmtime(schema_registry_path)
                cached_mtime = self._relation_mappings_cache_time.get(schema_registry_path)
                if cached_mtime is None or current_mtime > cached_mtime:
                    return True
            
            return False
        except Exception as e:
            safe_print(f"[WARN] íŒŒì¼ ë³€ê²½ í™•ì¸ ì˜¤ë¥˜: {e}")
            return True  # ì˜¤ë¥˜ ì‹œ ì¬ë¡œë“œ
    
    def _update_cache_times(self):
        """ìºì‹œ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        try:
            # relation_mappings.json
            rel_mapping_path = os.path.join(self.metadata_path, "relation_mappings.json")
            if os.path.exists(rel_mapping_path):
                self._relation_mappings_cache_time[rel_mapping_path] = os.path.getmtime(rel_mapping_path)
            
            # schema_registry.yaml
            schema_registry_path = os.path.join(self.metadata_path, "schema_registry.yaml")
            if os.path.exists(schema_registry_path):
                self._relation_mappings_cache_time[schema_registry_path] = os.path.getmtime(schema_registry_path)
        except Exception as e:
            safe_print(f"[WARN] ìºì‹œ ì‹œê°„ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def clear_relation_mappings_cache(self):
        """ê´€ê³„ ë§¤í•‘ ìºì‹œ ë¬´íš¨í™”"""
        self._relation_mappings = None
        self._relation_mappings_cache_time = {}
        safe_print("[INFO] ê´€ê³„ ë§¤í•‘ ìºì‹œê°€ ë¬´íš¨í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ========== Schema Auto-Sync Methods (NEW) ==========
    
    def _infer_dtype(self, series: pd.Series) -> str:
        """
        pandas Seriesì—ì„œ YAML íƒ€ì… ì¶”ë¡ 
        
        Args:
            series: pandas Series
            
        Returns:
            "string", "number", "datetime", "boolean" ì¤‘ í•˜ë‚˜
        """
        dtype = str(series.dtype)
        
        if 'int' in dtype or 'float' in dtype:
            return "number"
        elif 'datetime' in dtype:
            return "datetime"
        elif 'bool' in dtype:
            return "boolean"
        else:
            return "string"
    
    def _infer_fk_target(self, col_name: str) -> Optional[str]:
        """
        ì»¬ëŸ¼ëª…ì—ì„œ FK ëŒ€ìƒ í…Œì´ë¸” ì¶”ë¡ 
        
        Args:
            col_name: ì»¬ëŸ¼ëª… (ì˜ˆ: "ì„ë¬´ID", "mission_id")
            
        Returns:
            FK ëŒ€ìƒ ë¬¸ìì—´ (ì˜ˆ: "ì„ë¬´ì •ë³´.ì„ë¬´ID") ë˜ëŠ” None
        """
        # FK íŒ¨í„´: {í…Œì´ë¸”ëª…}ID -> {í…Œì´ë¸”ëª…}
        patterns = {
            'ì„ë¬´ID': 'ì„ë¬´ì •ë³´.ì„ë¬´ID',
            'ì¶•ì„ ID': 'ì „ì¥ì¶•ì„ .ì¶•ì„ ID',
            'ì§€í˜•ì…€ID': 'ì§€í˜•ì…€.ì§€í˜•ì…€ID',
            'ì•„êµ°ë¶€ëŒ€ID': 'ì•„êµ°ë¶€ëŒ€í˜„í™©.ë¶€ëŒ€ID',
            'ì êµ°ë¶€ëŒ€ID': 'ì êµ°ë¶€ëŒ€í˜„í™©.ë¶€ëŒ€ID',
            'ìœ„í˜‘ID': 'ìœ„í˜‘ìƒí™©.ìœ„í˜‘ID',
            'ì œì•½ID': 'ì œì•½ì¡°ê±´.ì œì•½ID',
            'mission_id': 'ì„ë¬´ì •ë³´.ì„ë¬´ID',
            'axis_id': 'ì „ì¥ì¶•ì„ .ì¶•ì„ ID',
            'terrain_cell_id': 'ì§€í˜•ì…€.ì§€í˜•ì…€ID',
            'friendly_unit_id': 'ì•„êµ°ë¶€ëŒ€í˜„í™©.ë¶€ëŒ€ID',
            'enemy_unit_id': 'ì êµ°ë¶€ëŒ€í˜„í™©.ë¶€ëŒ€ID',
            'threat_id': 'ìœ„í˜‘ìƒí™©.ìœ„í˜‘ID',
        }
        
        # ì§ì ‘ ë§¤í•‘ í™•ì¸
        if col_name in patterns:
            return patterns[col_name]
        
        # íŒ¨í„´ ê¸°ë°˜ ì¶”ë¡  (ë¶€ë¶„ ë§¤ì¹­)
        for pattern, target in patterns.items():
            if pattern.lower() in col_name.lower():
                return target
        
        return None
    
    def _save_schema_registry(self, registry_path: str):
        """
        schema_registryë¥¼ YAML íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            registry_path: YAML íŒŒì¼ ê²½ë¡œ
        """
        import yaml
        
        try:
            # ê¸°ì¡´ íŒŒì¼ ë°±ì—…
            if os.path.exists(registry_path):
                backup_path = registry_path + '.backup'
                shutil.copy2(registry_path, backup_path)
                safe_print(f"[INFO] ê¸°ì¡´ schema_registry.yaml ë°±ì—…: {backup_path}")
            
            # YAML ì €ì¥
            with open(registry_path, 'w', encoding='utf-8') as f:
                yaml_data = {
                    'version': '1.1',
                    'last_updated': datetime.now().strftime('%Y-%m-%d'),
                    'tables': self.schema_registry
                }
                yaml.dump(yaml_data, f, allow_unicode=True, sort_keys=False)
            
            safe_print(f"[INFO] schema_registry.yaml ì €ì¥ ì™„ë£Œ: {registry_path}")
            
        except Exception as e:
            safe_print(f"[ERROR] schema_registry.yaml ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def _infer_schema(self, table_name: str, df: pd.DataFrame) -> Dict:
        """
        DataFrameì—ì„œ ìŠ¤í‚¤ë§ˆ ìë™ ì¶”ë¡ 
        
        Args:
            table_name: í…Œì´ë¸”ëª…
            df: pandas DataFrame
            
        Returns:
            ìŠ¤í‚¤ë§ˆ ë”•ì…”ë„ˆë¦¬
        """
        schema = {
            'description': f'{table_name} (ìë™ ìƒì„±)',
            'file_name': f'{table_name}.xlsx',
            'columns': {}
        }
        
        # PK ë° ë¼ë²¨ ì»¬ëŸ¼ ì¶”ë¡ 
        columns_list = list(df.columns)
        id_col = self.get_id_column(table_name, columns_list)
        label_col = self.get_label_column(table_name, columns_list)
        
        for col in df.columns:
            col_info = {'type': self._infer_dtype(df[col])}
            
            if col == id_col:
                col_info['pk'] = True
            if col == label_col and label_col:
                col_info['label'] = True
            
            # FK ì¶”ë¡ 
            fk_target = self._infer_fk_target(col)
            if fk_target:
                col_info['fk'] = fk_target
            
            schema['columns'][col] = col_info
        
        safe_print(f"[INFO] {table_name} ìŠ¤í‚¤ë§ˆ ìë™ ì¶”ë¡  ì™„ë£Œ (PK: {id_col}, ë¼ë²¨: {label_col if label_col else 'N/A'})")
        return schema
    
    def _sync_schema_registry(self, data: Dict[str, pd.DataFrame], 
                               auto_update: bool = True) -> Dict:
        """
        ì‹¤ì œ ë°ì´í„°ì™€ schema_registry.yaml ë™ê¸°í™”
        
        Args:
            data: {í…Œì´ë¸”ëª…: DataFrame} ë”•ì…”ë„ˆë¦¬
            auto_update: Trueì¼ ê²½ìš° ìë™ìœ¼ë¡œ YAML íŒŒì¼ ì—…ë°ì´íŠ¸
            
        Returns:
            {
                'has_changes': bool,
                'new_tables': List[str],
                'updated_tables': List[str],
                'summary': str
            }
        """
        # 1. í˜„ì¬ ë“±ë¡ëœ í…Œì´ë¸” ëª©ë¡
        registered_tables = set(self.schema_registry.keys())
        
        # 2. ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í…Œì´ë¸” ëª©ë¡
        actual_tables = set(data.keys())
        
        # 3. ì°¨ì´ ë¶„ì„
        new_tables = actual_tables - registered_tables
        
        if not new_tables:
            return {
                'has_changes': False,
                'new_tables': [],
                'updated_tables': [],
                'summary': 'ë³€ê²½ì‚¬í•­ ì—†ìŒ'
            }
        
        safe_print(f"[INFO] ì‹ ê·œ í…Œì´ë¸” {len(new_tables)}ê°œ ë°œê²¬: {list(new_tables)}")
        
        # 4. ì‹ ê·œ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ìë™ ìƒì„±
        for table_name in new_tables:
            df = data[table_name]
            auto_schema = self._infer_schema(table_name, df)
            self.schema_registry[table_name] = auto_schema
            safe_print(f"[INFO] {table_name} ìŠ¤í‚¤ë§ˆ ìë™ ë“±ë¡ ì™„ë£Œ")
        
        # 5. YAML íŒŒì¼ ì—…ë°ì´íŠ¸ (ì˜µì…˜)
        if auto_update:
            registry_path = os.path.join(self.metadata_path, "schema_registry.yaml")
            self._save_schema_registry(registry_path)
        
        return {
            'has_changes': True,
            'new_tables': sorted(list(new_tables)),
            'updated_tables': [],
            'summary': f"ì‹ ê·œ í…Œì´ë¸” {len(new_tables)}ê°œ ë“±ë¡: {', '.join(sorted(list(new_tables)))}"
        }
    
    def _load_fk_from_schema(self, table_name: str, excel_file: Path) -> List[Dict]:
        """í…Œì´ë¸”ì •ì˜ì„œì—ì„œ FK ì •ë³´ ì¶”ì¶œ"""
        fk_list = []
        
        try:
            # ì—‘ì…€ íŒŒì¼ì˜ ì‹œíŠ¸ ëª©ë¡ í™•ì¸
            excel_file_obj = pd.ExcelFile(excel_file)
            sheet_names = excel_file_obj.sheet_names
            
            # í…Œì´ë¸”ì •ì˜ì„œ ì‹œíŠ¸ ì°¾ê¸°
            schema_sheet = None
            for sheet in sheet_names:
                if "ì •ì˜ì„œ" in sheet or "schema" in sheet.lower() or "ì •ì˜" in sheet:
                    schema_sheet = sheet
                    break
            
            if not schema_sheet:
                return fk_list
            
            # í…Œì´ë¸”ì •ì˜ì„œ ì½ê¸°
            schema_df = pd.read_excel(excel_file, sheet_name=schema_sheet)
            
            # ì»¬ëŸ¼ëª… ì •ê·œí™”
            field_col = None
            fk_col = None
            relation_col = None
            
            for col in schema_df.columns:
                col_lower = str(col).lower()
                col_str = str(col)
                if "í•„ë“œ" in col_str or "field" in col_lower or "ì»¬ëŸ¼" in col_str:
                    field_col = col
                elif col_str == "FK" or col_lower == "fk":
                    fk_col = col
                elif "ê´€ê³„" in col_str or "relation" in col_lower:
                    relation_col = col
            
            if not field_col:
                return fk_list
            
            # FK ì •ë³´ ì¶”ì¶œ
            for idx, row in schema_df.iterrows():
                field_name = str(row[field_col]).strip() if field_col in row else ""
                if pd.isna(field_name) or field_name == "":
                    continue
                
                # FK ì»¬ëŸ¼ì—ì„œ Y ê°’ í™•ì¸
                if fk_col and fk_col in row:
                    fk_value = row[fk_col]
                    if not pd.isna(fk_value) and str(fk_value).upper() in ['Y', 'YES', 'TRUE', '1', 'ì˜ˆ', 'O']:
                        # ê´€ê³„ ì»¬ëŸ¼ì—ì„œ FK ê´€ê³„ ì •ë³´ ì¶”ì¶œ
                        if relation_col and relation_col in row:
                            relation = str(row[relation_col]) if not pd.isna(row[relation_col]) else ""
                            if relation and relation.strip():
                                # ì§€ì› í˜•ì‹:
                                # 1. "ì „ì¥ì¶•ì„ :ì¶•ì„ ID" (ì‹¤ì œ ì‚¬ìš© í˜•ì‹, ì½œë¡  êµ¬ë¶„)
                                # 2. "ì „ì¥ì¶•ì„ .ì¶•ì„ ID" (ì  êµ¬ë¶„)
                                # 3. "FKâ†’ì „ì¥ì¶•ì„ .ì¶•ì„ ID" (FKâ†’ ì ‘ë‘ì‚¬ í¬í•¨, ì  êµ¬ë¶„)
                                # ì˜ˆ: ì „ì¥ì¶•ì„ :ì¶•ì„ ID, ì§€í˜•ì…€:ì§€í˜•ì…€ID
                                
                                # FKâ†’ ì ‘ë‘ì‚¬ ì œê±° (ìˆìœ¼ë©´)
                                relation_clean = re.sub(r'^FK\s*â†’\s*', '', relation, flags=re.IGNORECASE)
                                relation_clean = relation_clean.strip()
                                
                                # í…Œì´ë¸”ëª…ê³¼ ì»¬ëŸ¼ëª… ì¶”ì¶œ (ì½œë¡  ë˜ëŠ” ì ìœ¼ë¡œ êµ¬ë¶„)
                                # ì •ê·œì‹: ([^:.,]+) - í…Œì´ë¸”ëª… (ì½œë¡ /ì /ì‰¼í‘œ ì œì™¸), [:.,] - êµ¬ë¶„ì, ([^\s,]+) - ì»¬ëŸ¼ëª…
                                fk_match = re.search(r'([^:.,]+)[:.,]\s*([^\s,]+)', relation_clean)
                                if fk_match:
                                    target_table = fk_match.group(1).strip()
                                    target_column = fk_match.group(2).strip()
                                    fk_list.append({
                                        "column": field_name,
                                        "target_table": target_table,
                                        "target_column": target_column
                                    })
                                else:
                                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê²½ê³  (ë””ë²„ê¹…ìš©)
                                    safe_print(f"[WARN] FK ê´€ê³„ íŒŒì‹± ì‹¤íŒ¨: {table_name}.{field_name} = '{relation}'")
        
        except Exception as e:
            safe_print(f"[WARN] í…Œì´ë¸”ì •ì˜ì„œ FK ì¶”ì¶œ ì‹¤íŒ¨ ({table_name}): {e}")
        
        return fk_list

    def _load_coa_library_data(self) -> Optional[pd.DataFrame]:
        """COA ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°ì´í„° ë¡œë“œ (íŒŒì¼ ì§ì ‘ ì½ê¸°)"""
        try:
            # ì„¤ì •ëœ ë°ì´í„° ê²½ë¡œ ë˜ëŠ” ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
            data_lake_path = self.config.get("data_lake_path", "./data_lake")
            base_dir = Path(__file__).parent.parent
            file_path = base_dir / data_lake_path / "COA_Library.xlsx"
            
            if file_path.exists():
                return pd.read_excel(file_path)
            else:
                safe_print(f"[WARN] COA Library íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                return None
        except Exception as e:
            safe_print(f"[ERROR] COA Library ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _add_coa_library_to_graph(self):
        """COA Library ë°ì´í„°ë¥¼ ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë³€í™˜ (Phase 1)"""
        if self.graph is None:
            return

        df = self._load_coa_library_data()
        if df is None or df.empty:
            return

        safe_print(f"[INFO] COA Library ë°ì´í„°ë¥¼ ì˜¨í†¨ë¡œì§€ë¡œ ë³€í™˜ ì‹œì‘ ({len(df)}ê°œ ë°©ì±…)")
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë‹¨ì¶•ì–´
        NS = self.ns
        
        # Property ì •ì˜ (ì—†ìœ¼ë©´ ìƒì„±)
        properties = {
            "countersThreat": "countersThreat",
            "requiresResource": "requiresResource",
            "hasConstraint": "hasConstraint",
            "hasSuccessRate": "hasSuccessRate",
            "compatibleWith": "compatibleWith",  # ğŸ”¥ NEW: í™˜ê²½ í˜¸í™˜ì„±
            "incompatibleWith": "incompatibleWith"  # ğŸ”¥ NEW: í™˜ê²½ ë¹„í˜¸í™˜ì„±
        }
        
        for p_name, p_uri_suffix in properties.items():
            p_uri = URIRef(NS[p_uri_suffix])
            if (p_uri, RDF.type, OWL.ObjectProperty) not in self.graph:
                self.graph.add((p_uri, RDF.type, OWL.ObjectProperty))
        
        # ë°ì´í„°í˜• ì†ì„± (DatatypeProperty) ì •ì˜
        success_rate_prop = URIRef(NS["hasSuccessRateValue"])
        self.graph.add((success_rate_prop, RDF.type, OWL.DatatypeProperty))
        
        # ğŸ”¥ NEW: í™˜ê²½ í˜¸í™˜ì„± ì ìˆ˜ ì†ì„±
        compatibility_score_prop = URIRef(NS["compatibilityScore"])
        if (compatibility_score_prop, RDF.type, OWL.DatatypeProperty) not in self.graph:
            self.graph.add((compatibility_score_prop, RDF.type, OWL.DatatypeProperty))

        # ğŸ”¥ NEW: ì‹œê°í™” ì†ì„± ì¶”ê°€ (Visualization Properties)
        vis_props = {
            "hasPhaseInfo": "hasPhaseInfo",       # ë‹¨ê³„ ì •ë³´ (Phase 1, 2...)
            "isMainEffort": "isMainEffort",       # ì£¼ë…¸ë ¥ ì—¬ë¶€ (Y/N)
            "hasVisualStyle": "hasVisualStyle"    # ì‹œê°í™” ìŠ¤íƒ€ì¼ (Solid, Dashed...)
        }
        for prop_name, prop_uri_suffix in vis_props.items():
            p_uri = URIRef(NS[prop_uri_suffix])
            if (p_uri, RDF.type, OWL.DatatypeProperty) not in self.graph:
                 self.graph.add((p_uri, RDF.type, OWL.DatatypeProperty))
        
        count = 0
        for _, row in df.iterrows():
            try:
                # 1. COA ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                # ì»¬ëŸ¼ëª… ìœ ì—°ì„± í™•ë³´: 'ID', 'COA_ID', 'ë°©ì±…ID', 'ì‹ë³„ì' ë“± í™•ì¸
                raw_id = row.get('COA_ID') or row.get('ID') or row.get('ë°©ì±…ID') or row.get('ì‹ë³„ì')
                coa_id = str(raw_id) if pd.notna(raw_id) else f'COA_{count}'
                coa_name = str(row.get('ëª…ì¹­', 'Unknown Strategy'))
                desc = str(row.get('ì„¤ëª…', ''))
                
                coa_uri = URIRef(NS[self._make_uri_safe(coa_id)])
                
                if count == 0:
                     safe_print(f"[DEBUG] First COA ID processing: '{coa_id}'")
                
                # íƒ€ì… ì •ì˜ (def:COA)
                self.graph.add((coa_uri, RDF.type, URIRef(NS["COA"])))
                self.graph.add((coa_uri, RDFS.label, Literal(coa_name)))
                self.graph.add((coa_uri, RDFS.comment, Literal(desc)))
                
                # ğŸ”¥ ì„¸ë¶€ íƒ€ì… ì¶”ë¡  (ID ê¸°ë°˜)
                # COA_DEF -> DefenseCOA
                # COA_OFF -> OffensiveCOA
                # COA_CAT -> CounterAttackCOA
                # COA_PRE -> PreemptiveCOA
                # COA_DET -> DeterrenceCOA
                # COA_MAN -> ManeuverCOA
                # COA_INF -> InformationOpsCOA
                
                specific_type = None
                if "COA_DEF" in coa_id:
                    specific_type = "DefenseCOA"
                elif "COA_OFF" in coa_id:
                    specific_type = "OffensiveCOA"
                elif "COA_CAT" in coa_id:
                    specific_type = "CounterAttackCOA"
                elif "COA_PRE" in coa_id:
                    specific_type = "PreemptiveCOA"
                elif "COA_DET" in coa_id:
                    specific_type = "DeterrenceCOA"
                elif "COA_MAN" in coa_id:
                    specific_type = "ManeuverCOA"
                elif "COA_INF" in coa_id:
                    specific_type = "InformationOpsCOA"
                
                if specific_type:
                    self.graph.add((coa_uri, RDF.type, URIRef(NS[specific_type])))
                    safe_print(f"[DEBUG] COA íƒ€ì… ìƒì„¸í™”: {coa_id} -> {specific_type}")
                
                # 2. ìœ„í˜‘ ëŒ€ì‘ ê´€ê³„ (countersThreat)
                threat_type = row.get('í‚¤ì›Œë“œ') or row.get('Keywords')
                if pd.notna(threat_type):
                    for threat in str(threat_type).split(','):
                        t_safe = self._make_uri_safe(threat.strip())
                        if t_safe:
                            # ìœ„í˜‘ ë…¸ë“œê°€ ì—†ìœ¼ë©´ ìƒì„± (ê°œë…ì  ë…¸ë“œ)
                            threat_uri = URIRef(NS[t_safe])
                            self.graph.add((threat_uri, RDF.type, URIRef(NS["Threat"])))
                            self.graph.add((coa_uri, URIRef(NS["countersThreat"]), threat_uri))

                # 3. í•„ìš” ìì› ê´€ê³„ (requiresResource)
                resources = row.get('í•„ìš”ìì›') or row.get('Required_Resources')
                if pd.notna(resources):
                    for res in str(resources).split(','):
                        r_safe = self._make_uri_safe(res.strip())
                        if r_safe:
                            res_uri = URIRef(NS[r_safe])
                            self.graph.add((res_uri, RDF.type, URIRef(NS["Resource"])))
                            self.graph.add((coa_uri, URIRef(NS["requiresResource"]), res_uri))
                
                # 4. ì œì•½ ì¡°ê±´ (hasConstraint)
                constraints = row.get('ì „ì¥í™˜ê²½_ì œì•½') or row.get('Environmental_Constraints')
                if pd.notna(constraints):
                    for con in str(constraints).split(','):
                        c_safe = self._make_uri_safe(con.strip())
                        if c_safe:
                            con_uri = URIRef(NS[c_safe])
                            self.graph.add((con_uri, RDF.type, URIRef(NS["Constraint"])))
                            self.graph.add((coa_uri, URIRef(NS["hasConstraint"]), con_uri))

                # 5. ì„±ê³µë¥  (hasSuccessRate) - Literalë¡œ ì¶”ê°€
                success_rate = row.get('ì›Œê²Œì„_ëª¨ì˜_ë¶„ì„_ìŠ¹ë¥ ') or row.get('ì˜ˆìƒì„±ê³µë¥ ') or row.get('Estimated_Success_Rate')
                if pd.notna(success_rate):
                    try:
                        rate_val = float(success_rate)
                        self.graph.add((coa_uri, success_rate_prop, Literal(rate_val, datatype=XSD.float)))
                    except:
                        pass
                
                # ğŸ”¥ NEW: ì‹œê°í™” ì†ì„± ë§¤í•‘
                # ë‹¨ê³„ ì •ë³´
                phase_info = row.get('ë‹¨ê³„ì •ë³´') or row.get('Phase_Info')
                if pd.notna(phase_info):
                    self.graph.add((coa_uri, URIRef(NS["hasPhaseInfo"]), Literal(str(phase_info))))
                
                # ì£¼ë…¸ë ¥ ì—¬ë¶€
                main_effort = row.get('ì£¼ë…¸ë ¥ì—¬ë¶€') or row.get('Main_Effort')
                if pd.notna(main_effort):
                    self.graph.add((coa_uri, URIRef(NS["isMainEffort"]), Literal(str(main_effort))))

                # ì‹œê°í™” ìŠ¤íƒ€ì¼
                vis_style = row.get('ì‹œê°í™”ìŠ¤íƒ€ì¼') or row.get('Visual_Style')
                if pd.notna(vis_style):
                    self.graph.add((coa_uri, URIRef(NS["hasVisualStyle"]), Literal(str(vis_style))))
                
                count += 1
            except Exception as e:
                safe_print(f"[WARN] COA ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ({coa_id}): {e}")
        
        safe_print(f"[INFO] COA Library ì˜¨í†¨ë¡œì§€ ë³€í™˜ ì™„ë£Œ: {count}ê°œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
    
    def generate_owl_ontology(self, data: Dict[str, pd.DataFrame], 
                             meta_t: Optional[pd.DataFrame] = None,
                             meta_c: Optional[pd.DataFrame] = None) -> Optional[Graph]:
        """
        OWL ì˜¨í†¨ë¡œì§€ ìƒì„± (í˜„ì¬ ì‹œìŠ¤í…œì˜ generate_ontology.py ë¡œì§)
        
        Args:
            data: {í…Œì´ë¸”ëª…: DataFrame} ë”•ì…”ë„ˆë¦¬
            meta_t: í…Œì´ë¸” ë©”íƒ€ë°ì´í„° (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            meta_c: ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„° (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
        
        Returns:
            RDF Graph ê°ì²´
        """
        if not RDFLIB_AVAILABLE:
            return None
        
        # ê·¸ë˜í”„ ì´ˆê¸°í™” (ë§¤ë²ˆ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ê´€ê³„ ë³€ê²½ ì‚¬í•­ ë°˜ì˜)
        # ê¸°ì¡´ ê·¸ë˜í”„ë¥¼ ìœ ì§€í•˜ë©´ ì´ì „ ê´€ê³„ê°€ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë§¤ë²ˆ ì´ˆê¸°í™”
        self.graph = Graph()
        safe_print("[DEBUG] generate_owl_ontology: ê·¸ë˜í”„ ì´ˆê¸°í™” ì™„ë£Œ (ê¸°ì¡´ ê·¸ë˜í”„ ì œê±°)")
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”©
        self.graph.bind("ns", self.ns)  # í†µì¼ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚¬ìš©
        self.graph.bind("owl", OWL)
        self.graph.bind("rdfs", RDFS)
        self.graph.bind("rdf", RDF)
        
        # ë©”íƒ€ë°ì´í„° ìë™ ìƒì„± (ì—†ëŠ” ê²½ìš°)
        if meta_t is None or meta_c is None:
            meta_t, meta_c = self._generate_metadata(data)
        
        # ê´€ê³„ ë§¤í•‘ ë¡œë“œ (ê°•ì œ ì¬ë¡œë“œí•˜ì—¬ ìµœì‹  ìƒíƒœ ë³´ì¥)
        relation_mappings = self.load_relation_mappings(force_reload=True)
        # ğŸ”¥ ë¡œê·¸ ìµœì í™”: ë¶ˆí•„ìš”í•œ DEBUG ë¡œê·¸ ì œê±°
        # safe_print(f"[DEBUG] ë¡œë“œëœ ê´€ê³„ ë§¤í•‘ ìˆ˜: {len(relation_mappings)}ê°œ")
        # for i, rel_map in enumerate(relation_mappings[:5]):  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
        #     safe_print(f"[DEBUG] ê´€ê³„ {i+1}: {rel_map.get('src_table')}.{rel_map.get('src_col')} -> {rel_map.get('tgt_table')} (ì†ŒìŠ¤: {rel_map.get('source', 'unknown')})")
        
        # í´ë˜ìŠ¤ ê³„ì¸µ êµ¬ì¡° ë¡œë“œ
        class_hierarchy = self._load_class_hierarchy()
        
        # í´ë˜ìŠ¤ ì •ì˜ (OWL Class)
        for _, t in meta_t.iterrows():
            table_name = t['table_name']
            class_uri = URIRef(self.ns[self._make_uri_safe(table_name)])
            
            # owl:Classë¡œ ì •ì˜
            self.graph.add((class_uri, RDF.type, OWL.Class))
            
            # ìƒìœ„ í´ë˜ìŠ¤ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if table_name in class_hierarchy:
                super_uri = URIRef(self.ns[class_hierarchy[table_name]])
                self.graph.add((class_uri, RDFS.subClassOf, super_uri))
        
        safe_print(f"í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ: {len(meta_t)}ê°œ")
        
        # âœ¨ COA íƒ€ì…ë³„ í´ë˜ìŠ¤ ë° ì†ì„± ì¶”ê°€ (Week 1 ê°œì„ )
        self._add_coa_type_classes()
        
        # ë©”íƒ€ë°ì´í„°ì— ì •ì˜ëœ í…Œì´ë¸”ëª… ì§‘í•© (ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•´)
        defined_tables = set(meta_t['table_name'].tolist()) if not meta_t.empty else set()
        
        # ObjectProperty ì •ì˜ (í…Œì´ë¸” ê°„ ê´€ê³„)
        # ì‹¤ì œ ë°ì´í„° íŒŒì¼ì´ ìˆëŠ” í…Œì´ë¸” ê°„ì˜ ê´€ê³„ë§Œ Propertyë¡œ ìƒì„±
        property_count = 0
        skipped_count = 0
        dynamic_relations = {}  # ë™ì  FK ê´€ê³„ ì¶”ì  (ê°™ì€ ê´€ê³„ëª…, ë‹¤ë¥¸ íƒ€ê²Ÿ í…Œì´ë¸”)
        
        for rel_map in relation_mappings:
            rel_name = rel_map.get('relation', '')
            if not rel_name:
                continue
            
            src_table = rel_map.get('src_table', '')
            tgt_table = rel_map.get('tgt_table', '')
            is_dynamic = rel_map.get('dynamic', False)
            type_mapping = rel_map.get('type_mapping', {})
            
            # ì†ŒìŠ¤ í…Œì´ë¸” í™•ì¸
            if src_table not in defined_tables:
                # ê°€ìƒ íƒ€ê²Ÿ(Ontology.ë¡œ ì‹œì‘)ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ê²½ê³  ì¶œë ¥
                if not src_table.startswith("Ontology."):
                    safe_print(f"[WARN] relation_mappingsì—ì„œ ì°¸ì¡°ëœ ì†ŒìŠ¤ í…Œì´ë¸” '{src_table}'ì— ëŒ€í•œ ë°ì´í„° íŒŒì¼ì´ ì—†ì–´ Property ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                skipped_count += 1
                continue
            
            # ë™ì  FK ê´€ê³„ ì²˜ë¦¬: ê° íƒ€ê²Ÿ í…Œì´ë¸”ì— ëŒ€í•´ ë³„ë„ Property ìƒì„±
            if is_dynamic and type_mapping:
                for target_type, actual_tgt_table in type_mapping.items():
                    if actual_tgt_table not in defined_tables:
                        continue
                    
                    # ë™ì  ê´€ê³„ëŠ” íƒ€ê²Ÿ í…Œì´ë¸”ë³„ë¡œ Property ìƒì„±
                    prop_uri = URIRef(self.ns[rel_name])
                    self.graph.add((prop_uri, RDF.type, OWL.ObjectProperty))
                    
                    domain_uri = URIRef(self.ns[src_table])
                    range_uri = URIRef(self.ns[actual_tgt_table])
                    
                    self.graph.add((prop_uri, RDFS.domain, domain_uri))
                    self.graph.add((prop_uri, RDFS.range, range_uri))
                    
                    property_count += 1
                    safe_print(f"[DEBUG] Property ìƒì„± (ë™ì  FK): {rel_name} (domain: {src_table}, range: {actual_tgt_table})")
            # ì¼ë°˜ FK ê´€ê³„ ì²˜ë¦¬
            elif tgt_table in defined_tables:
                prop_uri = URIRef(self.ns[rel_name])
                self.graph.add((prop_uri, RDF.type, OWL.ObjectProperty))
                
                domain_uri = URIRef(self.ns[src_table])
                range_uri = URIRef(self.ns[tgt_table])
                
                self.graph.add((prop_uri, RDFS.domain, domain_uri))
                self.graph.add((prop_uri, RDFS.range, range_uri))
                
                property_count += 1
                safe_print(f"[DEBUG] Property ìƒì„±: {rel_name} (domain: {src_table}, range: {tgt_table}, ì†ŒìŠ¤: {rel_map.get('source', 'unknown')})")
            else:
                # ê°€ìƒ íƒ€ê²Ÿ(Ontology.ë¡œ ì‹œì‘)ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ê²½ê³  ì¶œë ¥
                if not tgt_table.startswith("Ontology."):
                    safe_print(f"[WARN] relation_mappingsì—ì„œ ì°¸ì¡°ëœ íƒ€ê²Ÿ í…Œì´ë¸” '{tgt_table}'ì— ëŒ€í•œ ë°ì´í„° íŒŒì¼ì´ ì—†ì–´ Property ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                skipped_count += 1
        
        safe_print(f"ObjectProperty ì •ì˜ ì™„ë£Œ: {property_count}ê°œ (ê±´ë„ˆëœ€: {skipped_count}ê°œ)")
        if property_count == 0 and len(relation_mappings) > 0:
            safe_print(f"[WARN] ê´€ê³„ ë§¤í•‘ì€ {len(relation_mappings)}ê°œ ìˆì§€ë§Œ Propertyê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í…Œì´ë¸”ëª…ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            safe_print(f"[DEBUG] ì •ì˜ëœ í…Œì´ë¸” ëª©ë¡: {sorted(defined_tables)}")
            relation_tables = sorted(set([r.get('src_table') for r in relation_mappings] + [r.get('tgt_table') for r in relation_mappings]))
            safe_print(f"[DEBUG] ê´€ê³„ ë§¤í•‘ì˜ í…Œì´ë¸” ëª©ë¡: {relation_tables}")
        
        # COA ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°ì´í„°ëŠ” generate_instances()ì—ì„œ ì¼ë°˜ í…Œì´ë¸”ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
        # ì¤‘ë³µ ìƒì„±ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ _add_coa_library_to_graph() í˜¸ì¶œì„ ì œê±°í–ˆìŠµë‹ˆë‹¤.
        
        # [MOD] ì¤‘ë³µ ì €ì¥ ë°©ì§€: save_graph()ì—ì„œ í†µí•© ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
        # output_file = os.path.join(self.output_path, "k_c4i_ontology_owl.ttl")
        # os.makedirs(self.output_path, exist_ok=True)
        # self.graph.serialize(destination=output_file, format="turtle")
        # safe_print(f"OWL ì˜¨í†¨ë¡œì§€ ì €ì¥ ì™„ë£Œ: {output_file}")
        
        # ê·¸ë˜í”„ ìƒíƒœ í™•ì¸
        triples_count = len(list(self.graph.triples((None, None, None))))
        safe_print(f"[INFO] generate_owl_ontology: ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ - {triples_count} triples")
        
        return self.graph
    
    def _generate_metadata(self, data: Dict[str, pd.DataFrame]) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        ë©”íƒ€ë°ì´í„° ìë™ ìƒì„± (ì—‘ì…€ì˜ "í…Œì´ë¸”ì •ì˜ì„œ" ì‹œíŠ¸ ìš°ì„  ì‚¬ìš©)
        
        ì—‘ì…€ íŒŒì¼ì˜ "í…Œì´ë¸”ì •ì˜ì„œ" ì‹œíŠ¸ì—ì„œ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì½ê³ ,
        ì—†ìœ¼ë©´ DataFrameì—ì„œ ìë™ ì¶”ë¡ í•©ë‹ˆë‹¤.
        """
        meta_t_rows = []
        meta_c_rows = []
        
        # DataManagerë¥¼ í†µí•´ ë¡œë” ì‚¬ìš© ì‹œë„
        data_manager = getattr(self, 'data_manager', None)
        if data_manager is None:
            # configì—ì„œ data_paths ê°€ì ¸ì˜¤ê¸°
            data_paths = self.config.get("data_paths", {})
        
        for table_name, df in data.items():
            # í…Œì´ë¸” ë©”íƒ€ë°ì´í„°
            meta_t_rows.append({
                'table_name': table_name,
                'row_count': len(df)
            })
            
            # ì—‘ì…€ì˜ "í…Œì´ë¸”ì •ì˜ì„œ" ì‹œíŠ¸ì—ì„œ ìŠ¤í‚¤ë§ˆ ì •ë³´ ì½ê¸° ì‹œë„
            schema_info = None
            if data_manager:
                try:
                    loader = data_manager.get_loader(table_name)
                    if loader:
                        schema_info = loader.load_schema()
                except Exception as e:
                    safe_print(f"[WARN] Failed to load schema for {table_name}: {e}")
            
            # ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ DataFrameì—ì„œ ì¶”ë¡ 
            if schema_info and schema_info.get('fields'):
                # í…Œì´ë¸”ì •ì˜ì„œì—ì„œ í•„ë“œ ì •ë³´ ì‚¬ìš©
                for field_info in schema_info['fields']:
                    field_name = field_info.get('name', '')
                    field_type = field_info.get('type', 'string')
                    
                    meta_c_rows.append({
                        'table_name': table_name,
                        'column_name': field_name,
                        'data_type': field_type
                    })
            # Schema Registry í™•ì¸
            elif table_name in self.schema_registry:
                table_info = self.schema_registry[table_name]
                for col_name, col_info in table_info.get('columns', {}).items():
                    meta_c_rows.append({
                        'table_name': table_name,
                        'column_name': col_name,
                        'data_type': col_info.get('type', 'string')
                    })
            else:
                # DataFrameì—ì„œ ìë™ ì¶”ë¡  (í•˜ìœ„ í˜¸í™˜ì„±)
                for col in df.columns:
                    meta_c_rows.append({
                        'table_name': table_name,
                        'column_name': col,
                        'data_type': str(df[col].dtype)
                    })
        
        meta_t = pd.DataFrame(meta_t_rows)
        meta_c = pd.DataFrame(meta_c_rows)
        
        return meta_t, meta_c
    
    def _load_class_hierarchy(self) -> Dict[str, str]:
        """í´ë˜ìŠ¤ ê³„ì¸µ êµ¬ì¡° ë¡œë“œ"""
        onto_config_path = os.path.join(self.metadata_path, "ontology_config.xlsx")
        if not os.path.exists(onto_config_path):
            return {}
        
        try:
            onto_config = pd.read_excel(onto_config_path, sheet_name=None)
            if 'ClassHierarchy' not in onto_config:
                return {}
            
            class_hierarchy = {}
            class_df = onto_config['ClassHierarchy']
            for _, row in class_df.iterrows():
                table_name = row.get('table_name', '')
                super_class = row.get('super_class', '')
                if table_name and super_class:
                    class_hierarchy[table_name] = super_class
            
            return class_hierarchy
        except Exception as e:
            safe_print(f"í´ë˜ìŠ¤ ê³„ì¸µ êµ¬ì¡° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {}
    
    def _add_coa_type_classes(self):
        """
        COA íƒ€ì…ë³„ í´ë˜ìŠ¤ ë° ì†ì„± ì •ì˜ ì¶”ê°€ (Week 1 ê°œì„ )
        
        7ê°€ì§€ COA íƒ€ì…ì— ëŒ€í•œ OWL í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ê³ ,
        ê° íƒ€ì…ë³„ë¡œ í•„ìš”í•œ ì†ì„±(DataProperty)ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        # COA íƒ€ì…ë³„ í´ë˜ìŠ¤ ì •ì˜
        coa_types = {
            "ë°©ì–´ë°©ì±…": "DefenseCOA",
            "ê³µê²©ë°©ì±…": "OffensiveCOA",
            "ë°˜ê²©ë°©ì±…": "CounterAttackCOA",
            "ì„ ì œë°©ì±…": "PreemptiveCOA",
            "ì–µì œë°©ì±…": "DeterrenceCOA",
            "ê¸°ë™ë°©ì±…": "ManeuverCOA",
            "ì •ë³´ë°©ì±…": "InformationOpsCOA"
        }
        
        # 1. ìƒìœ„ ë°©ì±… í´ë˜ìŠ¤ ì •ì˜ (COA)
        coa_class_uri = URIRef(self.ns["COA"])
        self.graph.add((coa_class_uri, RDF.type, OWL.Class))
        self.graph.add((coa_class_uri, RDFS.label, Literal("ë°©ì±…", lang="ko")))
        self.graph.add((coa_class_uri, RDFS.label, Literal("Course of Action", lang="en")))
        
        # 2. ê° íƒ€ì…ë³„ í´ë˜ìŠ¤ ìƒì„±
        for korean_name, english_name in coa_types.items():
            class_uri = URIRef(self.ns[english_name])
            
            # OWL Classë¡œ ì •ì˜
            self.graph.add((class_uri, RDF.type, OWL.Class))
            
            # COAì˜ í•˜ìœ„ í´ë˜ìŠ¤ë¡œ ì„¤ì •
            self.graph.add((class_uri, RDFS.subClassOf, coa_class_uri))
            
            # í•œê¸€/ì˜ë¬¸ ë¼ë²¨ ì¶”ê°€
            self.graph.add((class_uri, RDFS.label, Literal(korean_name, lang="ko")))
            self.graph.add((class_uri, RDFS.label, Literal(english_name, lang="en")))
            
            safe_print(f"[DEBUG] COA íƒ€ì… í´ë˜ìŠ¤ ìƒì„±: {korean_name} ({english_name})")
        
        # 3. ë°©ì–´ ë°©ì±… ì „ìš© ì†ì„±
        defense_strength = URIRef(self.ns["defenseStrength"])
        self.graph.add((defense_strength, RDF.type, OWL.DatatypeProperty))
        self.graph.add((defense_strength, RDFS.domain, URIRef(self.ns["DefenseCOA"])))
        self.graph.add((defense_strength, RDFS.range, XSD.float))
        self.graph.add((defense_strength, RDFS.label, Literal("ë°©ì–´ê°•ë„", lang="ko")))
        
        defense_coverage = URIRef(self.ns["defenseCoverage"])
        self.graph.add((defense_coverage, RDF.type, OWL.DatatypeProperty))
        self.graph.add((defense_coverage, RDFS.domain, URIRef(self.ns["DefenseCOA"])))
        self.graph.add((defense_coverage, RDFS.range, XSD.string))
        self.graph.add((defense_coverage, RDFS.label, Literal("ë°©ì–´ë²”ìœ„", lang="ko")))
        
        # 4. ê³µê²© ë°©ì±… ì „ìš© ì†ì„±
        attack_power = URIRef(self.ns["attackPower"])
        self.graph.add((attack_power, RDF.type, OWL.DatatypeProperty))
        self.graph.add((attack_power, RDFS.domain, URIRef(self.ns["OffensiveCOA"])))
        self.graph.add((attack_power, RDFS.range, XSD.float))
        self.graph.add((attack_power, RDFS.label, Literal("ê³µê²©ë ¥", lang="ko")))
        
        breakthrough_capability = URIRef(self.ns["breakthroughCapability"])
        self.graph.add((breakthrough_capability, RDF.type, OWL.DatatypeProperty))
        self.graph.add((breakthrough_capability, RDFS.domain, URIRef(self.ns["OffensiveCOA"])))
        self.graph.add((breakthrough_capability, RDFS.range, XSD.float))
        self.graph.add((breakthrough_capability, RDFS.label, Literal("ëŒíŒŒëŠ¥ë ¥", lang="ko")))
        
        # 5. ê³µí†µ ì†ì„± (ëª¨ë“  COA íƒ€ì…)
        effectiveness = URIRef(self.ns["effectiveness"])
        self.graph.add((effectiveness, RDF.type, OWL.DatatypeProperty))
        self.graph.add((effectiveness, RDFS.domain, coa_class_uri))
        self.graph.add((effectiveness, RDFS.range, XSD.float))
        self.graph.add((effectiveness, RDFS.label, Literal("íš¨ê³¼ì„±", lang="ko")))
        
        resource_requirement = URIRef(self.ns["resourceRequirement"])
        self.graph.add((resource_requirement, RDF.type, OWL.DatatypeProperty))
        self.graph.add((resource_requirement, RDFS.domain, coa_class_uri))
        self.graph.add((resource_requirement, RDFS.range, XSD.string))
        self.graph.add((resource_requirement, RDFS.label, Literal("ìì›ìš”êµ¬", lang="ko")))
        
        safe_print(f"[INFO] COA íƒ€ì…ë³„ í´ë˜ìŠ¤ 7ê°œ ë° ì†ì„± ì¶”ê°€ ì™„ë£Œ")
    
    def generate_instances(self, data: Dict[str, pd.DataFrame], 
                          enable_virtual_entities: bool = True) -> Optional[Graph]:
        """
        ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í˜„ì¬ ì‹œìŠ¤í…œì˜ generate_instances.py ë¡œì§)
        
        Args:
            data: {í…Œì´ë¸”ëª…: DataFrame} ë”•ì…”ë„ˆë¦¬
            enable_virtual_entities: ê°€ìƒ ì—”í‹°í‹° ìƒì„± í™œì„±í™”
        
        Returns:
            RDF Graph ê°ì²´
        """
        if not RDFLIB_AVAILABLE:
            return None
        
        # ê¸°ì¡´ ê·¸ë˜í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if self.graph is None:
            self.graph = Graph()
            safe_print("[DEBUG] generate_instances: ê·¸ë˜í”„ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ í™•ì¸ ë° ë¡œë“œ (ê¸°ì¡´ ê·¸ë˜í”„ì— ì¶”ê°€)
        self.virtual_entities_count = 0  # ì¹´ìš´í„° ì´ˆê¸°í™”
        # [MOD] 3ë‹¨ê³„ êµ¬ì¡°(schema.ttl) ìš°ì„  ìˆœìœ„ ì ìš©
        schema_file = Path(self.ontology_path) / "schema.ttl"
        legacy_ontology_file = Path(self.output_path) / "k_c4i_ontology_owl.ttl"
        
        # ì´ë¯¸ ê·¸ë˜í”„ê°€ ì±„ì›Œì ¸ ìˆìœ¼ë©´(ì˜ˆ: generate_owl_ontologyì—ì„œ) ì¶”ê°€ ë¡œë“œ ê±´ë„ˆëœ€
        schema_triples = len(list(self.graph.triples((None, RDFS.subClassOf, None))))
        if schema_triples > 0:
            # safe_print(f"[DEBUG] generate_instances: ì´ë¯¸ {schema_triples}ê°œì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ë©”ëª¨ë¦¬ì— ìˆìŒ. ë¡œë”© ê±´ë„ˆëœ€.")
            is_owl = True
        elif schema_file.exists():
            self.graph.parse(str(schema_file), format="turtle")
            # safe_print(f"[INFO] ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì™„ë£Œ (3ë‹¨ê³„ í‘œì¤€): {schema_file}")
            is_owl = True
        elif legacy_ontology_file.exists():
            self.graph.parse(str(legacy_ontology_file), format="turtle")
            # safe_print(f"[INFO] ë ˆê±°ì‹œ ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì™„ë£Œ: {legacy_ontology_file}")
            is_owl = True
        else:
            # safe_print("[WARN] ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (schema.ttl ë˜ëŠ” legacy). ì¸ìŠ¤í„´ìŠ¤ë§Œ ìƒì„±í•©ë‹ˆë‹¤.")
            is_owl = False
        
        # ê´€ê³„ ë§¤í•‘ ë¡œë“œ
        relation_mappings = self.load_relation_mappings()
        
        # [MOD] í…Œì´ë¸” ì²˜ë¦¬ ìˆœì„œ ìµœì í™” (ë§ˆìŠ¤í„° ë°ì´í„° ìš°ì„  ì²˜ë¦¬)
        # ìœ„í˜‘ìœ í˜•_ë§ˆìŠ¤í„° ë“± ê¸°ì¤€ ì •ë³´ë¥¼ ë¨¼ì € ì²˜ë¦¬í•´ì•¼ COA_Library ë“±ì—ì„œ ì‹œë§¨í‹± ë§í¬ë¥¼ ê±¸ ìˆ˜ ìˆìŒ
        prioritized_tables = ["ìœ„í˜‘ìœ í˜•_ë§ˆìŠ¤í„°", "ì„ë¬´ì •ë³´", "ë¶€ëŒ€ê³µí†µì†ì„±", "ì§€í˜•ìœ í˜•_ë§ˆìŠ¤í„°"]
        sorted_tables = [t for t in prioritized_tables if t in data] + \
                        [t for t in data.keys() if t not in prioritized_tables]
        
        for table_name in sorted_tables:
            df = data[table_name]
            if df.empty:
                continue
            
            # ID ì»¬ëŸ¼ ìë™ ê°ì§€ (Schema Registry í™œìš©)
            id_col = self.get_id_column(table_name, list(df.columns))
            label_col = self.get_label_column(table_name, list(df.columns))
            
            for idx, row in df.iterrows():
                # ì¸ìŠ¤í„´ìŠ¤ URI ìƒì„±
                # ID ì»¬ëŸ¼ ê°’ ê°€ì ¸ì˜¤ê¸° (ë¹„ì–´ìˆê±°ë‚˜ NaNì¸ ê²½ìš° ì²˜ë¦¬)
                row_id = None
                if id_col and id_col in row:
                    id_value = row[id_col]
                    # NaN, None, ë¹ˆ ë¬¸ìì—´ ì²´í¬
                    if pd.notna(id_value) and str(id_value).strip() and str(id_value).lower() != 'nan':
                        row_id = str(id_value).strip()
                
                # ID ê°’ì´ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ ì‚¬ìš© (ê²½ê³  ë¡œê·¸ ì¶œë ¥)
                if not row_id:
                    row_id = f"{table_name}_{idx}"
                    safe_print(f"[WARN] {table_name} í…Œì´ë¸”ì˜ {idx}ë²ˆì§¸ í–‰ì— ID ì»¬ëŸ¼('{id_col}') ê°’ì´ ì—†ì–´ ì¸ë±ìŠ¤ ê¸°ë°˜ ID ì‚¬ìš©: {row_id}")
                
                instance_id_safe = self._make_uri_safe(f"{table_name}_{row_id}")
                instance_uri = URIRef(self.ns[instance_id_safe])
                
                # í´ë˜ìŠ¤ íƒ€ì… ì¶”ê°€
                class_uri = URIRef(self.ns[self._make_uri_safe(table_name)])
                self.graph.add((instance_uri, RDF.type, class_uri))
                
                # ë¼ë²¨ ì¶”ê°€
                if label_col and label_col in row:
                    label_val = str(row[label_col])
                    self.graph.add((instance_uri, RDFS.label, Literal(label_val)))
                
                # Literal ì†ì„± ì¶”ê°€
                for col in df.columns:
                    if col == id_col or col == label_col:
                        continue
                    
                    val = row[col]
                    if pd.notna(val) and val != "":
                        # FK ì»¬ëŸ¼ì¸ì§€ í™•ì¸
                        is_fk = self._is_foreign_key(table_name, col, relation_mappings)
                        
                        if not is_fk:
                            # Literal ì†ì„±ìœ¼ë¡œ ì¶”ê°€
                            prop_uri = URIRef(self.ns[col])

                            # [NEW] ë¶€ëŒ€/ìì‚° ìƒì„¸ ì†ì„± ë§¤í•‘ (í‘œì¤€í™”ëœ í”„ë¡œí¼í‹° ì‚¬ìš©)
                            if col == "SIDC":
                                prop_uri = URIRef(self.ns["hasSIDC"])
                                self.graph.add((instance_uri, prop_uri, Literal(str(val))))
                                continue # ì•„ë˜ì˜ ê¸°ë³¸ ì¶”ê°€ ë¡œì§ ê±´ë„ˆëœ€
                            
                            elif col == "ì „íˆ¬ë ¥ì§€ìˆ˜" or col == "Combat_Power":
                                try:
                                    prop_uri = URIRef(self.ns["hasCombatPower"])
                                    self.graph.add((instance_uri, prop_uri, Literal(float(val), datatype=XSD.float)))
                                except:
                                    pass
                                continue

                            elif col == "ì´ë™ì†ë„_kmh" or col == "Max_Speed":
                                try:
                                    prop_uri = URIRef(self.ns["hasMaxSpeed"])
                                    self.graph.add((instance_uri, prop_uri, Literal(float(val), datatype=XSD.float)))
                                except:
                                    pass
                                continue

                            elif col == "ê°ì§€ë²”ìœ„_km" or col == "Detection_Range":
                                try:
                                    prop_uri = URIRef(self.ns["hasDetectionRange"])
                                    self.graph.add((instance_uri, prop_uri, Literal(float(val), datatype=XSD.float)))
                                except:
                                    pass
                                continue
                            
                            # ì ìš©ì¡°ê±´ í•„ë“œ íŠ¹ìˆ˜ ì²˜ë¦¬: expression â†’ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
                            if col == "ì ìš©ì¡°ê±´" or col == "Apply_Condition":
                                # expression í˜•ì‹ (ì˜ˆ: "threat_level > 0.8")ì„ í‚¤ì›Œë“œë¡œ ì¶”ì¶œ
                                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§: ë³€ìˆ˜ëª…ê³¼ ì—°ì‚°ì ê¸°ë°˜
                                keywords = self._extract_keywords_from_condition(str(val))
                                for keyword in keywords:
                                    if keyword:
                                        self.graph.add((instance_uri, prop_uri, Literal(keyword)))
                                continue
                            
                            # ê¸°ë³¸ ë§¤í•‘ (ìœ„ì—ì„œ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²½ìš°)
                            self.graph.add((instance_uri, prop_uri, Literal(str(val))))

                            # [NEW] ì¢Œí‘œ ì •ë³´ íŠ¹ìˆ˜ ì²˜ë¦¬ (ëª¨ë“  í…Œì´ë¸” ì ìš©)
                            # ì§€í˜•ì…€, ìœ„í˜‘ìƒí™©, ì•„êµ°ë¶€ëŒ€í˜„í™© ë“± ì–´ë–¤ í…Œì´ë¸”ì´ë“  "ì¢Œí‘œì •ë³´" ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì²˜ë¦¬
                            if col == "ì¢Œí‘œì •ë³´" or col == "coordinates":
                                try:
                                    # "ê²½ë„, ìœ„ë„" í˜•ì‹ íŒŒì‹± (ì˜ˆ: "127.5, 36.5")
                                    coords = str(val).split(',')
                                    if len(coords) >= 2:
                                        # GeoJSON ìˆœì„œ (x, y) = (ê²½ë„, ìœ„ë„) ì¤€ìˆ˜
                                        lon = float(coords[0].strip())
                                        lat = float(coords[1].strip())
                                        
                                        self.graph.add((instance_uri, URIRef(self.ns["hasLongitude"]), Literal(lon, datatype=XSD.float)))
                                        self.graph.add((instance_uri, URIRef(self.ns["hasLatitude"]), Literal(lat, datatype=XSD.float)))
                                        # safe_print(f"[DEBUG] ì¢Œí‘œ ë“±ë¡ ({table_name}): {row_id} ({lon}, {lat})")
                                except Exception as e:
                                    safe_print(f"[WARN] ì¢Œí‘œ íŒŒì‹± ì‹¤íŒ¨ ({table_name} - {row_id}): {val} - {e}")
                
                # COA_Library í…Œì´ë¸” íŠ¹ìˆ˜ ì²˜ë¦¬: í‚¤ì›Œë“œ, í•„ìš”ìì›, ì „ì¥í™˜ê²½_ì œì•½ì„ ê´€ê³„ë¡œ ë³€í™˜
                if table_name == "COA_Library":
                    self._process_coa_library_relations(instance_uri, row, enable_virtual_entities)
                
                # ìœ„í˜‘ìƒí™© í…Œì´ë¸” íŠ¹ìˆ˜ ì²˜ë¦¬: ìœ„í˜‘ìœ í˜•ì„ ê´€ê³„ë¡œ ë³€í™˜ (ì „ëµì²´ì¸ ì—°ê²°ìš©)
                if table_name == "ìœ„í˜‘ìƒí™©":
                    self._process_threat_situation_relations(instance_uri, row, data)
                
                # FK ê´€ê³„ ìƒì„±
                self._create_fk_relationships(
                    instance_uri, table_name, row, df.columns, 
                    relation_mappings, data, enable_virtual_entities
                )
        
        # [MOD] ì¤‘ë³µ ì €ì¥ ë°©ì§€: save_graph()ì—ì„œ í†µí•© ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
        # output_file = os.path.join(self.output_path, "k_c4i_instances_owl.ttl")
        # self.graph.serialize(destination=output_file, format="turtle")
        # safe_print(f"ì¸ìŠ¤í„´ìŠ¤ TTL íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}")
        
        # ê·¸ë˜í”„ ìƒíƒœ í™•ì¸
        triples_count = len(list(self.graph.triples((None, None, None))))
        schema_subclass = len(list(self.graph.triples((None, RDFS.subClassOf, None))))
        schema_domain = len(list(self.graph.triples((None, RDFS.domain, None))))
        schema_range = len(list(self.graph.triples((None, RDFS.range, None))))
        safe_print(f"[INFO] generate_instances: ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ - {triples_count} triples (ê°€ìƒ ì—”í‹°í‹°: {self.virtual_entities_count}ê°œ)")
        safe_print(f"[DEBUG] generate_instances: ìŠ¤í‚¤ë§ˆ ìƒíƒœ - subClassOf={schema_subclass}, domain={schema_domain}, range={schema_range}")
        
        return self.graph
    
    def _is_foreign_key(self, table_name: str, col_name: str, 
                       relation_mappings: List[Dict]) -> bool:
        """ì»¬ëŸ¼ì´ ì™¸ë˜í‚¤ì¸ì§€ í™•ì¸ (relation_mappings + í…Œì´ë¸”ì •ì˜ì„œ ê¸°ë°˜)"""
        # 1. relation_mappingsì—ì„œ í™•ì¸
        for rel_map in relation_mappings:
            if rel_map.get('src_table') == table_name and rel_map.get('src_col') == col_name:
                return True
        
        # 2. í…Œì´ë¸”ì •ì˜ì„œì—ì„œ í™•ì¸ (ìºì‹œëœ relation_mappingsì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìŒ)
        # relation_mappingsì— source="table_schema"ì¸ í•­ëª©ì´ ìˆìœ¼ë©´ ì´ë¯¸ ì²˜ë¦¬ë¨
        
        # 3. ìë™ ê°ì§€: *ID íŒ¨í„´ (í´ë°±, relation_mappingsì™€ í…Œì´ë¸”ì •ì˜ì„œì— ì—†ì„ ë•Œë§Œ)
        if col_name.endswith('ID') or col_name.endswith('id') or col_name.endswith('_id'):
            return True
        
        return False
    
    def _create_fk_relationships(self, src_uri: URIRef, table_name: str, row: pd.Series,
                                columns: List[str], relation_mappings: List[Dict],
                                data: Dict[str, pd.DataFrame], 
                                enable_virtual_entities: bool):
        """FK ê´€ê³„ ìƒì„±"""
        for rel_map in relation_mappings:
            if rel_map.get('src_table') != table_name:
                continue
            
            # ë™ì  FK ê´€ê³„ ì²˜ë¦¬
            if rel_map.get('dynamic', False):
                self._create_dynamic_fk_relationship(
                    src_uri, table_name, row, columns, rel_map, data, enable_virtual_entities
                )
                continue
            
            # ì¼ë°˜ FK ê´€ê³„ ì²˜ë¦¬
            src_col = rel_map.get('src_col')
            if src_col not in columns:
                continue
            
            fk_val = str(row[src_col]).strip()
            if not fk_val or pd.isna(row[src_col]):
                continue
            
            tgt_table = rel_map.get('tgt_table')
            relation_name = rel_map.get('relation', f"has{tgt_table}")
            is_inferred = rel_map.get('inferred', False)
            
            # ì¶”ë¡  ê´€ê³„ì¸ ê²½ìš° ê°’ ë§¤í•‘ ë° ì‹¤ì œ ë°ì´í„° í™•ì¸ ê°•í™”
            if is_inferred:
                tgt_uri = self._find_target_instance_with_mapping(
                    tgt_table, fk_val, data, src_col
                )
            else:
                # ì¼ë°˜ FK ê´€ê³„: íƒ€ê²Ÿ í…Œì´ë¸”ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ì°¾ê¸°
                tgt_uri = self._find_target_instance(tgt_table, fk_val, data)
            
            if tgt_uri:
                # ì‹¤ì œ ë°ì´í„°ì—ì„œ ì°¾ì€ ê²½ìš° ê´€ê³„ ìƒì„±
                prop_uri = URIRef(self.ns[relation_name])
                self.graph.add((src_uri, prop_uri, tgt_uri))
            elif enable_virtual_entities and is_inferred:
                # ì¶”ë¡  ê´€ê³„ì´ê³  ì‹¤ì œ ë°ì´í„°ì— ì—†ì„ ë•Œë§Œ ê°€ìƒ ì—”í‹°í‹° ìƒì„±
                # ì¤‘ë³µ ì²´í¬ í›„ ìƒì„±
                virtual_uri = self._create_virtual_entity_safe(tgt_table, fk_val)
                if virtual_uri:
                    prop_uri = URIRef(self.ns[relation_name])
                    self.graph.add((src_uri, prop_uri, virtual_uri))
                    safe_print(f"[INFO] ì¶”ë¡  ê´€ê³„ ê°€ìƒ ë…¸ë“œ ìƒì„±: {table_name}.{src_col}='{fk_val}' -> {tgt_table}")
            elif enable_virtual_entities:
                # ì¼ë°˜ FK ê´€ê³„ì—ì„œë„ ê°€ìƒ ì—”í‹°í‹° ìƒì„± (ê¸°ì¡´ ë™ì‘ ìœ ì§€)
                virtual_uri = self._create_virtual_entity_safe(tgt_table, fk_val)
                if virtual_uri:
                    prop_uri = URIRef(self.ns[relation_name])
                    self.graph.add((src_uri, prop_uri, virtual_uri))
    
    def _create_dynamic_fk_relationship(self, src_uri: URIRef, table_name: str, row: pd.Series,
                                        columns: List[str], rel_map: Dict,
                                        data: Dict[str, pd.DataFrame], 
                                        enable_virtual_entities: bool):
        """ë™ì  FK ê´€ê³„ ìƒì„± (ì œì•½ì¡°ê±´ ë“±)"""
        type_col = rel_map.get('type_column')
        type_mapping = rel_map.get('type_mapping', {})
        src_col = rel_map.get('src_col')
        
        if not type_col or not src_col:
            return
        
        if type_col not in row or src_col not in row:
            return
        
        # ì ìš©ëŒ€ìƒìœ í˜• ê°’ í™•ì¸
        target_type = str(row[type_col]).strip()
        if not target_type or pd.isna(row[type_col]):
            return
        
        # íƒ€ì… ë§¤í•‘ì—ì„œ íƒ€ê²Ÿ í…Œì´ë¸” ì°¾ê¸°
        target_table = type_mapping.get(target_type)
        if not target_table:
            safe_print(f"[WARN] ì•Œ ìˆ˜ ì—†ëŠ” ì ìš©ëŒ€ìƒìœ í˜•: {target_type} (í…Œì´ë¸”: {table_name})")
            return
        
        # FK ê°’ í™•ì¸
        fk_val = str(row[src_col]).strip()
        if not fk_val or pd.isna(row[src_col]):
            return
        
        # ê´€ê³„ëª… ê°€ì ¸ì˜¤ê¸°
        relation_name = rel_map.get('relation', 'appliesTo')
        
        # íƒ€ê²Ÿ í…Œì´ë¸”ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ì°¾ê¸°
        tgt_uri = self._find_target_instance(target_table, fk_val, data)
        
        if tgt_uri:
            # ê´€ê³„ ìƒì„±
            prop_uri = URIRef(self.ns[relation_name])
            self.graph.add((src_uri, prop_uri, tgt_uri))
            safe_print(f"[INFO] ë™ì  FK ê´€ê³„ ìƒì„±: {table_name} -[{relation_name}]-> {target_table} ({fk_val})")
        elif enable_virtual_entities:
            # ê°€ìƒ ì—”í‹°í‹° ìƒì„±
            virtual_uri = self._create_virtual_entity(target_table, fk_val)
            if virtual_uri:
                prop_uri = URIRef(self.ns[relation_name])
                self.graph.add((src_uri, prop_uri, virtual_uri))
                safe_print(f"[INFO] ë™ì  FK ê´€ê³„ ìƒì„± (ê°€ìƒ ì—”í‹°í‹°): {table_name} -[{relation_name}]-> {target_table} ({fk_val})")
    
    def _find_target_instance(self, tgt_table: str, fk_val: str, 
                             data: Dict[str, pd.DataFrame]) -> Optional[URIRef]:
        """íƒ€ê²Ÿ í…Œì´ë¸”ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ì°¾ê¸°"""
        if tgt_table not in data:
            return None
        
        df = data[tgt_table]
        if df.empty:
            return None
        
        # ID ì»¬ëŸ¼ ì°¾ê¸°
        id_col = suggest_id_column(tgt_table, list(df.columns))
        
        # ë§¤ì¹­ë˜ëŠ” í–‰ ì°¾ê¸°
        matching_rows = df[df[id_col].astype(str).str.strip() == fk_val]
        if matching_rows.empty:
            return None
        
        row_id = str(matching_rows.iloc[0][id_col]).strip()
        safe_instance_id = self._make_uri_safe(f"{tgt_table}_{row_id}")
        return URIRef(self.ns[safe_instance_id])
    
    def _find_target_instance_with_mapping(self, tgt_table: str, fk_val: str,
                                          data: Dict[str, pd.DataFrame],
                                          src_col: str) -> Optional[URIRef]:
        """
        íƒ€ê²Ÿ í…Œì´ë¸”ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ì°¾ê¸° (ê°’ ë§¤í•‘ ì§€ì›)
        
        ì¶”ë¡  ê´€ê³„ì—ì„œ ì‚¬ìš©: ì „ëµìœ í˜• ê°’(offensive/defensive/ê³µê²©/ë°©ì–´)ì„
        ì‹¤ì œ ì„ë¬´ì •ë³´ ë°ì´í„°ì˜ ì„ë¬´ì¢…ë¥˜ ì»¬ëŸ¼ê³¼ ë§¤ì¹­
        """
        if tgt_table not in data:
            return None
        
        df = data[tgt_table]
        if df.empty:
            return None
        
        # ì „ëµìœ í˜• ì»¬ëŸ¼ì¸ ê²½ìš° ê°’ ë§¤í•‘ ì ìš©
        if src_col == 'ì „ëµìœ í˜•' and tgt_table == 'ì„ë¬´ì •ë³´':
            # ì„ë¬´ì •ë³´ í…Œì´ë¸”ì˜ ì„ë¬´ì¢…ë¥˜ ì»¬ëŸ¼ ì°¾ê¸°
            mission_type_col = None
            for col in df.columns:
                if 'ì„ë¬´ì¢…ë¥˜' in str(col) or 'mission_type' in str(col).lower():
                    mission_type_col = col
                    break
            
            if mission_type_col:
                # ê°’ ë§¤í•‘: offensive â†” ê³µê²©, defensive â†” ë°©ì–´
                mapped_values = self.STRATEGY_TYPE_MAPPING.get(fk_val.lower(), [fk_val])
                
                # ë§¤í•‘ëœ ê°’ë“¤ë¡œ ê²€ìƒ‰
                for mapped_val in mapped_values:
                    matching_rows = df[df[mission_type_col].astype(str).str.strip().str.lower() == mapped_val.lower()]
                    if not matching_rows.empty:
                        # ID ì»¬ëŸ¼ ì°¾ê¸°
                        id_col = suggest_id_column(tgt_table, list(df.columns))
                        row_id = str(matching_rows.iloc[0][id_col]).strip()
                        safe_print(f"[INFO] ì¶”ë¡  ê´€ê³„ ë§¤ì¹­ ì„±ê³µ: '{fk_val}' -> '{mapped_val}' (ì„ë¬´ì •ë³´_{row_id})")
                        safe_instance_id = self._make_uri_safe(f"{tgt_table}_{row_id}")
                        return URIRef(self.ns[safe_instance_id])
        
        # ì¼ë°˜ FK ê´€ê³„ë¡œ ì²˜ë¦¬ (ID ì»¬ëŸ¼ ì§ì ‘ ë§¤ì¹­)
        return self._find_target_instance(tgt_table, fk_val, data)
    
    def _process_coa_library_relations(self, coa_uri: URIRef, row: pd.Series, enable_virtual_entities: bool):
        """
        COA_Library í…Œì´ë¸”ì˜ íŠ¹ìˆ˜ ê´€ê³„ ì²˜ë¦¬
        í‚¤ì›Œë“œ, í•„ìš”ìì›, ì „ì¥í™˜ê²½_ì œì•½ ì»¬ëŸ¼ì„ ê´€ê³„ë¡œ ë³€í™˜
        """
        NS = self.ns  # í†µì¼ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚¬ìš©
        
        # [DEBUG]
        if "TW01" in str(coa_uri):
             safe_print(f"[DEBUG] Processing TW01. Row keys: {list(row.keys())}")
             safe_print(f"[DEBUG] ì í•©ìœ„í˜‘ìœ í˜• value: {row.get('ì í•©ìœ„í˜‘ìœ í˜•')}")
        
        # 1. í‚¤ì›Œë“œ -> respondsTo ê´€ê³„ (í‘œì¤€í™”)
        keywords = row.get('í‚¤ì›Œë“œ') or row.get('Keywords')
        if pd.notna(keywords):
            for threat in str(keywords).split(','):
                keyword_clean = threat.strip()
                if keyword_clean:
                    threat_uri = URIRef(NS[self._make_uri_safe(keyword_clean)])
                    # ìœ„í˜‘ ë…¸ë“œê°€ ì—†ìœ¼ë©´ ìƒì„± (ê°œë…ì  ë…¸ë“œ)
                    if (threat_uri, RDF.type, None) not in self.graph:
                        self.graph.add((threat_uri, RDF.type, URIRef(NS["Threat"])))
                    # [MOD] countersThreat ëŒ€ì‹  í‘œì¤€í™”ëœ respondsTo ì‚¬ìš©
                    self.graph.add((coa_uri, URIRef(NS["respondsTo"]), threat_uri))
        
        # 2. í•„ìš”ìì› -> requiresResource ê´€ê³„
        resources = row.get('í•„ìš”ìì›') or row.get('Required_Resources')
        if pd.notna(resources):
            for resource in str(resources).split(','):
                resource_clean = resource.strip()
                if resource_clean:
                    res_uri = URIRef(NS[self._make_uri_safe(resource_clean)])
                    # ìì› ë…¸ë“œê°€ ì—†ìœ¼ë©´ ìƒì„±
                    if (res_uri, RDF.type, None) not in self.graph:
                        self.graph.add((res_uri, RDF.type, URIRef(NS["Resource"])))
                    self.graph.add((coa_uri, URIRef(NS["requiresResource"]), res_uri))
        
        # 3. ì „ì¥í™˜ê²½_ì œì•½ -> hasConstraint ê´€ê³„
        constraints = row.get('ì „ì¥í™˜ê²½_ì œì•½') or row.get('Environmental_Constraints')
        if pd.notna(constraints):
            for constraint in str(constraints).split(','):
                constraint_clean = constraint.strip()
                if constraint_clean:
                    con_uri = URIRef(NS[self._make_uri_safe(constraint_clean)])
                    # ì œì•½ ë…¸ë“œê°€ ì—†ìœ¼ë©´ ìƒì„±
                    if (con_uri, RDF.type, None) not in self.graph:
                        self.graph.add((con_uri, RDF.type, URIRef(NS["Constraint"])))
                    self.graph.add((coa_uri, URIRef(NS["hasConstraint"]), con_uri))
        
        # 4. ì›Œê²Œì„_ëª¨ì˜_ë¶„ì„_ìŠ¹ë¥  -> hasSuccessRateValue (Literal)
        success_rate = row.get('ì›Œê²Œì„_ëª¨ì˜_ë¶„ì„_ìŠ¹ë¥ ') or row.get('ì˜ˆìƒì„±ê³µë¥ ') or row.get('Estimated_Success_Rate')
        if pd.notna(success_rate):
            try:
                rate_val = float(success_rate)
                success_rate_prop = URIRef(NS["hasSuccessRateValue"])
                self.graph.add((coa_uri, success_rate_prop, Literal(rate_val, datatype=XSD.float)))
            except:
                pass
        
        # 5. í™˜ê²½ í˜¸í™˜ì„± -> compatibleWith ê´€ê³„ (NEW)
        compatible_envs = row.get('í™˜ê²½í˜¸í™˜ì„±') or row.get('Environmental_Compatibility')
        if pd.notna(compatible_envs):
            for env in str(compatible_envs).split(','):
                env_clean = env.strip()
                if env_clean:
                    env_uri = URIRef(NS[self._make_uri_safe(env_clean)])
                    # í™˜ê²½ ë…¸ë“œê°€ ì—†ìœ¼ë©´ ìƒì„±
                    if (env_uri, RDF.type, None) not in self.graph:
                        self.graph.add((env_uri, RDF.type, URIRef(NS["Environment"])))
                    self.graph.add((coa_uri, URIRef(NS["compatibleWith"]), env_uri))
                    # í˜¸í™˜ì„± ì ìˆ˜ ì¶”ê°€ (í˜¸í™˜ í™˜ê²½ì€ ë†’ì€ ì ìˆ˜)
                    compatibility_score_prop = URIRef(NS["compatibilityScore"])
                    self.graph.add((coa_uri, compatibility_score_prop, Literal(1.0, datatype=XSD.float)))
        
        # 6. í™˜ê²½ ë¹„í˜¸í™˜ì„± -> incompatibleWith ê´€ê³„ (NEW)
        incompatible_envs = row.get('í™˜ê²½ë¹„í˜¸í™˜ì„±') or row.get('Environmental_Incompatibility')
        if pd.notna(incompatible_envs):
            for env in str(incompatible_envs).split(','):
                env_clean = env.strip()
                if env_clean:
                    env_uri = URIRef(NS[self._make_uri_safe(env_clean)])
                    # í™˜ê²½ ë…¸ë“œê°€ ì—†ìœ¼ë©´ ìƒì„±
                    if (env_uri, RDF.type, None) not in self.graph:
                        self.graph.add((env_uri, RDF.type, URIRef(NS["Environment"])))
                    self.graph.add((coa_uri, URIRef(NS["incompatibleWith"]), env_uri))
                    # ë¹„í˜¸í™˜ì„± ì ìˆ˜ ì¶”ê°€ (ë¹„í˜¸í™˜ í™˜ê²½ì€ ë‚®ì€ ì ìˆ˜)
                    compatibility_score_prop = URIRef(NS["compatibilityScore"])
                    self.graph.add((coa_uri, compatibility_score_prop, Literal(0.2, datatype=XSD.float)))
        
        # 7. ì í•©ìœ„í˜‘ìœ í˜• -> ì í•©ìœ„í˜‘ìœ í˜• (Literal List) + respondsTo (Semantic Link)
        threat_types = row.get('ì í•©ìœ„í˜‘ìœ í˜•') or row.get('Suitable_Threat_Types')
        if pd.notna(threat_types):
            prop_uri = URIRef(NS["ì í•©ìœ„í˜‘ìœ í˜•"])
            responds_to_prop = URIRef(NS["respondsTo"])
            for t_type in str(threat_types).split(','):
                t_clean = t_type.strip()
                if t_clean:
                    # ê¸°ì¡´ ì£¼ì„ ë¦¬í„°ëŸ´ ìœ ì§€
                    self.graph.add((coa_uri, prop_uri, Literal(t_clean)))
                    
                    # [NEW] ë¼ë²¨ ë° í‚¤ì›Œë“œ ê¸°ë°˜ ì‹œë§¨í‹± ë§í¬(respondsTo) ìë™ ìƒì„±
                    keyword_prop = URIRef(NS["ëŒ€í‘œí‚¤ì›Œë“œ"])
                    for threat_master_uri in self.graph.subjects(RDF.type, URIRef(NS["ìœ„í˜‘ìœ í˜•_ë§ˆìŠ¤í„°"])):
                        found = False
                        # 1. ë¼ë²¨(ëª…ì¹­/ìœ„í˜‘ìœ í˜•ëª…) ëŒ€ì¡°
                        for label in self.graph.objects(threat_master_uri, RDFS.label):
                            if str(label) == t_clean:
                                found = True
                                break
                        
                        # 2. ëŒ€í‘œí‚¤ì›Œë“œ ëŒ€ì¡° (ì‰¼í‘œ ë¶„ë¦¬ í›„ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸)
                        if not found:
                            for keywords_literal in self.graph.objects(threat_master_uri, keyword_prop):
                                kw_list = [k.strip() for k in str(keywords_literal).split(',')]
                                if t_clean in kw_list:
                                    found = True
                                    break
                        
                        if found:
                            self.graph.add((coa_uri, responds_to_prop, threat_master_uri))
                            # safe_print(f"[DEBUG] Semantic Link: {coa_uri} -[respondsTo]-> {threat_master_uri} (Matched via: {t_clean})")
        
        # [NEW] 7.1 ì„¤ëª…(Description) í•„ë“œ ê¸°ë°˜ í´ë°± ë§¤ì¹­
        # ì í•©ìœ„í˜‘ìœ í˜•ì´ ë¹„ì–´ìˆê±°ë‚˜ ë¶€ì¡±í•œ ê²½ìš° ì„¤ëª…ì„ ìŠ¤ìº”í•˜ì—¬ ì¶”ê°€ ë§¤í•‘ ì‹œë„
        description = row.get('ì„¤ëª…') or row.get('Description')
        if pd.notna(description) and str(description).strip():
            desc_text = str(description)
            responds_to_prop = URIRef(NS["respondsTo"])
            keyword_prop = URIRef(NS["ëŒ€í‘œí‚¤ì›Œë“œ"])
            
            for threat_master_uri in self.graph.subjects(RDF.type, URIRef(NS["ìœ„í˜‘ìœ í˜•_ë§ˆìŠ¤í„°"])):
                # ì´ë¯¸ ë§¤í•‘ëœ ê²½ìš°ëŠ” ìŠ¤í‚µ (ì„ íƒ ì‚¬í•­)
                if (coa_uri, responds_to_prop, threat_master_uri) in self.graph:
                    continue
                    
                found = False
                # 1. ë¼ë²¨/ëª…ì¹­ í¬í•¨ ì—¬ë¶€ í™•ì¸
                for label in self.graph.objects(threat_master_uri, RDFS.label):
                    if str(label) in desc_text:
                        found = True
                        break
                
                # 2. ëŒ€í‘œí‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
                if not found:
                    for keywords_literal in self.graph.objects(threat_master_uri, keyword_prop):
                        kw_list = [k.strip() for k in str(keywords_literal).split(',') if k.strip()]
                        for kw in kw_list:
                            if kw in desc_text:
                                found = True
                                break
                        if found: break
                
                if found:
                    self.graph.add((coa_uri, responds_to_prop, threat_master_uri))
                    # safe_print(f"[DEBUG] Description Fallback Link: {coa_uri} -[respondsTo]-> {threat_master_uri}")

        # 8. ìì›ìš°ì„ ìˆœìœ„ -> ìì›ìš°ì„ ìˆœìœ„ (Literal List)
        res_priorities = row.get('ìì›ìš°ì„ ìˆœìœ„') or row.get('Resource_Priorities')
        if pd.notna(res_priorities):
            prop_uri = URIRef(NS["ìì›ìš°ì„ ìˆœìœ„"])
            for res_p in str(res_priorities).split(','):
                p_clean = res_p.strip()
                if p_clean:
                    self.graph.add((coa_uri, prop_uri, Literal(p_clean)))
                    
        # 9. ì—°ê³„ë°©ì±… -> hasRelatedCOA (Relation) - ì„œìˆ í˜• í•„ë“œëŠ” ì œê±°, ê´€ê³„ë§Œ ìœ ì§€
        # ê°’ ì˜ˆì‹œ: COA_ATK_001(í›„í–‰)
        # [MOD] ì—°ê³„ë°©ì±… ì„œìˆ í˜• í•„ë“œëŠ” ì œê±°ë˜ì—ˆìœ¼ë‚˜, Excel ë°ì´í„°ì—ì„œ ì½ì–´ì„œ ê´€ê³„ë§Œ ìƒì„±
        related_coas = row.get('ì—°ê³„ë°©ì±…') or row.get('Related_COAs')
        if pd.notna(related_coas):
            for r_coa in str(related_coas).split(','):
                r_clean = r_coa.strip()
                if r_clean:
                    # ê°„ë‹¨í•œ íŒŒì‹±: IDë§Œ ì¶”ì¶œ (ê´„í˜¸ ì œê±°)
                    r_id = r_clean.split('(')[0].strip()
                    if r_id:
                        r_uri = URIRef(NS[self._make_uri_safe(r_id)])
                        self.graph.add((coa_uri, URIRef(NS["hasRelatedCOA"]), r_uri))
        
        # 10. ì ìš©ë¶€ëŒ€ -> participating_units (Literal)
        participating_units = row.get('ì ìš©ë¶€ëŒ€') or row.get('Participating_Units')
        if pd.notna(participating_units):
            self.graph.add((coa_uri, URIRef(NS["participating_units"]), Literal(str(participating_units))))
            # also as hasMainEffort for backward compatibility
            self.graph.add((coa_uri, URIRef(NS["hasMainEffort"]), Literal(str(participating_units))))

        # 11. ì „ìˆ ê·¸ë˜í”½ -> hasTacticalGraphics (Literal)
        tactical_graphics = row.get('ì „ìˆ ê·¸ë˜í”½') or row.get('Tactical_Graphics')
        if pd.notna(tactical_graphics):
            self.graph.add((coa_uri, URIRef(NS["hasTacticalGraphics"]), Literal(str(tactical_graphics))))

        # 12. ë‹¨ê³„ì •ë³´ -> hasPhasingInfo (Literal)
        phasing_info = row.get('ë‹¨ê³„ì •ë³´') or row.get('Phasing_Info')
        if pd.notna(phasing_info):
            self.graph.add((coa_uri, URIRef(NS["hasPhasingInfo"]), Literal(str(phasing_info))))

        # 13. ì£¼ë…¸ë ¥ì—¬ë¶€ -> isMainEffort (Literal)
        is_main_effort = row.get('ì£¼ë…¸ë ¥ì—¬ë¶€') or row.get('Main_Effort')
        if pd.notna(is_main_effort):
            self.graph.add((coa_uri, URIRef(NS["isMainEffort"]), Literal(str(is_main_effort))))

        # 14. ì‹œê°í™”ìŠ¤íƒ€ì¼ -> hasVisualStyle (Literal)
        vis_style = row.get('ì‹œê°í™”ìŠ¤íƒ€ì¼') or row.get('Visual_Style')
        if pd.notna(vis_style):
            self.graph.add((coa_uri, URIRef(NS["hasVisualStyle"]), Literal(str(vis_style))))

        # [REMOVED] ì—°ê³„ë°©ì±… ì„œìˆ í˜• í•„ë“œëŠ” ë” ì´ìƒ ì˜¨í†¨ë¡œì§€ì— ì €ì¥í•˜ì§€ ì•ŠìŒ (RAGë¡œ ì´ë™)

        # [REMOVED] ì ëŒ€ì‘ì „ìˆ  í•„ë“œ ì œê±° - RAG ë¬¸ì„œë¡œ ì´ë™
        # ì ëŒ€ì‘ì „ìˆ  ì„¤ëª…ì€ RAG ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•˜ë„ë¡ ë³€ê²½
        
        # 7. COA íƒ€ì… ì¶”ë¡  (ID ê¸°ë°˜) - COA_Library_COA_DEF_001 -> DefenseCOA
        coa_id = str(row.get('COA_ID', ''))
        if coa_id:
            specific_type = None
            if "COA_DEF" in coa_id:
                specific_type = "DefenseCOA"
            elif "COA_OFF" in coa_id:
                specific_type = "OffensiveCOA"
            elif "COA_CNT" in coa_id or "COA_CAT" in coa_id:
                specific_type = "CounterAttackCOA"
            elif "COA_PRE" in coa_id:
                specific_type = "PreemptiveCOA"
            elif "COA_DET" in coa_id:
                specific_type = "DeterrenceCOA"
            elif "COA_MAN" in coa_id:
                specific_type = "ManeuverCOA"
            elif "COA_INF" in coa_id:
                specific_type = "InformationOpsCOA"
            
            if specific_type:
                self.graph.add((coa_uri, RDF.type, URIRef(NS[specific_type])))
                # COA ìƒìœ„ í´ë˜ìŠ¤ë„ ì¶”ê°€
                self.graph.add((coa_uri, RDF.type, URIRef(NS["COA"])))
                
                # [NEW] íŒŒì¼ëŸ¿ ì‹œê°í™” ë°ì´í„° ì£¼ì… (Visualization Pilot Data Injection)
                # ì—‘ì…€ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ í´ë°±ìš©ìœ¼ë¡œ ë°ì´í„° ì£¼ì…
                vis_props = {}
                
                # ê¸°ì¡´ì— ë“±ë¡ëœ ì†ì„± í™•ì¸ìš©
                has_efforts = (coa_uri, URIRef(NS["hasMainEffort"]), None) in self.graph
                has_graphics = (coa_uri, URIRef(NS["hasTacticalGraphics"]), None) in self.graph
                has_phasing = (coa_uri, URIRef(NS["hasPhasingInfo"]), None) in self.graph

                if not has_efforts or not has_graphics or not has_phasing:
                    if "COA_DEF" in coa_id:
                         vis_props = {
                            "hasMainEffort": "ì œ1ê¸°ê³„í™”ë³´ë³‘ì—¬ë‹¨",
                            "hasPhasingInfo": "1ë‹¨ê³„:ì§€ì—°ì „,2ë‹¨ê³„:ë°©ì–´,3ë‹¨ê³„:í¸ì œí™”ë ¥ì§€ì›",
                            "hasActionType": "Defend",
                            "hasTacticalGraphics": "Block:[(37.8,126.9),(37.9,127.1)]", # íŒŒì£¼ ì¶•ì„  ì°¨ë‹¨
                            "hasExpectedEffect": "ì  ê¸°ê³„í™”ë¶€ëŒ€ ì§„ì¶œ 48ì‹œê°„ ì§€ì—°"
                         }
                    elif "COA_OFF" in coa_id: # Counter Attack í¬í•¨
                         vis_props = {
                            "hasMainEffort": "ì œ7ê¸°ë™êµ°ë‹¨",
                            "hasPhasingInfo": "1ë‹¨ê³„:ì ‘ì ê¸°ë™,2ë‹¨ê³„:ëŒíŒŒ,3ë‹¨ê³„:ëª©í‘œí™•ë³´",
                            "hasActionType": "Attack",
                            "hasTacticalGraphics": "Axis:[(37.5,127.0),(37.9,126.8),(38.1,126.6)]", # ê°œì„± ë°©í–¥ ì§„ê²©
                            "hasExpectedEffect": "ì  ì§€íœ˜ì†Œ ë¬´ë ¥í™” ë° ì˜í†  íšŒë³µ"
                         }
                    elif "COA_PRE" in coa_id or "COA_DET" in coa_id:
                         vis_props = {
                            "hasMainEffort": "ì „ëµë¯¸ì‚¬ì¼ì‚¬ë ¹ë¶€",
                            "hasPhasingInfo": "1ë‹¨ê³„:í‘œì ì‹ë³„,2ë‹¨ê³„:ì„ ì œíƒ€ê²©,3ë‹¨ê³„:í”¼í•´í‰ê°€",
                            "hasActionType": "Strike",
                            "hasTacticalGraphics": "PointTarget:[(39.0,125.7)]", # í‰ì–‘ ì¸ê·¼
                            "hasExpectedEffect": "ì  ë¯¸ì‚¬ì¼ ë°œì‚¬ ëŠ¥ë ¥ 70% ê°ì†Œ"
                         }
                    
                    for prop, val in vis_props.items():
                        # í•´ë‹¹ ì†ì„±ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
                        if (coa_uri, URIRef(NS[prop]), None) not in self.graph:
                            self.graph.add((coa_uri, URIRef(NS[prop]), Literal(val)))

    def _process_threat_situation_relations(self, threat_uri: URIRef, row: pd.Series, data: Dict[str, pd.DataFrame]):
        """
        ìœ„í˜‘ìƒí™© í…Œì´ë¸”ì˜ íŠ¹ìˆ˜ ê´€ê³„ ì²˜ë¦¬
        ìœ„í˜‘ìœ í˜• ì»¬ëŸ¼ì„ URI ê¸°ë°˜ì˜ ns:Threat ê´€ê³„ë¡œ ë³€í™˜í•˜ì—¬ ì „ëµì²´ì¸ ì—°ê²°ì„± í™•ë³´
        [NEW] ìœ„í˜‘ìƒí™©ê³¼ í˜„ì¬ ê°€ìš©ìì› ê°„ì˜ ìŠ¤ëƒ…ìƒ· ê´€ê³„(hasResourceSnapshot) ìƒì„±
        """
        NS = self.ns
        
        # 1. ìœ„í˜‘ìœ í˜• -> ns:Threat ê´€ê³„ (ì „ëµì²´ì¸ íƒìƒ‰ìš© í•µì‹¬ ì—°ê²°ê³ ë¦¬)
        # [FIX] ë§ˆìŠ¤í„° ë°ì´í„° URI í˜•ì‹ì— ë§ê²Œ ìƒì„± (ìœ„í˜‘ìœ í˜•_ë§ˆìŠ¤í„°_THR_TYPE_xxx)
        threat_type = row.get('ìœ„í˜‘ìœ í˜•ì½”ë“œ') or row.get('ìœ„í˜‘ìœ í˜•') or row.get('threat_type')
        if pd.notna(threat_type):
            type_clean = str(threat_type).strip()
            if type_clean:
                # ë§ˆìŠ¤í„° í…Œì´ë¸” ID íŒ¨í„´ì¸ ê²½ìš° í•´ë‹¹ URI ì‚¬ìš©
                if type_clean.startswith("THR_TYPE_"):
                    keyword_uri = URIRef(self.ns[self._make_uri_safe(f"ìœ„í˜‘ìœ í˜•_ë§ˆìŠ¤í„°_{type_clean}")])
                else:
                    keyword_uri = URIRef(NS[self._make_uri_safe(type_clean)])
                    
                if (keyword_uri, RDF.type, None) not in self.graph:
                    self.graph.add((keyword_uri, RDF.type, URIRef(NS["ìœ„í˜‘ìœ í˜•_ë§ˆìŠ¤í„°"])))
                self.graph.add((threat_uri, URIRef(NS["hasìœ„í˜‘ìœ í˜•"]), keyword_uri))
        
        # 2. [NEW]ê°€ìš©ìì› ìŠ¤ëƒ…ìƒ· ì—°ê²°
        # ìœ„í˜‘ ìƒí™© ë°œìƒ ì‹œì ì˜ ê°€ìš© ìì›ë“¤ì„ ì—°ê²°í•˜ì—¬ ì¶”ë¡  ì—”ì§„ì´ ìì› ê°€ìš©ì„±ì„ íŒŒì•…í•  ìˆ˜ ìˆê²Œ í•¨
        if 'ê°€ìš©ìì›' in data:
            resource_df = data['ê°€ìš©ìì›']
            # ID ì»¬ëŸ¼ ê°ì§€
            res_id_col = self.get_id_column('ê°€ìš©ìì›', list(resource_df.columns))
            
            for _, res_row in resource_df.iterrows():
                res_id = str(res_row.get(res_id_col)).strip()
                if res_id and res_id.lower() != 'nan':
                    res_instance_id = self._make_uri_safe(f"ê°€ìš©ìì›_{res_id}")
                    res_uri = URIRef(NS[res_instance_id])
                    
                    # ê´€ê³„ ì¶”ê°€: Threat -> hasResourceSnapshot -> Resource
                    self.graph.add((threat_uri, URIRef(NS["hasResourceSnapshot"]), res_uri))
            
            # safe_print(f"[DEBUG] ìœ„í˜‘ìƒí™© {threat_uri}ì— ê°€ìš©ìì› ìŠ¤ëƒ…ìƒ· ì—°ê²° ì™„ë£Œ")
    
    def _create_virtual_entity(self, entity_type: str, entity_id: str) -> Optional[URIRef]:
        """ê°€ìƒ ì—”í‹°í‹° ìƒì„± (ë ˆê±°ì‹œ í˜¸í™˜ìš©)"""
        return self._create_virtual_entity_safe(entity_type, entity_id)
    
    def _create_virtual_entity_safe(self, entity_type: str, entity_id: str) -> Optional[URIRef]:
        """
        ê°€ìƒ ì—”í‹°í‹° ìƒì„± (ì¤‘ë³µ ì²´í¬ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€)
        
        Args:
            entity_type: ì—”í‹°í‹° íƒ€ì… (ì˜ˆ: 'ì„ë¬´ì •ë³´')
            entity_id: ì—”í‹°í‹° ID (ì˜ˆ: 'offensive')
        
        Returns:
            ê°€ìƒ ì—”í‹°í‹° URI ë˜ëŠ” None (ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš°)
        """
        if not entity_type or not entity_id:
            return None
        
        # URI-safe ë¬¸ìì—´ ìƒì„±
        entity_type_clean = self._make_uri_safe(entity_type)
        entity_id_clean = self._make_uri_safe(str(entity_id))
        
        virtual_id = f"{entity_type_clean}_{entity_id_clean}"
        virtual_uri = URIRef(self.virtual_ns[virtual_id]) # Use virtual_ns for virtual entities
        
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ìƒì„± ë°©ì§€)
        if (virtual_uri, RDF.type, None) in self.graph:
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ê¸°ì¡´ URI ë°˜í™˜
            return virtual_uri
        
        # ê°€ìƒ ì—”í‹°í‹° ìƒì„±
        class_uri = URIRef(self.ns[entity_type_clean])
        self.graph.add((virtual_uri, RDF.type, class_uri))
        self.graph.add((virtual_uri, RDFS.label, Literal(f"ê°€ìƒ_{entity_type_clean}_{entity_id_clean}")))
        
        # í†µê³„ ì¹´ìš´íŠ¸ ì¦ê°€
        self.virtual_entities_count += 1
        
        # [NEW] ê°€ìƒ ì—”í‹°í‹° ì†ì„± ì¶”ê°€ (ì›ë³¸ ë©”íƒ€ë°ì´í„° ë³´ì¡´)
        # (í–¥í›„ í†µê³„ì—ì„œ ì‹¤ì œ ë°ì´í„°ì™€ êµ¬ë¶„ ê°€ëŠ¥)
        self.graph.add((virtual_uri, URIRef(self.ns["isVirtualEntity"]), Literal(True, datatype=XSD.boolean)))
        self.graph.add((virtual_uri, URIRef(self.ns["virtualEntitySource"]), Literal("inferred_relation")))
        
        return virtual_uri
    
    # ========== OntologyManager í˜¸í™˜ ë©”ì„œë“œ ì¶”ê°€ ==========
    
    def _is_schema_triple(self, s, p, o) -> bool:
        """ìŠ¤í‚¤ë§ˆ ì •ì˜ íŠ¸ë¦¬í”Œì¸ì§€ í™•ì¸ (í´ë˜ìŠ¤/ì†ì„±/ê³„ì¸µêµ¬ì¡° ë“±)"""
        from rdflib import RDF, RDFS, OWL
        # ê¸°ìˆ ì  ì†ì„±ë“¤
        if p in [RDFS.subClassOf, RDFS.domain, RDFS.range, RDFS.subPropertyOf,
                 OWL.inverseOf, OWL.equivalentClass, OWL.equivalentProperty,
                 OWL.disjointWith, OWL.unionOf, OWL.intersectionOf]:
            return True
        # í´ë˜ìŠ¤/ì†ì„± ì •ì˜ íƒ€ì…
        if p == RDF.type and o in [OWL.Class, OWL.ObjectProperty, OWL.DatatypeProperty, 
                                 OWL.TransitiveProperty, OWL.SymmetricProperty, 
                                 OWL.FunctionalProperty, OWL.InverseFunctionalProperty,
                                 RDFS.Class, OWL.Ontology, OWL.Restriction, OWL.Axiom]:
            return True
        return False

    def save_graph(self, output_path: Optional[str] = None,
                   save_schema_separately: bool = True,
                   save_instances_separately: bool = True,
                   save_reasoned_separately: bool = False,
                   enable_semantic_inference: bool = True,
                   reasoned_graph: Optional[Graph] = None,
                   cleanup_old_files: bool = True,
                   backup_old_files: bool = True) -> Dict[str, Any]:
        """
        RDF ê·¸ë˜í”„ë¥¼ TTL íŒŒì¼ë¡œ ì €ì¥ (3ë‹¨ê³„ êµ¬ì¡°: schema.ttl + instances.ttl + instances_reasoned.ttl)
        
        Args:
            output_path: ì¶œë ¥ ê²½ë¡œ (ê¸°ë³¸ê°’: self.ontology_path)
            save_schema_separately: schema.ttl ì €ì¥ ì—¬ë¶€
            save_instances_separately: instances.ttl ì €ì¥ ì—¬ë¶€
            save_reasoned_separately: instances_reasoned.ttl ì €ì¥ ì—¬ë¶€ (ì¶”ë¡  ê²°ê³¼ í¬í•¨)
            enable_semantic_inference: ì¶”ë¡  ê·¸ë˜í”„ ìƒì„± ì‹œ ì˜ë¯¸ ê¸°ë°˜ ì¶”ë¡  í™œì„±í™” ì—¬ë¶€
            reasoned_graph: [NEW] ì´ë¯¸ ê³„ì‚°ëœ ì¶”ë¡  ê·¸ë˜í”„ê°€ ìˆëŠ” ê²½ìš° ì „ë‹¬ (ì¤‘ë³µ ì¶”ë¡  ë°©ì§€)
            cleanup_old_files: ê¸°ì¡´ ì¤‘ê°„ ìƒì„±ë¬¼ íŒŒì¼ ì‚­ì œ ì—¬ë¶€
            backup_old_files: ê¸°ì¡´ íŒŒì¼ ë°±ì—… ì—¬ë¶€
        
        Returns:
            Dict: ì €ì¥ í†µê³„ (success, schema_triples, instances_triples, reasoned_triples ë“±)
        """
        stats = {
            "success": False,
            "schema_triples": 0,
            "instances_triples": 0,
            "reasoned_triples": 0,
            "message": ""
        }
        
        if not RDFLIB_AVAILABLE or self.graph is None:
            stats["message"] = "RDFLib not available or graph is None"
            safe_print(f"[WARN] {stats['message']}")
            return stats
        
        if output_path is None:
            output_path = self.ontology_path
        
        # output_pathê°€ íŒŒì¼ ê²½ë¡œì¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œì¸ì§€ í™•ì¸
        output_path_obj = Path(output_path)
        if output_path_obj.suffix in ['.ttl', '.owl', '.rdf']:
            # íŒŒì¼ ê²½ë¡œë¡œ ì „ë‹¬ëœ ê²½ìš°, ë””ë ‰í† ë¦¬ ê²½ë¡œë¡œ ë³€í™˜
            ontology_dir = output_path_obj.parent
            safe_print(f"[WARN] output_pathê°€ íŒŒì¼ ê²½ë¡œë¡œ ì „ë‹¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë””ë ‰í† ë¦¬ ê²½ë¡œë¡œ ë³€í™˜: {ontology_dir}")
        else:
            # ë””ë ‰í† ë¦¬ ê²½ë¡œë¡œ ì „ë‹¬ëœ ê²½ìš°
            ontology_dir = output_path_obj
        
        ontology_dir.mkdir(parents=True, exist_ok=True)
        
        # [NEW] ì €ì¥ ì „ ëª¨ë“  ì¤‘ìš” íŒŒì¼ ë°±ì—… (ì•ˆì •ì„± ê°•í™”)
        if backup_old_files:
            for filename in ["schema.ttl", "instances.ttl", "instances_reasoned.ttl"]:
                source_file = ontology_dir / filename
                if source_file.exists():
                    try:
                        backup_dir = ontology_dir / "backup"
                        backup_dir.mkdir(parents=True, exist_ok=True)
                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                        backup_path = backup_dir / f"{filename}.backup_{timestamp}"
                        shutil.copy2(source_file, backup_path)
                        safe_print(f"[INFO] ì‚¬ì „ ë°±ì—… ì™„ë£Œ: {backup_path}")
                    except Exception as backup_e:
                        safe_print(f"[WARN] ì‚¬ì „ ë°±ì—… ì‹¤íŒ¨ ({filename}): {backup_e}")
        
        try:
            # 1. ìŠ¤í‚¤ë§ˆë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥ (schema.ttl)
            if save_schema_separately:
                schema_graph = Graph()
                # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”© ë³µì‚¬
                for prefix, namespace in self.graph.namespaces():
                    schema_graph.bind(prefix, namespace)
                
                # ìŠ¤í‚¤ë§ˆ ê´€ë ¨ íŠ¸ë¦¬í”Œë§Œ ì¶”ì¶œ
                for s, p, o in self.graph:
                    if self._is_schema_triple(s, p, o):
                        schema_graph.add((s, p, o))
                
                schema_path = ontology_dir / "schema.ttl"
                schema_graph.serialize(destination=str(schema_path), format="turtle")
                stats["schema_triples"] = len(list(schema_graph.triples((None, None, None))))
                safe_print(f"[INFO] ìŠ¤í‚¤ë§ˆ ì €ì¥ ì™„ë£Œ: {schema_path} ({stats['schema_triples']} triples)")
            
            # 2. ì¸ìŠ¤í„´ìŠ¤ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥ (instances.ttl)
            if save_instances_separately:
                instances_graph = Graph()
                # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”© ë³µì‚¬
                for prefix, namespace in self.graph.namespaces():
                    instances_graph.bind(prefix, namespace)
                
                # ì¸ìŠ¤í„´ìŠ¤ ë°ì´í„°ë§Œ ì¶”ì¶œ (ìŠ¤í‚¤ë§ˆ ì œì™¸ ë° ì¶”ë¡  ê²°ê³¼ ì œì™¸)
                inferred_triples_count = 0
                for s, p, o in self.graph:
                    # [MOD] ìŠ¤í‚¤ë§ˆ íŠ¸ë¦¬í”Œ ì œì™¸
                    if self._is_schema_triple(s, p, o):
                        continue
                        
                    # [FIX] ì¶”ë¡ ëœ íŠ¸ë¦¬í”Œ ì œì™¸ (ì›ë³¸ ìˆœìˆ˜ì„± ìœ ì§€)
                    if self._is_inferred_triple(s, p, o):
                        inferred_triples_count += 1
                        continue
                        
                    instances_graph.add((s, p, o))
                
                if inferred_triples_count > 0:
                    safe_print(f"[INFO] {inferred_triples_count}ê°œì˜ ì¶”ë¡ ëœ íŠ¸ë¦¬í”Œì„ instances.ttl ì €ì¥ì—ì„œ ì œì™¸í–ˆìŠµë‹ˆë‹¤.")
                
                instances_path = ontology_dir / "instances.ttl"
                instances_graph.serialize(destination=str(instances_path), format="turtle")
                stats["instances_triples"] = len(list(instances_graph.triples((None, None, None))))
                safe_print(f"[INFO] ì¸ìŠ¤í„´ìŠ¤ ì €ì¥ ì™„ë£Œ: {instances_path} ({stats['instances_triples']} triples)")
            
            # 3. ì¶”ë¡ ëœ ê·¸ë˜í”„ ì €ì¥ (instances_reasoned.ttl)
            if save_reasoned_separately:
                # [FIX] reasoned_graphê°€ ì¸ìë¡œ ì „ë‹¬ë˜ì—ˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒì„±
                if reasoned_graph is None:
                    safe_print("[INFO] ì €ì¥ìš© ì¶”ë¡  ê·¸ë˜í”„ ìƒì„± ì¤‘...")
                    reasoned_graph = self.generate_reasoned_graph(enable_semantic_inference=enable_semantic_inference)
                else:
                    safe_print("[INFO] ì „ë‹¬ë°›ì€ ì¶”ë¡  ê·¸ë˜í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤. (ì¤‘ë³µ ìƒì„± ìŠ¤í‚µ)")
                
                if reasoned_graph:
                    reasoned_instances_graph = Graph()
                    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”© ë³µì‚¬
                    for prefix, namespace in reasoned_graph.namespaces():
                        reasoned_instances_graph.bind(prefix, namespace)
                    
                    # ì¶”ë¡ ëœ ê·¸ë˜í”„ì˜ ëª¨ë“  íŠ¸ë¦¬í”Œ ë³µì‚¬ (ìŠ¤í‚¤ë§ˆ ì œì™¸)
                    for s, p, o in reasoned_graph:
                        # [MOD] ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìŠ¤í‚¤ë§ˆ ì œì™¸
                        if self._is_schema_triple(s, p, o):
                            continue
                        reasoned_instances_graph.add((s, p, o))
                    
                    reasoned_path = ontology_dir / "instances_reasoned.ttl"
                    reasoned_instances_graph.serialize(destination=str(reasoned_path), format="turtle")
                    stats["reasoned_triples"] = len(list(reasoned_instances_graph.triples((None, None, None))))
                    safe_print(f"[INFO] ì¶”ë¡ ëœ ì¸ìŠ¤í„´ìŠ¤ ì €ì¥ ì™„ë£Œ: {reasoned_path} ({stats['reasoned_triples']} triples)")
                    
                    # [FIX] ì›ë³¸ ì˜¤ì—¼ ë°©ì§€ë¥¼ ìœ„í•´ self.graphë¥¼ reasoned_graphë¡œ ìë™ êµì²´í•˜ëŠ” ë¡œì§ ì œê±°
                    # self.graph = reasoned_graph
                    # safe_print(f"[INFO] ë©”ëª¨ë¦¬ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(reasoned_graph)} triples")
                    pass
                else:
                    safe_print("[WARN] ì¶”ë¡  ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨. instances_reasoned.ttl ì €ì¥ ê±´ë„ˆëœ€")
            
            # 4. ê¸°ì¡´ ì¤‘ê°„ ìƒì„±ë¬¼ íŒŒì¼ ë° êµ¬í˜• í†µí•© íŒŒì¼ ì •ë¦¬
            if cleanup_old_files:
                old_files = [
                    Path(self.ontology_path) / "updated_graph.ttl",
                ]
                
                for old_file in old_files:
                    if old_file.exists():
                        try:
                            # ë°±ì—… (íŒŒì¼ ìœ„ì¹˜ì— ë§ëŠ” ë°±ì—… ë””ë ‰í† ë¦¬ ì‚¬ìš©)
                            if backup_old_files:
                                # íŒŒì¼ì´ ontology_pathì— ìˆìœ¼ë©´ ontology_path/backup, ì•„ë‹ˆë©´ output_path/backup
                                if str(old_file).startswith(str(Path(self.ontology_path))):
                                    backup_dir = Path(self.ontology_path) / "backup"
                                else:
                                    backup_dir = ontology_dir / "backup" # Use ontology_dir here
                                backup_dir.mkdir(parents=True, exist_ok=True)
                                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                                backup_path = backup_dir / f"{old_file.name}.backup_{timestamp}"
                                shutil.copy2(old_file, backup_path)
                                safe_print(f"[INFO] ë°±ì—… ì™„ë£Œ: {backup_path}")
                            
                            # ì‚­ì œ
                            old_file.unlink()
                            safe_print(f"[INFO] ê¸°ì¡´ íŒŒì¼ ì‚­ì œ: {old_file}")
                        except Exception as e:
                            safe_print(f"[WARN] íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {old_file}, {e}")
            
            stats["success"] = True
            stats["message"] = "Graph saved successfully"
            return stats
            
        except Exception as e:
            stats["message"] = f"Failed to save RDF graph: {str(e)}"
            safe_print(f"[WARN] {stats['message']}")
            import traceback
            traceback.print_exc()
            return stats
    
    def _is_inferred_triple(self, s, p, o) -> bool:
        """
        íŠ¹ì • íŠ¸ë¦¬í”Œì´ ì¶”ë¡ ëœ ê²ƒì¸ì§€ íŒë‹¨
        
        íŒë‹¨ ê¸°ì¤€:
        1. Axiom(ì£¼ì„) ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
        2. íŠ¹ì • ì¶”ë¡  ì „ìš© í”„ë ˆë””ì¼€ì´íŠ¸ì¸ì§€ í™•ì¸ (hasAdvantage ë“±)
        """
        if self.graph is None:
            return False
            
        # 1. Axiom ì •ë³´ í™•ì¸ (annotatedSource/Property/Target)
        # ì´ ì‘ì—…ì€ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜
        from rdflib import OWL, RDF, RDFS
        # [MOD] íš¨ìœ¨ì„±ì„ ìœ„í•´ ëª¨ë“  Axiomì„ ë¨¼ì € ì°¾ì§€ ì•Šê³ , 
        # (Axiom, annotatedSource, s)ê°€ ìˆëŠ” ë…¸ë“œë§Œ í•„í„°ë§
        for axiom in self.graph.subjects(OWL.annotatedSource, s):
            if (axiom, RDF.type, OWL.Axiom) in self.graph and \
               (axiom, OWL.annotatedProperty, p) in self.graph and \
               (axiom, OWL.annotatedTarget, o) in self.graph:
                # Axiom ì„¤ëª… í™•ì¸
                for _, _, comment in self.graph.triples((axiom, RDFS.comment, None)):
                    if str(comment) == "inferred_relation":
                        return True
        
        # 2. ì¶”ë¡  ì „ìš© í”„ë ˆë””ì¼€ì´íŠ¸ í™•ì¸ (ì „ìˆ  ê·œì¹™ ë“±ì—ì„œ ìƒì„±)
        # NSê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¬¸ìì—´ í‰íƒ„í™”
        ns_str = str(self.ns) if self.ns else "http://coa-agent-platform.org/ontology#"
        inferred_predicates = [
            ns_str + "hasAdvantage", 
            ns_str + "hasDisadvantage",
            ns_str + "tacticalEffect",
            ns_str + "inferred_relation"
        ]
        if str(p) in inferred_predicates:
            return True
            
        return False

    def generate_reasoned_graph(self, 
                               enable_semantic_inference: bool = True,
                               run_tactical_rules: bool = True,
                               run_owl_reasoner: bool = True) -> Optional[Graph]:
        """
        ì¶”ë¡  ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ëœ ê·¸ë˜í”„ ìƒì„±
        
        Args:
            enable_semantic_inference: ì˜ë¯¸ ê¸°ë°˜ ì¶”ë¡ (LLM/Search) í™œì„±í™” ì—¬ë¶€
            run_tactical_rules: SPARQL ê¸°ë°˜ ì „ìˆ  ê·œì¹™ ì‹¤í–‰ ì—¬ë¶€
            run_owl_reasoner: OWL-RL ì¶”ë¡ ê¸° ì‹¤í–‰ ì—¬ë¶€
        
        Returns:
            ì¶”ë¡  ê²°ê³¼ê°€ ì¶”ê°€ëœ Graph ê°ì²´ (instances_reasoned.ttlë¡œ ì €ì¥ë  ê·¸ë˜í”„)
        """
        if not RDFLIB_AVAILABLE or self.graph is None:
            safe_print("[WARN] RDFLib not available or graph is None. Cannot generate reasoned graph.")
            return None
        
        import time
        start_total = time.time()
        
        try:
            # ê¸°ì¡´ ê·¸ë˜í”„ ë³µì‚¬ (ì¶”ë¡  ê²°ê³¼ë¥¼ ì¶”ê°€í•˜ê¸° ìœ„í•´)
            reasoned_graph = Graph()
            
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”© ë³µì‚¬
            for prefix, namespace in self.graph.namespaces():
                reasoned_graph.bind(prefix, namespace)
            
            # ê¸°ì¡´ ê·¸ë˜í”„ì˜ ëª¨ë“  íŠ¸ë¦¬í”Œ ë³µì‚¬
            for s, p, o in self.graph:
                reasoned_graph.add((s, p, o))
            
            safe_print("[INFO] ì¶”ë¡  ê·¸ë˜í”„ ìƒì„± ì‹œì‘...")
            
            # SemanticInferenceë¥¼ ì‚¬ìš©í•œ ì˜ë¯¸ ê¸°ë°˜ ì¶”ë¡ 
            if enable_semantic_inference:
                try:
                    from core_pipeline.semantic_inference import SemanticInference
                    semantic_inference = SemanticInference(self.config)
                    
                    # ê·¸ë˜í”„ì˜ ì£¼ìš” ì—”í‹°í‹°ë“¤ì— ëŒ€í•´ ì¶”ë¡  ìˆ˜í–‰
                    # COA, Threat, Asset ë“±ì˜ ì£¼ìš” ì—”í‹°í‹° íƒ€ì… ì¶”ì¶œ
                    from rdflib import RDF, RDFS
                    
                    # COA í´ë˜ìŠ¤ ì°¾ê¸° (í†µì¼ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚¬ìš©)
                    coa_class = self.ns["COA"]
                    coa_library_class = self.ns["COA_Library"]
                    
                    # COA ì¸ìŠ¤í„´ìŠ¤ ì°¾ê¸° (COA íƒ€ì…ì´ê±°ë‚˜ COA_Library íƒ€ì…ì¸ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤)
                    coa_instances = []
                    # ì§ì ‘ COA íƒ€ì…ì¸ ê²ƒë“¤
                    coa_instances.extend(list(self.graph.triples((None, RDF.type, coa_class))))
                    # COA_Library íƒ€ì…ì¸ ê²ƒë“¤ë„ í¬í•¨ (ì‹¤ì œ ë°ì´í„°ëŠ” COA_Libraryë¡œ ì €ì¥ë¨)
                    coa_instances.extend(list(self.graph.triples((None, RDF.type, coa_library_class))))
                    
                    # COAì˜ í•˜ìœ„ í´ë˜ìŠ¤ë“¤ë„ ê²€ìƒ‰
                    for s, p, o in self.graph.triples((None, RDFS.subClassOf, coa_class)):
                        # í•˜ìœ„ í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë“¤ë„ ì°¾ê¸°
                        subclass_instances = list(self.graph.triples((None, RDF.type, s)))
                        coa_instances.extend(subclass_instances)
                    
                    # ì¤‘ë³µ ì œê±°
                    coa_instances = list(set(coa_instances))
                    safe_print(f"[INFO] COA ì¸ìŠ¤í„´ìŠ¤ {len(coa_instances)}ê°œ ë°œê²¬")
                    
                    if len(coa_instances) == 0:
                        safe_print("[WARN] COA ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # [PERFORMANCE] ì²˜ë¦¬í•  ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ì œí•œ ë° ë¡œê·¸ ê°•í™”
                    max_coa_to_process = self.config.get("max_coa_semantic_inference", 20)
                    process_count = min(len(coa_instances), max_coa_to_process)
                    safe_print(f"[INFO] Semantic Inference ì‹œì‘ (ëŒ€ìƒ: {process_count}/{len(coa_instances)}ê°œ)")
                    
                    inferred_count = 0
                    start_semantic = time.time()
                    for idx, (coa_subj, _, _) in enumerate(coa_instances[:process_count]):
                        if idx > 0 and idx % 5 == 0:
                            safe_print(f"  - ì§„í–‰ë¥ : {idx}/{process_count}...")
                        # [FIX] Subject URIì— ê³µë°±ì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
                        coa_str = str(coa_subj)
                        if " " in coa_str:
                            coa_subj = URIRef(coa_str.replace(" ", "_"))
                        
                        coa_uri = str(coa_subj)
                        coa_local = _localname(coa_subj)
                        
                        # ì˜ë¯¸ ê¸°ë°˜ ê´€ê³„ ì¶”ë¡ 
                        relations = semantic_inference.infer_relations(self.graph, coa_local, max_depth=2)
                        
                        # ì¶”ë¡ ëœ ê´€ê³„ë¥¼ ê·¸ë˜í”„ì— ì¶”ê°€
                        for rel in relations.get('direct', []) + relations.get('indirect', []):
                            related_entity = rel.get('entity', '')
                            predicate = rel.get('predicate', '')
                            
                            if related_entity and predicate:
                                try:
                                    # URI ë³€í™˜
                                    if not related_entity.startswith('http://'):
                                        safe_related = _make_uri_safe(related_entity)
                                        related_uri = URIRef(self.ns[safe_related])
                                    else:
                                        # ì´ë¯¸ URIì¸ ê²½ìš°ì—ë„ ê³µë°±ì´ ìˆìœ¼ë©´ ì²˜ë¦¬ (ëª¨ë“  ê³µë°± ë¬¸ì ëŒ€ìƒ)
                                        safe_related = re.sub(r'\s+', '_', related_entity)
                                        related_uri = URIRef(safe_related)
                                    
                                    if not predicate.startswith('http://'):
                                        safe_pred = _make_uri_safe(predicate)
                                        pred_uri = URIRef(self.ns[safe_pred])
                                    else:
                                        # ì´ë¯¸ URIì¸ ê²½ìš°ì—ë„ ê³µë°±ì´ ìˆìœ¼ë©´ ì²˜ë¦¬ (ëª¨ë“  ê³µë°± ë¬¸ì ëŒ€ìƒ)
                                        safe_pred = re.sub(r'\s+', '_', predicate)
                                        pred_uri = URIRef(safe_pred)
                                    
                                    
                                    # [NEW] ì¶”ë¡  í•„í„°ë§ (ê³¼ë„í•œ ì¶”ë¡  ë°©ì§€)
                                    # [MOD] rdf:type, sameAs, equivalentClass ë“±ì€ ì¶”ë¡  ê²°ê³¼ë¡œ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ì˜¤ì—¼ ë°©ì§€)
                                    from rdflib import RDF, OWL
                                    if pred_uri in [RDF.type, OWL.sameAs, OWL.equivalentClass, OWL.equivalentProperty]:
                                        continue

                                    if hasattr(semantic_inference, '_should_exclude_inference'):
                                        if semantic_inference._should_exclude_inference(str(coa_subj), str(pred_uri), str(related_uri)):
                                            continue
                                            
                                    # ì¶”ë¡ ëœ ê´€ê³„ ì¶”ê°€ (ì¤‘ë³µ ì²´í¬)

                                    if (coa_subj, pred_uri, related_uri) not in reasoned_graph:
                                        reasoned_graph.add((coa_subj, pred_uri, related_uri))
                                        inferred_count += 1
                                        
                                        # ì¶”ë¡ ëœ ê´€ê³„ì„ì„ í‘œì‹œí•˜ëŠ” ë©”íƒ€ë°ì´í„° ì¶”ê°€
                                        from rdflib import BNode
                                        inference_node = BNode()
                                        reasoned_graph.add((inference_node, RDF.type, OWL.Axiom))
                                        reasoned_graph.add((inference_node, OWL.annotatedSource, coa_subj))
                                        reasoned_graph.add((inference_node, OWL.annotatedProperty, pred_uri))
                                        reasoned_graph.add((inference_node, OWL.annotatedTarget, related_uri))
                                        reasoned_graph.add((inference_node, RDFS.comment, Literal("inferred_relation")))
                                        
                                except Exception as e:
                                    safe_print(f"[WARN] ì¶”ë¡  ê´€ê³„ ì¶”ê°€ ì‹¤íŒ¨: {e}")
                    
                    safe_print(f"[INFO] Semantic Inference ì™„ë£Œ: {inferred_count}ê°œ ê´€ê³„ ì¶”ê°€ (ì‹œê°„: {time.time() - start_semantic:.2f}ì´ˆ)")
                    
                except ImportError as e:
                    safe_print(f"[WARN] SemanticInferenceë¥¼ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
                except Exception as e:
                    safe_print(f"[WARN] ì˜ë¯¸ ê¸°ë°˜ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                    import traceback
                    traceback.print_exc()
            
            # ğŸ”¥ NEW: SPARQL ê¸°ë°˜ ì „ìˆ ì  ìœ ë¶ˆë¦¬ ì¶”ë¡  (hasAdvantage, hasDisadvantage)
            tactical_rules_path = Path(self.ontology_path) / "tactical_rules.sparql"
            if run_tactical_rules and tactical_rules_path.exists():
                start_tactical = time.time()
                try:
                    safe_print(f"[INFO] ì „ìˆ  ì¶”ë¡  ê·œì¹™ ì‹¤í–‰ ì¤‘: {tactical_rules_path}")
                    with open(tactical_rules_path, 'r', encoding='utf-8') as f:
                        rules_content = f.read()
                    
                    # PREFIX ì¶”ì¶œ
                    prefixes = []
                    for line in rules_content.split('\n'):
                        if line.strip().upper().startswith('PREFIX'):
                            prefixes.append(line.strip())
                    prefix_str = '\n'.join(prefixes) + '\n'
                    
                    # SPARQL CONSTRUCT ì¿¼ë¦¬ ë¶„ë¦¬
                    raw_queries = re.split(r'(?=CONSTRUCT)', rules_content, flags=re.IGNORECASE)
                    
                    tactical_inferred_count = 0
                    for q in raw_queries:
                        if 'CONSTRUCT' not in q.upper():
                            continue
                        
                        # PREFIXì™€ ê²°í•©
                        full_query = prefix_str + q
                        
                        # [DEBUG] ì¿¼ë¦¬ ì‹¤í–‰ ì‹œë„
                        try:
                            result_graph = reasoned_graph.query(full_query)
                            for s, p, o in result_graph:
                                if (s, p, o) not in reasoned_graph:
                                    reasoned_graph.add((s, p, o))
                                    tactical_inferred_count += 1
                        except Exception as qe:
                            safe_print(f"[WARN] ê°œë³„ ì „ìˆ  ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {qe}")
                    
                    safe_print(f"[INFO] ì „ìˆ  ì¶”ë¡  ì™„ë£Œ: {tactical_inferred_count}ê°œ ìœ ë¶ˆë¦¬ ê´€ê³„ ì¶”ê°€ (ì‹œê°„: {time.time() - start_tactical:.2f}ì´ˆ)")
                    
                except Exception as e:
                    safe_print(f"[WARN] ì „ìˆ  ì¶”ë¡  ê·œì¹™ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    import traceback
                    traceback.print_exc()
            
            # OWL-RL ì¶”ë¡  ì‹¤í–‰ (SemanticInference ì´í›„)
            if run_owl_reasoner:
                start_owl = time.time()
                try:
                    from core_pipeline.owl_reasoner import OWLReasoner, OWLRL_AVAILABLE
                    if OWLRL_AVAILABLE:
                        # [PERFORMANCE] ëŒ€ê·œëª¨ ê·¸ë˜í”„ ìë™ ì²´í¬ ë° ë³´í˜¸
                        graph_size = len(reasoned_graph)
                        include_rdfs = self.config.get("include_rdfs_inference", False) # ê¸°ë³¸ê°’ Falseë¡œ ë³€ê²½
                        
                        if graph_size > 20000 and include_rdfs:
                            safe_print(f"[WARN] ëŒ€ê·œëª¨ ê·¸ë˜í”„ ê°ì§€ ({graph_size} triples). ì•ˆì „ì„ ìœ„í•´ RDFS ì¶”ë¡ ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
                            include_rdfs = False
                            
                        safe_print(f"[INFO] OWL-RL ì¶”ë¡ ê¸° ê°€ë™ ì¤‘ (ëŒ€ìƒ: {graph_size} triples, RDFS: {include_rdfs})...")
                        namespace = str(self.ns) if self.ns else None
                        reasoner = OWLReasoner(reasoned_graph, namespace)
                        inferred_graph = reasoner.run_inference(include_rdfs=include_rdfs)
                        
                        if inferred_graph is not None:
                            stats = reasoner.get_stats()
                            if stats.get("success"):
                                owl_new_count = stats.get("new_inferences", 0)
                                if owl_new_count > 0:
                                    safe_print(f"[INFO] OWL-RL ì¶”ë¡  ì™„ë£Œ: {owl_new_count}ê°œ ìƒˆë¡œìš´ íŠ¸ë¦¬í”Œ ìƒì„± (ì‹œê°„: {time.time() - start_owl:.2f}ì´ˆ)")
                                    # OWL-RL ì¶”ë¡  ê²°ê³¼ë¥¼ reasoned_graphì— ë°˜ì˜
                                    reasoned_graph = inferred_graph
                                else:
                                    safe_print(f"[INFO] OWL-RL ì¶”ë¡  ì™„ë£Œ: ìƒˆë¡œìš´ íŠ¸ë¦¬í”Œ ì—†ìŒ (ì‹œê°„: {time.time() - start_owl:.2f}ì´ˆ)")
                            else:
                                safe_print(f"[WARN] OWL-RL ì¶”ë¡  ì‹¤íŒ¨: {stats.get('error', 'Unknown error')}")
                    else:
                        safe_print("[INFO] owlrl ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ OWL-RL ì¶”ë¡ ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                except Exception as e:
                    safe_print(f"[WARN] OWL-RL ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                    import traceback
                    traceback.print_exc()
            
            # ì¶”ë¡ ëœ ê·¸ë˜í”„ì˜ íŠ¸ë¦¬í”Œ ìˆ˜ í™•ì¸ (ì •í™•í•œ ì¸¡ì •)
            reasoned_triples_set = set(reasoned_graph)
            reasoned_triples = len(reasoned_triples_set)
            original_triples_set = set(self.graph)
            original_triples = len(original_triples_set)
            new_triples = reasoned_triples - original_triples
            
            safe_print(f"[INFO] ì „ì²´ ì¶”ë¡  í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ: ì›ë³¸ {original_triples}ê°œ -> ìµœì¢… {reasoned_triples}ê°œ (ì´ ì†Œìš”ì‹œê°„: {time.time() - start_total:.2f}ì´ˆ)")
            
            return reasoned_graph
            
        except Exception as e:
            safe_print(f"[WARN] ì¶”ë¡  ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def load_from_file(self, file_path: str):
        """
        íŒŒì¼ë¡œë¶€í„° ì˜¨í†¨ë¡œì§€ ë¡œë“œ
        
        Args:
            file_path: OWL, TTL ë“±ì˜ ì˜¨í†¨ë¡œì§€ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            RDF Graph ê°ì²´
        """
        path_obj = Path(file_path)
        if not path_obj.exists():
            safe_print(f"[WARN] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return None
            
        try:
            # í¬ë§· ìë™ ê°ì§€ (í™•ì¥ì ê¸°ë°˜)
            fmt = 'turtle' if path_obj.suffix == '.ttl' else 'xml'
            if path_obj.suffix == '.nt':
                fmt = 'nt'
                
            safe_print(f"[INFO] ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì¤‘... ({file_path})")
            
            # ê¸°ì¡´ ê·¸ë˜í”„ ë³‘í•© (ìƒˆë¡œìš´ Graph ê°ì²´ ìƒì„±ì´ ì•„ë‹Œ ë³‘í•©)
            # [FIX] rdflib parse ì‹œ ì¸ì½”ë”© ë¬¸ì œë¡œ ì¸í•œ ê°€ìƒ ë³€ìˆ˜ ì˜¤ë¥˜(AttributeError) ê°€ëŠ¥ì„± ëŒ€ë¹„ ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”
            try:
                self.graph.parse(str(path_obj), format=fmt)
            except AttributeError as ae:
                if 'newUniversal' in str(ae):
                    safe_print(f"[CRITICAL] ì˜¨í†¨ë¡œì§€ íŒŒì‹± ì¤‘ ë³€ìˆ˜ ìƒì„± ì˜¤ë¥˜ ë°œìƒ. íŒŒì¼ì— ê·œì¹™ì— ì–´ê¸‹ë‚˜ëŠ” '?' ê¸°í˜¸ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤. ({file_path})")
                raise ae
                
            safe_print(f"[INFO] ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì™„ë£Œ. í˜„ì¬ íŠ¸ë¦¬í”Œ ìˆ˜: {len(self.graph)}")
            return self.graph
        except Exception as e:
            safe_print(f"[ERROR] ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì‹¤íŒ¨ ({file_path}): {e}")
            import traceback
            safe_print(traceback.format_exc())
            return None
            
    def try_load_existing_graph(self):
        """
        ê¸°ì¡´ì— ì €ì¥ëœ ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ ìë™ ë¡œë“œ ì‹œë„
        ìš°ì„ ìˆœìœ„: instances_reasoned.ttl > instances.ttl > schema.ttl
        
        instances.ttlë§Œ ìˆëŠ” ê²½ìš° OWL-RL ì¶”ë¡ ì„ ìë™ ì‹¤í–‰í•˜ì—¬ ì¶”ë¡  íŠ¸ë¦¬í”Œ ìƒì„±
        """
        if not RDFLIB_AVAILABLE:
            return

        ontology_dir = Path(self.ontology_path)
        if not ontology_dir.exists():
            return

        # ë¡œë“œ ìš°ì„ ìˆœìœ„ íŒŒì¼ ëª©ë¡
        load_candidates = [
            "instances_reasoned.ttl", # ì¶”ë¡ ëœ ì™„ì„±ë³¸
            "instances.ttl",          # ì¶”ë¡  ì „ ì¸ìŠ¤í„´ìŠ¤
            "schema.ttl"              # ìŠ¤í‚¤ë§ˆë§Œ
        ]
        
        loaded = False
        loaded_filename = None
        for filename in load_candidates:
            file_path = ontology_dir / filename
            if file_path.exists():
                safe_print(f"[INFO] ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ íŒŒì¼ ë°œê²¬: {filename}. ìë™ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                
                # instances.ttl ë¡œë“œ ì‹œ ì›ë³¸ í¬ê¸°ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•´
                # ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì „ì— ì›ë³¸ í¬ê¸° ì¸¡ì •
                if filename == "instances.ttl":
                    # ìŠ¤í‚¤ë§ˆê°€ ë¶„ë¦¬ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ schema.ttlì€ í•­ìƒ ë¨¼ì € ë¡œë“œ ì‹œë„
                    schema_path = ontology_dir / "schema.ttl"
                    if schema_path.exists():
                        self.load_from_file(str(schema_path))
                    
                    # instances.ttl ë¡œë“œ ì „ ê·¸ë˜í”„ í¬ê¸° ì €ì¥ (ìŠ¤í‚¤ë§ˆë§Œ ìˆëŠ” ìƒíƒœ)
                    before_instances_size = len(set(self.graph)) if self.graph else 0
                    
                    # instances.ttl ë¡œë“œ
                    if self.load_from_file(str(file_path)):
                        loaded = True
                        loaded_filename = filename
                        # instances.ttl ë¡œë“œ ì§í›„ì˜ ì›ë³¸ í¬ê¸° ì €ì¥ (ìŠ¤í‚¤ë§ˆ + ì¸ìŠ¤í„´ìŠ¤)
                        # ì´ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¶”ë¡  ì—¬ë¶€ë¥¼ íŒë‹¨
                        self._original_graph_size = len(set(self.graph)) if self.graph else 0
                        safe_print(f"[INFO] '{filename}' ê¸°ë°˜ìœ¼ë¡œ ê·¸ë˜í”„ ì´ˆê¸°í™” ì™„ë£Œ. ì›ë³¸ í¬ê¸°: {self._original_graph_size} triples")
                        break
                else:
                    # instances_reasoned.ttl ë˜ëŠ” schema.ttl ë¡œë“œ ì‹œ
                    # ìŠ¤í‚¤ë§ˆê°€ ë¶„ë¦¬ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ schema.ttlì€ í•­ìƒ ë¨¼ì € ë¡œë“œ ì‹œë„ (instances_reasoned ë¡œë“œ ì‹œ)
                    if filename != "schema.ttl":
                        schema_path = ontology_dir / "schema.ttl"
                        if schema_path.exists():
                             self.load_from_file(str(schema_path))
                    
                    if self.load_from_file(str(file_path)):
                        loaded = True
                        loaded_filename = filename
                        safe_print(f"[INFO] '{filename}' ê¸°ë°˜ìœ¼ë¡œ ê·¸ë˜í”„ ì´ˆê¸°í™” ì™„ë£Œ.")
                        # instances_reasoned.ttlì´ ë¡œë“œëœ ê²½ìš°, ì´ë¯¸ ì¶”ë¡ ëœ ê·¸ë˜í”„ë¡œ ê°„ì£¼
                        if filename == "instances_reasoned.ttl":
                            self._inference_performed = True
                        break
        
        if not loaded:
            safe_print("[INFO] ë¡œë“œí•  ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìƒì„±ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        # instances.ttlë§Œ ë¡œë“œëœ ê²½ìš° (instances_reasoned.ttlì´ ì—†ëŠ” ê²½ìš°) OWL-RL ì¶”ë¡  ìë™ ì‹¤í–‰
        # ë‹¨, ì´ë¯¸ ì¶”ë¡ ì´ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ì¶”ë¡  ë°©ì§€)
        if loaded_filename == "instances.ttl":
            # ì´ë¯¸ ì¶”ë¡ ì´ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸ (í”Œë˜ê·¸ ë˜ëŠ” ê·¸ë˜í”„ í¬ê¸°ë¡œ íŒë‹¨)
            # instances.ttlë§Œ ë¡œë“œëœ ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ ì¶”ë¡ ì´ í•„ìš”í•˜ì§€ë§Œ,
            # ì´ë¯¸ ì¶”ë¡ ëœ ê·¸ë˜í”„ì¸ ê²½ìš° ìŠ¤í‚µ
            enable_auto_inference = self.config.get("enable_auto_owl_inference", True)
            
            # ì¶”ë¡  ì‹¤í–‰ ì—¬ë¶€ í™•ì¸: ì´ë¯¸ ì¶”ë¡ ëœ ê·¸ë˜í”„ì¸ì§€ ì²´í¬
            # ì›ë³¸ ê·¸ë˜í”„ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì¶”ì í•˜ì—¬ ë¹„ìœ¨ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨
            current_triples = len(set(self.graph)) if self.graph else 0
            
            # ì›ë³¸ ê·¸ë˜í”„ í¬ê¸°ê°€ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if self._original_graph_size is not None and self._original_graph_size > 0:
                # ì›ë³¸ ëŒ€ë¹„ ë¹„ìœ¨ë¡œ íŒë‹¨ (ì¶”ë¡ ëœ ê·¸ë˜í”„ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì›ë³¸ì˜ ì•½ 1.5ë°° ì´ìƒ)
                # ì•ˆì „ ë§ˆì§„ì„ ë‘ì–´ 1.3ë°° ì´ìƒì´ë©´ ì¶”ë¡ ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                ratio = current_triples / self._original_graph_size
                is_already_inferred = ratio >= 1.3  # ì›ë³¸ ëŒ€ë¹„ 1.3ë°° ì´ìƒ
                safe_print(f"[INFO] ì¶”ë¡  ì—¬ë¶€ íŒë‹¨ - ì›ë³¸: {self._original_graph_size} triples, í˜„ì¬: {current_triples} triples, ë¹„ìœ¨: {ratio:.2f}x")
            else:
                # ì›ë³¸ í¬ê¸°ê°€ ì €ì¥ë˜ì§€ ì•Šì€ ê²½ìš° (ì´ë¡ ì ìœ¼ë¡œ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨)
                # ê¸°ë³¸ê°’ìœ¼ë¡œ í˜„ì¬ í¬ê¸°ê°€ 0ë³´ë‹¤ í¬ë©´ ì¶”ë¡ ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì§€ ì•ŠìŒ
                is_already_inferred = False
                safe_print(f"[WARN] ì›ë³¸ ê·¸ë˜í”„ í¬ê¸°ê°€ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            
            if enable_auto_inference and not is_already_inferred:
                try:
                    from core_pipeline.owl_reasoner import OWLReasoner, OWLRL_AVAILABLE
                    if OWLRL_AVAILABLE:
                        safe_print("[INFO] instances_reasoned.ttlì´ ì—†ì–´ OWL-RL ì¶”ë¡ ì„ ìë™ ì‹¤í–‰í•©ë‹ˆë‹¤...")
                        namespace = str(self.ns) if self.ns else None
                        reasoner = OWLReasoner(self.graph, namespace)
                        inferred_graph = reasoner.run_inference()
                        
                        if inferred_graph is not None:
                            stats = reasoner.get_stats()
                            if stats.get("success"):
                                new_count = stats.get("new_inferences", 0)
                                if new_count > 0:
                                    safe_print(f"[INFO] OWL-RL ì¶”ë¡  ì™„ë£Œ: {new_count}ê°œ ìƒˆë¡œìš´ íŠ¸ë¦¬í”Œ ìƒì„±")
                                    # ì¶”ë¡ ëœ ê·¸ë˜í”„ë¥¼ ë©”ëª¨ë¦¬ì— ì ìš©
                                    self.graph = inferred_graph
                                    
                                    # ì¶”ë¡  ì‹¤í–‰ í”Œë˜ê·¸ ì„¤ì • (ì¤‘ë³µ ë°©ì§€)
                                    self._inference_performed = True
                                    # ì›ë³¸ í¬ê¸°ëŠ” ìœ ì§€ (ì¶”ë¡  ì—¬ë¶€ íŒë‹¨ì— ê³„ì† ì‚¬ìš©)
                                    
                                    # ì¶”ë¡ ëœ ê·¸ë˜í”„ ì €ì¥ (ì„ íƒì )
                                    save_reasoned = self.config.get("save_reasoned_graph_on_startup", False)
                                    if save_reasoned:
                                        try:
                                            reasoned_path = ontology_dir / "instances_reasoned.ttl"
                                            inferred_graph.serialize(destination=str(reasoned_path), format="turtle")
                                            safe_print(f"[INFO] ì¶”ë¡ ëœ ê·¸ë˜í”„ ì €ì¥: {reasoned_path}")
                                        except Exception as e:
                                            safe_print(f"[WARN] ì¶”ë¡ ëœ ê·¸ë˜í”„ ì €ì¥ ì‹¤íŒ¨: {e}")
                                else:
                                    safe_print("[INFO] OWL-RL ì¶”ë¡  ì™„ë£Œ: ìƒˆë¡œìš´ íŠ¸ë¦¬í”Œ ì—†ìŒ (ì´ë¯¸ ëª¨ë“  ê´€ê³„ê°€ ì¡´ì¬)")
                            else:
                                safe_print(f"[WARN] OWL-RL ì¶”ë¡  ì‹¤íŒ¨: {stats.get('error', 'Unknown error')}")
                    else:
                        safe_print("[INFO] owlrl ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ OWL-RL ì¶”ë¡ ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                except Exception as e:
                    safe_print(f"[WARN] OWL-RL ì¶”ë¡  ìë™ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            elif is_already_inferred:
                safe_print("[INFO] ì´ë¯¸ ì¶”ë¡ ëœ ê·¸ë˜í”„ì…ë‹ˆë‹¤. ì¶”ë¡ ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                self._inference_performed = True
    
    def query(self, query_string: str, bindings: Optional[Dict] = None, return_format: str = 'list') -> Union[List[Dict], pd.DataFrame]:
        """
        SPARQL ì¿¼ë¦¬ ë¬¸ìì—´ ì§ì ‘ ì‹¤í–‰
        
        Args:
            query_string: SPARQL ì¿¼ë¦¬ ë¬¸ìì—´
            bindings: ì¿¼ë¦¬ ë³€ìˆ˜ ë°”ì¸ë”©
            return_format: ë°˜í™˜ í˜•ì‹ ('list', 'dataframe')
            
        Returns:
            ì¿¼ë¦¬ ê²°ê³¼ (ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” DataFrame)
        """
        if not RDFLIB_AVAILABLE:
            safe_print("[WARN] rdflib not available. Cannot run SPARQL query.")
            return [] if return_format == 'list' else pd.DataFrame()
        
        if self.graph is None:
            safe_print("[WARN] Graph not initialized. Cannot run SPARQL query.")
            return [] if return_format == 'list' else pd.DataFrame()
        
        try:
            # ì¿¼ë¦¬ ì¤€ë¹„
            query = prepareQuery(query_string)
            
            # ë°”ì¸ë”©ì´ ìˆìœ¼ë©´ ì ìš©
            if bindings:
                query_result = self.graph.query(query, initBindings=bindings)
            else:
                query_result = self.graph.query(query)
            
            # ë°˜í™˜ í˜•ì‹ì— ë”°ë¼ ë³€í™˜
            if return_format == 'dataframe':
                return self._results_to_dataframe(query_result)
            else:
                return self._results_to_list(query_result)
                
        except Exception as e:
            safe_print(f"[ERROR] SPARQL query execution failed: {e}")
            import traceback
            traceback.print_exc()
            return [] if return_format == 'list' else pd.DataFrame()
    
    def run_sparql(self, query_path: str, bindings: Optional[Dict] = None, return_format: str = 'list') -> Union[List[Dict], pd.DataFrame]:
        """
        SPARQL ì¿¼ë¦¬ ì‹¤í–‰
        
        Args:
            query_path: SPARQL ì¿¼ë¦¬ íŒŒì¼ ê²½ë¡œ
            bindings: ì¿¼ë¦¬ ë³€ìˆ˜ ë°”ì¸ë”© (ì˜ˆ: {'?situation': URIRef(NS['SIT_001'])})
            return_format: ë°˜í™˜ í˜•ì‹ ('list', 'dataframe')
            
        Returns:
            ì¿¼ë¦¬ ê²°ê³¼ (ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” DataFrame)
        """
        if not RDFLIB_AVAILABLE:
            safe_print("[WARN] rdflib not available. Cannot run SPARQL query.")
            return [] if return_format == 'list' else pd.DataFrame()
        
        if self.graph is None:
            safe_print("[WARN] Graph not initialized. Cannot run SPARQL query.")
            return [] if return_format == 'list' else pd.DataFrame()
        
        if not os.path.exists(query_path):
            raise FileNotFoundError(f"SPARQL query file not found: {query_path}")
        
        with open(query_path, 'r', encoding='utf-8') as f:
            query_str = f.read()
        
        return self.query(query_str, bindings=bindings, return_format=return_format)
    
    def _results_to_dataframe(self, sparql_results) -> pd.DataFrame:
        """SPARQL ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        rows = []
        for row in sparql_results:
            row_dict = {}
            for key in row.labels:
                value = row[key]
                # URIRefë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                if hasattr(value, 'toPython'):
                    row_dict[key] = str(value.toPython())
                else:
                    row_dict[key] = str(value)
            rows.append(row_dict)
        
        if not rows:
            return pd.DataFrame()
        
        return pd.DataFrame(rows)
    
    def _results_to_list(self, sparql_results) -> List[Dict]:
        """SPARQL ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        rows = []
        for row in sparql_results:
            row_dict = {}
            for key in row.labels:
                value = row[key]
                if hasattr(value, 'toPython'):
                    row_dict[key] = str(value.toPython())
                else:
                    row_dict[key] = str(value)
            rows.append(row_dict)
        return rows
    
    def to_json(self) -> Dict:
        """
        ê·¸ë˜í”„ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê¸°ì¡´ graph_loader.pyì˜ ë°˜í™˜ í˜•ì‹)
        
        Returns:
            {"instances": {"nodes": [...], "links": [...]}, "schema": {"nodes": [...], "links": [...]}}
        """
        if not RDFLIB_AVAILABLE or self.graph is None:
            return {
                "instances": {"nodes": [], "links": []}, 
                "schema": {"nodes": [], "links": []},
                "stats": {}
            }
        
        # ë””ë²„ê¹…: ê·¸ë˜í”„ ìƒíƒœ í™•ì¸
        # [OPTIMIZATION] ë¶ˆí•„ìš”í•œ len(list(triples)) í˜¸ì¶œ ì œê±° (ë§¤ìš° ëŠë¦¼)
        # safe_print(f"[DEBUG] to_json ì‹œì‘")
        
        # [OPTIMIZATION] ìºì‹œ í™•ì¸
        # ê·¸ë˜í”„ ë³€ê²½ ê°ì§€ë¥¼ ìœ„í•´ idì™€ ì‚¬ì´ì¦ˆ ì²´í¬
        current_graph_hash = (id(self.graph), len(self.graph))
        if self._json_cache and self._last_graph_hash == current_graph_hash:
            # safe_print("[DEBUG] Using cached JSON data")
            return self._json_cache
            
        # ë³€ìˆ˜ ì´ˆê¸°í™” (Stats ìƒì„±ìš©)
        total_triples = len(self.graph)
        owl_class_count = 0
        owl_property_count = 0
        subClassOf_count = 0
        domain_count = 0
        range_count = 0
            
        instances = {"nodes": [], "links": []}
        schema = {"nodes": [], "links": []}
        
        # [OPTIMIZATION] í•œ ë²ˆì˜ ìˆœíšŒë¡œ í•„ìš”í•œ ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ (RDFLib triples íƒìƒ‰ ìµœì†Œí™”)
        node_groups = {}      # {uri: group_name} (ìµœì¢… ê²°ì •ëœ ê·¸ë£¹)
        node_labels = {}      # {uri: label}
        virtual_status = {}   # {uri: bool}
        
        # ì—”í‹°í‹° íƒ€ì… ìš°ì„ ìˆœìœ„ (ê°€ì¥ êµ¬ì²´ì ì´ê³  ì‚¬ìš©ìì—ê²Œ ì¹œìˆ™í•œ íƒ€ì… ìš°ì„ )
        type_priority = [
            "DefenseCOA", "OffensiveCOA", "CounterAttackCOA", "PreemptiveCOA",
            "DeterrenceCOA", "ManeuverCOA", "InformationOpsCOA",
            "COA", "COA_Library",
            "ì•„êµ°ë¶€ëŒ€í˜„í™©", "ì êµ°ë¶€ëŒ€í˜„í™©", "ì•„êµ°ê°€ìš©ìì‚°", "ìœ„í˜‘ìƒí™©", "ì„ë¬´ì •ë³´", 
            "ì „ì¥ì¶•ì„ ", "ì§€í˜•ì…€", "ê¸°ìƒìƒí™©", "ì œì•½ì¡°ê±´", "ë¯¼ê°„ì¸ì§€ì—­", "ì‹œë‚˜ë¦¬ì˜¤ëª¨ìŒ",
            "ìœ„í˜‘ìœ í˜•_ë§ˆìŠ¤í„°", "ì„ë¬´ë³„_ìì›í• ë‹¹"
        ]
        priority_set = set(type_priority)
        
        # 1. íƒ€ì… ì •ë³´ ìˆ˜ì§‘ ë° ê·¸ë£¹ ê²°ì •
        for s, _, o in self.graph.triples((None, RDF.type, None)):
            if isinstance(s, BNode): continue
            
            local_type = _localname(o)
            
            # ì´ë¯¸ ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ê·¸ë£¹ìœ¼ë¡œ ê²°ì •ëœ ê²½ìš° ìŠ¤í‚µ (ë‹¨, ë” ë†’ì€ ìš°ì„ ìˆœìœ„ê°€ ë‚˜ì˜¤ë©´ êµì²´)
            current_group = node_groups.get(s)
            
            if local_type in priority_set:
                # ìš°ì„ ìˆœìœ„ íƒ€ì… ë°œê²¬!
                # ê¸°ì¡´ ê·¸ë£¹ì´ ì—†ê±°ë‚˜, ê¸°ì¡´ ê·¸ë£¹ì´ ìš°ì„ ìˆœìœ„ ëª©ë¡ì— ì—†ê±°ë‚˜(ê¸°íƒ€ ë“±), 
                # í˜„ì¬ íƒ€ì…ì´ ë” ë†’ì€ ìš°ì„ ìˆœìœ„ë¼ë©´ êµì²´
                if not current_group or current_group not in priority_set:
                    node_groups[s] = local_type
                else:
                    # ë‘˜ ë‹¤ ìš°ì„ ìˆœìœ„ ëª©ë¡ì— ìˆë‹¤ë©´, ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ë¡œ ë¹„êµ (ë‚®ì€ ì¸ë±ìŠ¤ê°€ ë†’ì€ ìš°ì„ ìˆœìœ„)
                    try:
                        curr_idx = type_priority.index(current_group)
                        new_idx = type_priority.index(local_type)
                        if new_idx < curr_idx:
                            node_groups[s] = local_type
                    except ValueError:
                        pass # should not happen
            elif not current_group:
                # ì•„ì§ ê·¸ë£¹ì´ ì—†ìœ¼ë©´ ì¼ë°˜ íƒ€ì… í• ë‹¹ (ë‹¨, NamedIndividual ë“± ì œì™¸)
                if local_type not in ["NamedIndividual", "Thing", "Resource"]:
                    node_groups[s] = local_type
            
        # 2. ë¼ë²¨ ì •ë³´ ìˆ˜ì§‘
        for s, _, o in self.graph.triples((None, RDFS.label, None)):
            if isinstance(s, BNode): continue
            node_labels[s] = str(o)
            
        # 3. ê°€ìƒ ì—”í‹°í‹° ì •ë³´ ìˆ˜ì§‘
        is_virtual_uri = URIRef(self.ns["isVirtualEntity"])
        for s, _, o in self.graph.triples((None, is_virtual_uri, None)):
            virtual_status[s] = str(o).lower() in ['true', '1']

        inst_nodes = {}
        virtual_entity_count = 0
        actual_data_node_count = 0
        
        # ìˆ˜ì§‘ëœ ë…¸ë“œë“¤ ìƒì„±
        # (íƒ€ì…ì´ í•˜ë‚˜ë¼ë„ ìˆëŠ” ë…¸ë“œë“¤ ëŒ€ìƒ)
        # ì£¼ì˜: íƒ€ì…ì´ ì—†ëŠ” ë…¸ë“œ(ë¼ë²¨ë§Œ ìˆëŠ” ê²½ìš° ë“±)ëŠ” ì—¬ê¸°ì„œ ëˆ„ë½ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, 
        # node_groups, node_labels, virtual_statusì˜ í•©ì§‘í•©ì„ ìˆœíšŒí•´ì•¼ í•¨
        all_subjects = set(node_groups.keys()) | set(node_labels.keys()) | set(virtual_status.keys())
        
        for s in all_subjects:
            local_name = _localname(s)
            
            # ê·¸ë£¹ ê²°ì • (ì—†ìœ¼ë©´ 'ê¸°íƒ€')
            type_name = node_groups.get(s, "ê¸°íƒ€")
            
            # ì¸ë±ì‹±ëœ ì •ë³´ ì‚¬ìš© (ì„±ëŠ¥ í–¥ìƒ)
            is_virtual = virtual_status.get(s, False)
            if is_virtual:
                virtual_entity_count += 1
            else:
                actual_data_node_count += 1
            
            rdfs_label = node_labels.get(s)
            
            # ë…¸ë“œ í‘œì‹œ: IDë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•˜ê³ , rdfs:labelì´ ìˆìœ¼ë©´ ID (Label) í˜•ì‹ ì‚¬ìš©
            if rdfs_label and rdfs_label != local_name:
                display_label = f"{local_name} ({rdfs_label})"
            else:
                display_label = local_name
            
            inst_nodes[local_name] = {
                "id": local_name,
                "label": display_label,
                "group": type_name,
                "is_virtual": is_virtual
            }
        
        instances["nodes"] = list(inst_nodes.values())
        
        # ì¸ìŠ¤í„´ìŠ¤ ë§í¬ ì¶”ì¶œ
        inst_links = []
        excluded_predicates = {str(RDF.type), str(RDFS.label)}
        
        # [NEW] íƒ€ê²Ÿ ë…¸ë“œê°€ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” ê²½ìš° ìë™ ì¶”ê°€ (ê³ ë¦½ ë°©ì§€)
        missing_targets = set()
        
        for u, p, a in self.graph.triples((None, None, None)):
            if str(p) in excluded_predicates:
                continue
            if not isinstance(a, URIRef) and not isinstance(a, BNode): # ë¦¬í„°ëŸ´ ì œì™¸
                continue
            # BNode ì²´í¬ëŠ” ì´ë¯¸ í–ˆìœ¼ë¯€ë¡œ ìƒëµ ê°€ëŠ¥í•˜ë‚˜ ì•ˆì „ì¥ì¹˜
            if isinstance(u, BNode) or isinstance(a, BNode):
                continue
                
            u_local = _localname(u)
            a_local = _localname(a)
            
            # ì†ŒìŠ¤ ë…¸ë“œê°€ ìˆìœ¼ë©´ ë§í¬ ìƒì„± ì‹œë„, ì—†ìœ¼ë©´ ìë™ ìƒì„±
            if u_local not in inst_nodes:
                # ì†ŒìŠ¤ ë…¸ë“œ ìë™ ìƒì„± (íƒ€ì…/ë¼ë²¨ ì •ë³´ê°€ ì—†ì—ˆë˜ ê²½ìš°)
                inst_nodes[u_local] = {
                    "id": u_local,
                    "label": u_local, # ê¸°ë³¸ ë¼ë²¨
                    "group": "ê¸°íƒ€", # ê¸°ë³¸ ê·¸ë£¹
                    "is_virtual": False
                }
                # ë‚˜ì¤‘ì— ë¼ë²¨/íƒ€ì… ë³´ê°•ì„ ìœ„í•´ missing_targetsì²˜ëŸ¼ ê´€ë¦¬í•  ìˆ˜ë„ ìˆìœ¼ë‚˜, 
                # ì¼ë‹¨ ë§í¬ ì—°ê²°ì„±ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ì¦‰ì‹œ ìƒì„±
            
            # íƒ€ê²Ÿ ë…¸ë“œê°€ ì—†ìœ¼ë©´ ì¶”ê°€ ë¦¬ìŠ¤íŠ¸ì— ë„£ìŒ
            if a_local not in inst_nodes:
                missing_targets.add(a)
            
            inst_links.append({
                "source": u_local,
                "target": a_local,
                "relation": _localname(p)
            })
        
        # [NEW] ëˆ„ë½ëœ íƒ€ê²Ÿ ë…¸ë“œ ì¶”ê°€
        for missing_uri in missing_targets:
             local = _localname(missing_uri)
             if local not in inst_nodes:
                # ë¼ë²¨ ê°€ì ¸ì˜¤ê¸° ì‹œë„
                label = local
                for _, _, lbl in self.graph.triples((missing_uri, RDFS.label, None)):
                    label = f"{local} ({str(lbl)})"
                    break
                
                # íƒ€ì… ì¶”ë¡  (ê°„ë‹¨íˆ ì²«ë²ˆì§¸ íƒ€ì… ì‚¬ìš©)
                type_name = "ê¸°íƒ€"
                for _, _, t in self.graph.triples((missing_uri, RDF.type, None)):
                    t_local = _localname(t)
                    if t_local not in ["NamedIndividual", "Thing"]:
                        type_name = t_local
                        break
                
                inst_nodes[local] = {
                    "id": local,
                    "label": label,
                    "group": type_name,
                    "is_virtual": False
                }
        
        # ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ ì¬ê°±ì‹ 
        instances["nodes"] = list(inst_nodes.values())
        inst_links = [l for l in inst_links if l['target'] in inst_nodes] # ìµœì¢… ìœ íš¨ì„± ê²€ì‚¬
        instances["links"] = inst_links
        
        instances["links"] = inst_links
        
        # ìŠ¤í‚¤ë§ˆ ë…¸ë“œ ì¶”ì¶œ
        sch_nodes = {}
        schema_class_count = 0
        schema_property_count = 0
        
        # Table/Column ë…¸ë“œ (ë ˆê±°ì‹œ)
        for s, _, _ in self.graph.triples((None, RDF.type, self.ns.Table)):
            sch_nodes[_localname(s)] = {"id": _localname(s), "label": _get_label(self.graph, self.ns, s), "group": "Table"}
        for s, _, _ in self.graph.triples((None, RDF.type, self.ns.Column)):
            sch_nodes[_localname(s)] = {"id": _localname(s), "label": _get_label(self.graph, self.ns, s), "group": "Column"}
        
        # OWL.Class ë…¸ë“œ ì¶”ì¶œ (nsì™€ ns_legacy ëª¨ë‘ í™•ì¸)
        for s, _, _ in self.graph.triples((None, RDF.type, OWL.Class)):
            s_str = str(s)
            if s_str.startswith(str(self.ns)) or s_str.startswith(str(self.ns_legacy)):
                node_id = _localname(s)
                if node_id not in sch_nodes:
                    sch_nodes[node_id] = {"id": node_id, "label": _get_label(self.graph, self.ns, s), "group": "Class"}
                    schema_class_count += 1
        
        # ObjectProperty ë…¸ë“œ ì¶”ê°€
        for s, _, _ in self.graph.triples((None, RDF.type, OWL.ObjectProperty)):
            s_str = str(s)
            if s_str.startswith(str(self.ns)) or s_str.startswith(str(self.ns_legacy)):
                node_id = _localname(s)
                if node_id not in sch_nodes:
                    sch_nodes[node_id] = {"id": node_id, "label": _get_label(self.graph, self.ns, s), "group": "Property"}
                    schema_property_count += 1
        
        # DatatypeProperty ë…¸ë“œ ì¶”ê°€
        for s, _, _ in self.graph.triples((None, RDF.type, OWL.DatatypeProperty)):
            s_str = str(s)
            if s_str.startswith(str(self.ns)) or s_str.startswith(str(self.ns_legacy)):
                node_id = _localname(s)
                if node_id not in sch_nodes:
                    sch_nodes[node_id] = {"id": node_id, "label": _get_label(self.graph, self.ns, s), "group": "Property"}
                    schema_property_count += 1
        
        # ğŸ”¥ ë¡œê·¸ ìµœì í™”: ë¶ˆí•„ìš”í•œ DEBUG ë¡œê·¸ ì œê±°
        # safe_print(f"[DEBUG] to_json: ìŠ¤í‚¤ë§ˆ ë…¸ë“œ ì¶”ì¶œ - Class {schema_class_count}ê°œ, Property {schema_property_count}ê°œ")
        
        # ìŠ¤í‚¤ë§ˆ ë§í¬ ì¶”ì¶œ
        sch_links = []
        
        # hasColumn ê´€ê³„ (ê¸°ì¡´)
        for t, _, c in self.graph.triples((None, self.ns.hasColumn, None)):
            t_local = _localname(t)
            c_local = _localname(c)
            if t_local not in sch_nodes:
                sch_nodes[t_local] = {"id": t_local, "label": _get_label(self.graph, self.ns, t), "group": "Table"}
            if c_local not in sch_nodes:
                sch_nodes[c_local] = {"id": c_local, "label": _get_label(self.graph, self.ns, c), "group": "Column"}
            sch_links.append({"source": t_local, "target": c_local, "relation": "ì»¬ëŸ¼"})
        
        # subClassOf ê´€ê³„
        subClassOf_count = 0
        for s, _, o in self.graph.triples((None, RDFS.subClassOf, None)):
            s_local = _localname(s)
            o_local = _localname(o)
            s_str = str(s)
            o_str = str(o)
            if not (s_str.startswith(str(self.ns)) or s_str.startswith(str(self.ns_legacy))):
                continue
            if not (o_str.startswith(str(self.ns)) or o_str.startswith(str(self.ns_legacy))):
                continue
            
            if s_local not in sch_nodes:
                sch_nodes[s_local] = {"id": s_local, "label": _get_label(self.graph, self.ns, s), "group": "Class"}
            if o_local not in sch_nodes:
                sch_nodes[o_local] = {"id": o_local, "label": _get_label(self.graph, self.ns, o), "group": "Class"}
            sch_links.append({"source": s_local, "target": o_local, "relation": "subClassOf"})
            subClassOf_count += 1
        
        # domain ê´€ê³„ (Property -> Class)
        domain_count = 0
        for prop, _, cls in self.graph.triples((None, RDFS.domain, None)):
            prop_local = _localname(prop)
            cls_local = _localname(cls)
            prop_str = str(prop)
            cls_str = str(cls)
            if not (prop_str.startswith(str(self.ns)) or prop_str.startswith(str(self.ns_legacy))):
                continue
            if not (cls_str.startswith(str(self.ns)) or cls_str.startswith(str(self.ns_legacy))):
                continue
            
            if prop_local not in sch_nodes:
                sch_nodes[prop_local] = {"id": prop_local, "label": _get_label(self.graph, self.ns, prop), "group": "Property"}
            if cls_local not in sch_nodes:
                sch_nodes[cls_local] = {"id": cls_local, "label": _get_label(self.graph, self.ns, cls), "group": "Class"}
            sch_links.append({"source": prop_local, "target": cls_local, "relation": "domain"})
            domain_count += 1
        
        # range ê´€ê³„ (Property -> Class)
        range_count = 0
        for prop, _, cls in self.graph.triples((None, RDFS.range, None)):
            prop_local = _localname(prop)
            cls_local = _localname(cls)
            prop_str = str(prop)
            cls_str = str(cls)
            if not (prop_str.startswith(str(self.ns)) or prop_str.startswith(str(self.ns_legacy))):
                continue
            if not (cls_str.startswith(str(self.ns)) or cls_str.startswith(str(self.ns_legacy))):
                continue
            
            if prop_local not in sch_nodes:
                sch_nodes[prop_local] = {"id": prop_local, "label": _get_label(self.graph, self.ns, prop), "group": "Property"}
            if cls_local not in sch_nodes:
                sch_nodes[cls_local] = {"id": cls_local, "label": _get_label(self.graph, self.ns, cls), "group": "Class"}
            sch_links.append({"source": prop_local, "target": cls_local, "relation": "range"})
            range_count += 1
        
        schema["nodes"] = list(sch_nodes.values())
        schema["links"] = sch_links
        
        # ğŸ”¥ ë¡œê·¸ ìµœì í™”: ë¶ˆí•„ìš”í•œ DEBUG ë¡œê·¸ ì œê±°
        # safe_print(f"[DEBUG] to_json: ìŠ¤í‚¤ë§ˆ ë§í¬ ì¶”ì¶œ ì™„ë£Œ - subClassOf={subClassOf_count}, domain={domain_count}, range={range_count}, ì´ {len(sch_links)}ê°œ")
        
        # ìƒì„¸ í†µê³„ ê³„ì‚°
        # rdf:type triples (ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì„ ì–¸)
        instance_type_triples = len(list(self.graph.triples((None, RDF.type, None))))
        # rdfs:label triples
        label_triples = len(list(self.graph.triples((None, RDFS.label, None))))
        # Literal ê°’ì´ ìˆëŠ” triples (ì—£ì§€ë¡œ ë³€í™˜ë˜ì§€ ì•ŠìŒ)
        literal_triples = 0
        for s, p, o in self.graph.triples((None, None, None)):
            if isinstance(o, Literal):
                literal_triples += 1
        
        # ê·¸ë£¹ë³„ ë…¸ë“œ ìˆ˜ ê³„ì‚°
        group_counts = {}
        for node in instances["nodes"]:
            group = node.get("group", "ê¸°íƒ€")
            group_counts[group] = group_counts.get(group, 0) + 1
        
        # ë…¸ë“œë³„ ì—°ê²°ë„ ê³„ì‚°
        node_degrees = {}
        for link in instances["links"]:
            source = link.get("source")
            target = link.get("target")
            node_degrees[source] = node_degrees.get(source, 0) + 1
            node_degrees[target] = node_degrees.get(target, 0) + 1
        
        # í‰ê·  ì—°ê²°ë„ ê³„ì‚°
        avg_degree = sum(node_degrees.values()) / len(node_degrees) if node_degrees else 0
        
        # í†µê³„ ì •ë³´ êµ¬ì„±
        stats = {
            "total_triples": total_triples,
            "triples_by_category": {
                "instance_type": instance_type_triples,  # rdf:type (ì¸ìŠ¤í„´ìŠ¤)
                "labels": label_triples,  # rdfs:label
                "relationships": len(inst_links),  # ê´€ê³„ (ì—£ì§€ë¡œ ë³€í™˜ë¨)
                "literals": literal_triples,  # Literal ê°’ (ì—£ì§€ë¡œ ë³€í™˜ ì•ˆ ë¨)
                "schema": owl_class_count + owl_property_count + subClassOf_count + domain_count + range_count,  # ìŠ¤í‚¤ë§ˆ ì •ë³´
            },
            "visualization": {
                "nodes": len(instances["nodes"]),
                "edges": len(inst_links),
                "groups": len(group_counts),
                "node_to_triple_ratio": len(instances["nodes"]) / total_triples * 100 if total_triples > 0 else 0,
                "edge_to_triple_ratio": len(inst_links) / total_triples * 100 if total_triples > 0 else 0,
            },
            "node_breakdown": {
                "total_nodes": len(instances["nodes"]),
                "actual_data_nodes": actual_data_node_count,  # ì‹¤ì œ ë°ì´í„° í–‰ì—ì„œ ìƒì„±ëœ ë…¸ë“œ
                "virtual_entities": virtual_entity_count,  # ê°€ìƒ ì—”í‹°í‹° (ì¶”ë¡  ê´€ê³„ìš©)
                "virtual_to_actual_ratio": virtual_entity_count / actual_data_node_count * 100 if actual_data_node_count > 0 else 0,
            },
            "group_details": {
                group: {
                    "count": count,
                    "avg_degree": sum(node_degrees.get(node["id"], 0) for node in instances["nodes"] if node.get("group") == group) / count if count > 0 else 0
                }
                for group, count in group_counts.items()
            },
            "excluded": {
                "rdf_type_triples": instance_type_triples - len(instances["nodes"]),  # ë…¸ë“œ ìƒì„±ì— ì‚¬ìš©ë˜ì—ˆì§€ë§Œ ì—£ì§€ë¡œëŠ” í‘œì‹œ ì•ˆ ë¨
                "rdfs_label_triples": label_triples,  # ë¼ë²¨ì— ì‚¬ìš©ë˜ì—ˆì§€ë§Œ ì—£ì§€ë¡œëŠ” í‘œì‹œ ì•ˆ ë¨
                "literal_triples": literal_triples,  # Literal ê°’ì€ ì—£ì§€ë¡œ í‘œì‹œ ì•ˆ ë¨
            }
        }
        
        result = {
            "instances": instances,
            "schema": schema,
            "stats": stats
        }
        
        # [OPTIMIZATION] ìºì‹œ ì €ì¥
        self._json_cache = result
        self._last_graph_hash = current_graph_hash
        
        return result

    def get_node_details(self, node_id: str) -> Dict[str, Any]:
        """
        íŠ¹ì • ë…¸ë“œì˜ ìƒì„¸ ì •ë³´(ëª¨ë“  ì†ì„±)ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        
        Args:
            node_id: ë…¸ë“œ ID (Local Name ë˜ëŠ” URI)
            
        Returns:
            ì†ì„± ë”•ì…”ë„ˆë¦¬
        """
        if not RDFLIB_AVAILABLE or self.graph is None:
            return {}
            
        # URI í•´ê²°
        node_uri = None
        if node_id.startswith("http"):
            node_uri = URIRef(node_id)
        else:
            # 1. Try direct namespace lookup with strict error handling
            try:
                node_uri = self.ns[node_id]
            except Exception:
                # 2. Try legacy namespace
                try:
                    node_uri = self.ns_legacy[node_id]
                except Exception:
                    # 3. Fallback: Search by Label (rdfs:label or exact match in subjects)
                    found = False
                    # Search subjects ending with node_id
                    for s in self.graph.subjects():
                        if str(s).endswith(f"/{node_id}") or str(s).endswith(f"#{node_id}"):
                            node_uri = s
                            found = True
                            break
                    
                    if not found:
                        # Search by label (Language-insensitive)
                        logger.info(f"Searching for node by label: {node_id}")
                        for s, p, o in self.graph.triples((None, None, None)):
                             # Check for label-like predicates
                             if _localname(p) in ['label', 'name', 'prefLabel', 'altLabel']:
                                 # Handle Literal values (ignore language tags)
                                 val = o.value if hasattr(o, 'value') else str(o)
                                 if val == node_id: 
                                     node_uri = s
                                     found = True
                                     logger.info(f"Found node by label: {node_uri}")
                                     break
                    
                    if not found:
                        logger.warning(f"Node ID not found in graph: {node_id}")
                        # Final check: Iterate all subjects and check endswith (slower but safer)
                        for s in self.graph.subjects():
                            if str(s).endswith(node_id):
                                node_uri = s
                                found = True
                            logger.info(f"Found node by suffix match: {node_uri}")
                            break
                    
                    # [NEW] Ultimate Fallback: Check Schema Registry (for Table Names/Class Labels)
                    if not found and hasattr(self, 'schema_registry'):
                        # 1. Exact Match
                        if node_id in self.schema_registry:
                            target_key = node_id
                        else:
                            # 2. Partial Match (reversed): e.g. "PSYOPSíŒ€" -> User wants "PSYOPS" or similar? 
                            # Actually, usually it's the other way around: "ê³µë³‘ëŒ€ëŒ€" might be mapped to "ì•„êµ°ë¶€ëŒ€í˜„í™©" or "Engineer"
                            # Let's try to find if any key contains this node_id or vice versa
                            target_key = next((k for k in self.schema_registry if k in node_id or node_id in k), None)

                        if target_key:
                            logger.info(f"Found node in Schema Registry (fallback): {target_key}")
                            table_info = self.schema_registry[target_key]
                            return {
                                "_id": node_id,
                                "_uri": f"schema:{target_key}",
                                "type": "Class/Table",
                                "description": table_info.get("description", f"Schema Table: {target_key}"),
                                "columns": list(table_info.get("columns", {}).keys()),
                                "source": "SchemaRegistry",
                                "matched_key": target_key
                            }

                    if not found:
                             logger.error(f"Node lookup failed completely for: {node_id}")
                             return {}

        # ì†ì„± ì¡°íšŒ
        properties = {}
        if (node_uri, None, None) not in self.graph:
             logger.warning(f"Node URI found but no triples: {node_uri}")
             return {}

        for _, p, o in self.graph.triples((node_uri, None, None)):
            p_name = _localname(p)
            
            # ê°’ ì²˜ë¦¬
            if isinstance(o, Literal):
                value = str(o)
            elif isinstance(o, URIRef):
                value = _localname(o)
            else:
                value = str(o)
            
            if p_name in properties:
                if not isinstance(properties[p_name], list):
                    properties[p_name] = [properties[p_name]]
                properties[p_name].append(value)
            else:
                properties[p_name] = value
                
        # ê¸°ë³¸ ì •ë³´ ì¶”ê°€
        if not properties and (node_uri, None, None) not in self.graph:
             return {"id": node_id, "error": "Node not found"}

        properties["_id"] = node_id
        properties["_uri"] = str(node_uri)
        return properties
    
    def load_graph(self, inst_path: Optional[str] = None, onto_path: Optional[str] = None, 
                   load_all_files: bool = False, enable_semantic_inference: bool = False) -> Optional[Graph]:
        """
        ê·¸ë˜í”„ ë¡œë“œ (ìš°ì„ ìˆœìœ„: instances_reasoned.ttl > instances.ttl > schema.ttl)
        
        Args:
            inst_path: ì¸ìŠ¤í„´ìŠ¤ TTL íŒŒì¼ ê²½ë¡œ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ, ìë™ ê°ì§€)
            onto_path: ì˜¨í†¨ë¡œì§€ TTL íŒŒì¼ ê²½ë¡œ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ, ìë™ ê°ì§€)
            load_all_files: Trueì´ë©´ ëª¨ë“  ê´€ë ¨ TTL íŒŒì¼ì„ ë¡œë“œ (ê¸°ë³¸ê°’: False, ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë¡œë“œ)
            enable_semantic_inference: ì˜ë¯¸ ê¸°ë°˜ ì¶”ë¡  í™œì„±í™” (ë¯¸êµ¬í˜„)
            
        Returns:
            RDF Graph ê°ì²´
        """
        if not RDFLIB_AVAILABLE:
            safe_print("[WARN] rdflib not available. Cannot load graph.")
            return None
        
        # ê·¸ë˜í”„ë¥¼ ì™„ì „íˆ ìƒˆë¡œ ë¡œë“œ (ê¸°ì¡´ ê·¸ë˜í”„ ì´ˆê¸°í™”)
        self.graph = Graph()
        
        loaded_any = False
        
        # ìš°ì„ ìˆœìœ„ 1: instances_reasoned.ttl (ì¶”ë¡  ê²°ê³¼, ìµœìš°ì„ )
        reasoned_path = Path(self.ontology_path) / "instances_reasoned.ttl"
        if reasoned_path.exists():
            try:
                self.graph.parse(str(reasoned_path), format="turtle")
                loaded_any = True
                safe_print(f"[INFO] ì¶”ë¡ ëœ ì¸ìŠ¤í„´ìŠ¤ ë¡œë“œ: {reasoned_path}")
            except Exception as e:
                safe_print(f"[WARN] ì¶”ë¡ ëœ ì¸ìŠ¤í„´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {reasoned_path}, {e}")
        
        # ìš°ì„ ìˆœìœ„ 2: instances.ttl (ì¸ìŠ¤í„´ìŠ¤ ì „ìš©)
        instances_path = Path(self.ontology_path) / "instances.ttl"
        if instances_path.exists():
            try:
                self.graph.parse(str(instances_path), format="turtle")
                loaded_any = True
                safe_print(f"[INFO] ì¸ìŠ¤í„´ìŠ¤ ë¡œë“œ: {instances_path}")
            except Exception as e:
                safe_print(f"[WARN] ì¸ìŠ¤í„´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {instances_path}, {e}")
        
        # ìš°ì„ ìˆœìœ„ 3: schema.ttl (ìŠ¤í‚¤ë§ˆ)
        schema_path = Path(self.ontology_path) / "schema.ttl"
        if schema_path.exists():
            try:
                self.graph.parse(str(schema_path), format="turtle")
                loaded_any = True
                safe_print(f"[INFO] ìŠ¤í‚¤ë§ˆ ë¡œë“œ: {schema_path}")
            except Exception as e:
                safe_print(f"[WARN] ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì‹¤íŒ¨: {schema_path}, {e}")
        
        # ë ˆê±°ì‹œ íŒŒì¼ ì§€ì› (í•˜ìœ„ í˜¸í™˜ì„±) - ì œê±°ë¨ (Outputs Cleanup)
        if not loaded_any:
             safe_print("[INFO] ë ˆê±°ì‹œ íŒŒì¼ ë¡œë“œ ë¡œì§ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤ (k_c4i_*).")
        
        if enable_semantic_inference and loaded_any:
            safe_print("[INFO] Semantic inference is not yet implemented. Skipping.")
        
        if not loaded_any:
            safe_print("[WARN] ë¡œë“œí•  ê·¸ë˜í”„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        return self.graph
    
    def build_from_data(self, data: Dict[str, pd.DataFrame], force_rebuild: bool = False,
                         auto_sync_schema: bool = True) -> Optional[Graph]:
        """
        ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œë¶€í„° ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ êµ¬ì¶• (í˜¸í™˜ì„± ë˜í¼)
        
        EnhancedOntologyManagerëŠ” generate_owl_ontology() + generate_instances()ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
        ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ì´ ë©”ì„œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        
        Args:
            data: {í…Œì´ë¸”ëª…: DataFrame} ë”•ì…”ë„ˆë¦¬
            force_rebuild: ìºì‹œë¥¼ ë¬´ì‹œí•˜ê³  ê°•ì œë¡œ ì¬êµ¬ì¶• (ê¸°ë³¸: False)
            auto_sync_schema: ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ìë™ ë™ê¸°í™” (ê¸°ë³¸: True)
            
        Returns:
            RDF Graph ê°ì²´
        """
        global _cached_graph, _cached_data_hash
        
        # ê°•ì œ ì¬êµ¬ì¶•ì´ë©´ ìºì‹œ í´ë¦¬ì–´
        if force_rebuild:
            _cached_graph = None
            _cached_data_hash = None
            safe_print("[INFO] ìºì‹œ í´ë¦¬ì–´: ê°•ì œ ì¬êµ¬ì¶• ëª¨ë“œ")
        
        if not RDFLIB_AVAILABLE:
            safe_print("[WARN] rdflib not available. Cannot build ontology graph.")
            return None
        
        # ========== [NEW] ì„ í–‰ ë‹¨ê³„: ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë° ìë™ ì—…ë°ì´íŠ¸ ==========
        if auto_sync_schema:
            try:
                safe_print("[INFO] ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë™ê¸°í™” ì‹œì‘...")
                schema_sync_result = self._sync_schema_registry(data, auto_update=True)
                
                if schema_sync_result['has_changes']:
                    safe_print(f"[INFO] âœ“ ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {schema_sync_result['summary']}")
                    # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì¬ë¡œë“œ
                    self.schema_registry = self._load_schema_registry()
                else:
                    safe_print(f"[INFO] âœ“ ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê²€ì¦ ì™„ë£Œ: {schema_sync_result['summary']}")
            except Exception as e:
                safe_print(f"[WARN] ìŠ¤í‚¤ë§ˆ ìë™ ë™ê¸°í™” ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
        # ==============================================================
        
        # ë°ì´í„° í•´ì‹œ ê³„ì‚° (ìºì‹±ì„ ìœ„í•´)
        data_hash = None
        try:
            data_hash = self._calculate_data_hash(data)
            
            # ìºì‹œëœ ê·¸ë˜í”„ê°€ ìˆê³  ë°ì´í„°ê°€ ë™ì¼í•˜ë©´ ì¬ì‚¬ìš© (force_rebuildê°€ Falseì¸ ê²½ìš°ë§Œ)
            if not force_rebuild and _cached_graph is not None and _cached_data_hash == data_hash:
                safe_print("[INFO] ìºì‹œëœ ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ ì¬ì‚¬ìš©")
                self.graph = _cached_graph
                return self.graph
        except Exception as e:
            safe_print(f"[WARN] ë°ì´í„° í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨ (ìºì‹± ê±´ë„ˆëœ€): {e}")
            data_hash = None
        
        # Enhanced ë°©ì‹ìœ¼ë¡œ ê·¸ë˜í”„ ìƒì„±
        # 1. OWL ì˜¨í†¨ë¡œì§€ ìƒì„± (ìŠ¤í‚¤ë§ˆ)
        graph = self.generate_owl_ontology(data)
        if not graph:
            safe_print("[WARN] OWL ì˜¨í†¨ë¡œì§€ ìƒì„± ì‹¤íŒ¨")
            return None
        
        # 2. ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        graph = self.generate_instances(data, enable_virtual_entities=True)
        if not graph:
            safe_print("[WARN] ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨")
            return None
        
        # ìë™ìœ¼ë¡œ TTL íŒŒì¼ë¡œ ì €ì¥ (2ë‹¨ê³„ êµ¬ì¡°: schema.ttl + instances.ttl)
        # instances_reasoned.ttlì€ í•„ìš”ì‹œ ë³„ë„ë¡œ ìƒì„± (ì„±ëŠ¥ ê³ ë ¤)
        self.save_graph(
            save_schema_separately=True,
            save_instances_separately=True,
            save_reasoned_separately=False,  # ê¸°ë³¸ì ìœ¼ë¡œëŠ” ì¶”ë¡  ê·¸ë˜í”„ ìƒì„± ì•ˆ í•¨ (ì„±ëŠ¥ ê³ ë ¤)
            cleanup_old_files=True,
            backup_old_files=True
        )
        
        # ìºì‹œ ì €ì¥
        try:
            _cached_graph = self.graph
            _cached_data_hash = data_hash
            safe_print("[INFO] ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ ìºì‹œ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            safe_print(f"[WARN] ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return self.graph
    
    def _calculate_data_hash(self, data: Dict[str, pd.DataFrame]) -> str:
        """
        ë°ì´í„° ë”•ì…”ë„ˆë¦¬ì˜ í•´ì‹œ ê³„ì‚° (ìºì‹±ì„ ìœ„í•´)
        
        Args:
            data: {í…Œì´ë¸”ëª…: DataFrame} ë”•ì…”ë„ˆë¦¬
            
        Returns:
            í•´ì‹œ ë¬¸ìì—´
        """
        try:
            # ê° DataFrameì˜ í•´ì‹œ ê³„ì‚°
            hash_str = ""
            for name, df in sorted(data.items()):
                # DataFrameì˜ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í•´ì‹œ ê³„ì‚°
                df_str = df.to_string()
                hash_str += f"{name}:{hashlib.sha1(df_str.encode('utf-8')).hexdigest()}\n"
            
            # ì „ì²´ í•´ì‹œ ê³„ì‚°
            return hashlib.sha1(hash_str.encode('utf-8')).hexdigest()
        except Exception as e:
            safe_print(f"[WARN] ë°ì´í„° í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return ""
    
    def _extract_keywords_from_condition(self, condition: str) -> List[str]:
        """
        ì ìš©ì¡°ê±´ expressionì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        ì˜ˆ: "threat_level > 0.8" -> ["ê³ ìœ„í˜‘"]
        ì˜ˆ: "penetration == True" -> ["ì¹¨íˆ¬"]
        ì˜ˆ: "resources < 0.5" -> ["ìì›ë¶€ì¡±"]
        """
        keywords = []
        condition_lower = condition.lower().strip()
        
        # í‚¤ì›Œë“œ ë§¤í•‘ í…Œì´ë¸”
        keyword_mapping = {
            # ìœ„í˜‘ ê´€ë ¨
            "threat_level": "ìœ„í˜‘ìˆ˜ì¤€",
            "threat_level > 0.8": "ê³ ìœ„í˜‘",
            "threat_level >= 0.8": "ê³ ìœ„í˜‘",
            "threat_level < 0.3": "ì €ìœ„í˜‘",
            "threat_level <= 0.3": "ì €ìœ„í˜‘",
            # ì¹¨íˆ¬ ê´€ë ¨
            "penetration": "ì¹¨íˆ¬",
            "penetration == true": "ì¹¨íˆ¬",
            "penetration == True": "ì¹¨íˆ¬",
            # ìì› ê´€ë ¨
            "resources": "ìì›",
            "resources < 0.5": "ìì›ë¶€ì¡±",
            "resources <= 0.5": "ìì›ë¶€ì¡±",
            "resource": "ìì›",
            # ê¸°ë™ ê´€ë ¨
            "enemy_momentum": "ì ê¸°ì„¸",
            "enemy_momentum < 0.5": "ì ê¸°ì„¸ì•½í™”",
            "logistics_cut": "ë³´ê¸‰ì°¨ë‹¨",
            "logistics_cut > 0.7": "ë³´ê¸‰ì°¨ë‹¨",
            "deception": "ê¸°ë§Œ",
            "deception > 0.8": "ê¸°ë§Œ",
            "flank_exposed": "ì¸¡ë©´ë…¸ì¶œ",
            "flank_exposed == true": "ì¸¡ë©´ë…¸ì¶œ",
            "objective == 'limited'": "ì œí•œëª©í‘œ",
            "superiority": "ìš°ìœ„",
            "superiority > 0.6": "ìš°ìœ„",
            "firepower": "í™”ë ¥",
            "firepower > 0.8": "í™”ë ¥ìš°ìœ„",
            "reserve_available": "ì˜ˆë¹„ëŒ€ê°€ìš©",
            "reserve_available == true": "ì˜ˆë¹„ëŒ€ê°€ìš©",
        }
        
        # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
        if condition_lower in keyword_mapping:
            keywords.append(keyword_mapping[condition_lower])
        else:
            # ë¶€ë¶„ ë§¤ì¹­: ë³€ìˆ˜ëª… ì¶”ì¶œ
            import re
            # ë³€ìˆ˜ëª… íŒ¨í„´ (ì˜ë¬¸ì, ì–¸ë”ìŠ¤ì½”ì–´)
            var_pattern = r'\b([a-z_]+)\b'
            matches = re.findall(var_pattern, condition_lower)
            for match in matches:
                if match in keyword_mapping:
                    keywords.append(keyword_mapping[match])
                elif match not in ['and', 'or', 'not', 'true', 'false', 'level', 'available']:
                    # ì¼ë°˜ ë³€ìˆ˜ëª…ì„ í•œê¸€ë¡œ ë³€í™˜ ì‹œë„
                    var_keywords = {
                        'threat': 'ìœ„í˜‘',
                        'resource': 'ìì›',
                        'penetration': 'ì¹¨íˆ¬',
                        'momentum': 'ê¸°ì„¸',
                        'logistics': 'ë³´ê¸‰',
                        'deception': 'ê¸°ë§Œ',
                        'flank': 'ì¸¡ë©´',
                        'exposed': 'ë…¸ì¶œ',
                        'objective': 'ëª©í‘œ',
                        'superiority': 'ìš°ìœ„',
                        'firepower': 'í™”ë ¥',
                        'reserve': 'ì˜ˆë¹„ëŒ€',
                    }
                    if match in var_keywords:
                        keywords.append(var_keywords[match])
        
        # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì›ë³¸ì„ í‚¤ì›Œë“œë¡œ ì‚¬ìš© (ìµœì†Œí•œì˜ ì •ë³´ ë³´ì¡´)
        if not keywords:
            # ìˆ«ìì™€ ì—°ì‚°ì ì œê±° í›„ í‚¤ì›Œë“œ ì¶”ì¶œ
            cleaned = re.sub(r'[0-9.><=!&\|()\s]+', ' ', condition)
            cleaned_keywords = [k.strip() for k in cleaned.split() if k.strip() and len(k.strip()) > 2]
            keywords.extend(cleaned_keywords[:3])  # ìµœëŒ€ 3ê°œ
        
        return keywords if keywords else [condition[:20]]  # ìµœì†Œí•œ ì›ë³¸ ì¼ë¶€ ë°˜í™˜
    
    def _make_uri_safe(self, name: str) -> str:
        """
        URIì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë¬¸ìì—´ì„ ë³€í™˜
        ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
        
        Args:
            name: ë³€í™˜í•  ë¬¸ìì—´
            
        Returns:
            URI-safe ë¬¸ìì—´
        """
        if not name:
            return str(name) if name is not None else ""
        
        s = str(name).strip()
        # ê³µë°± -> ì–¸ë”ìŠ¤ì½”ì–´
        s = re.sub(r'\s+', '_', s)
        # URIì— ìœ„í—˜í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ìëŠ” ìœ ì§€)
        # ì œê±° ëŒ€ìƒ: ( ) { } [ ] < > | \ ^ ` " ' : ; , ? # % & + =
        s = re.sub(r'[(){}\[\]<>|\\^`"\':;,?#%&+=]', '', s)
        
        # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ ì •ë¦¬
        s = re.sub(r'_+', '_', s)
        s = s.strip('_')
        
        # ë¹ˆ ë¬¸ìì—´ì´ë©´ default ë°˜í™˜
        if not s:
            return "unknown"
            
        return s
    
    def get_sparql_template(self, template_name: str, **kwargs) -> str:
        """
        SPARQL ì¿¼ë¦¬ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            template_name: í…œí”Œë¦¿ ì´ë¦„ (find_suitable_coas, find_related_threats ë“±)
            **kwargs: í…œí”Œë¦¿ ë³€ìˆ˜ (top_k, situation_uri ë“±)
            
        Returns:
            ì™„ì„±ëœ SPARQL ì¿¼ë¦¬ ë¬¸ìì—´
        """
        try:
            import yaml
            from pathlib import Path
            
            template_path = Path(__file__).parent.parent / "config" / "sparql_templates.yaml"
            
            if not template_path.exists():
                safe_print(f"[WARN] SPARQL template file not found: {template_path}")
                return ""
            
            with open(template_path, 'r', encoding='utf-8') as f:
                templates = yaml.safe_load(f)
            
            template = templates.get(template_name, "")
            
            if not template:
                safe_print(f"[WARN] SPARQL template '{template_name}' not found")
                return ""
            
            # í…œí”Œë¦¿ ë³€ìˆ˜ ì¹˜í™˜
            # URI ë³€í™˜
            if 'situation_uri' in kwargs:
                situation_uri = kwargs['situation_uri']
                if isinstance(situation_uri, str):
                    # URI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    if not situation_uri.startswith('http://'):
                        # ì•ˆì „í•œ URIë¡œ ë³€í™˜
                        situation_uri = self._make_uri_safe(situation_uri)
                        situation_uri = URIRef(self.ns[situation_uri])
                    else:
                        situation_uri = URIRef(situation_uri)
                template = template.replace('?situation_uri', f'<{situation_uri}>')
            
            if 'coa_uri' in kwargs:
                coa_uri = kwargs['coa_uri']
                if isinstance(coa_uri, str):
                    if not coa_uri.startswith('http://'):
                        # ì•ˆì „í•œ URIë¡œ ë³€í™˜
                        coa_uri = self._make_uri_safe(coa_uri)
                        coa_uri = URIRef(self.ns[coa_uri])
                    else:
                        coa_uri = URIRef(coa_uri)
                template = template.replace('?coa_uri', f'<{coa_uri}>')
            
            # ê¸°íƒ€ ë³€ìˆ˜ ì¹˜í™˜
            for key, value in kwargs.items():
                if key not in ['situation_uri', 'coa_uri']:
                    template = template.replace(f'{{{key}}}', str(value))
            
            return template
            
        except Exception as e:
            safe_print(f"[WARN] Failed to load SPARQL template: {e}")
            return ""
    
    def execute_template_query(self, template_name: str, **kwargs) -> Union[List[Dict], pd.DataFrame]:
        """
        SPARQL í…œí”Œë¦¿ ì¿¼ë¦¬ ì‹¤í–‰
        
        Args:
            template_name: í…œí”Œë¦¿ ì´ë¦„
            **kwargs: í…œí”Œë¦¿ ë³€ìˆ˜ ë° return_format
            
        Returns:
            ì¿¼ë¦¬ ê²°ê³¼ (ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” DataFrame)
        """
        return_format = kwargs.pop('return_format', 'list')
        
        query_str = self.get_sparql_template(template_name, **kwargs)
        if not query_str:
            return [] if return_format == 'list' else pd.DataFrame()
        
        return self.query(query_str, return_format=return_format)
    
    def add_relationship(self, source_node_id: str, target_node_id: str, 
                       relation_name: str) -> bool:
        """
        ì œì•ˆëœ ê´€ê³„ë¥¼ ê·¸ë˜í”„ì— ì¶”ê°€
        
        Args:
            source_node_id: ì†ŒìŠ¤ ë…¸ë“œ ID (ì˜ˆ: "ì„ë¬´ì •ë³´_MSN001")
            target_node_id: íƒ€ê²Ÿ ë…¸ë“œ ID (ì˜ˆ: "ì „ì¥ì¶•ì„ _AXIS001")
            relation_name: ê´€ê³„ëª… (ì˜ˆ: "relatedTo", "hasMission")
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not RDFLIB_AVAILABLE or self.graph is None:
            return False
        
        try:
            # URI ìƒì„±
            source_uri = URIRef(self.ns[source_node_id])
            target_uri = URIRef(self.ns[target_node_id])
            relation_uri = URIRef(self.ns[relation_name])
            
            # ì´ë¯¸ ê´€ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸
            if (source_uri, relation_uri, target_uri) in self.graph:
                safe_print(f"[INFO] ê´€ê³„ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {source_node_id} -[{relation_name}]-> {target_node_id}")
                return True
            
            # ê´€ê³„ ì¶”ê°€
            self.graph.add((source_uri, relation_uri, target_uri))
            
            # ê´€ê³„ëª…ì´ OWL ObjectPropertyë¡œ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ì¶”ê°€
            if (relation_uri, RDF.type, OWL.ObjectProperty) not in self.graph:
                self.graph.add((relation_uri, RDF.type, OWL.ObjectProperty))
                safe_print(f"[INFO] ìƒˆë¡œìš´ ê´€ê³„ Property ìƒì„±: {relation_name}")
            
            safe_print(f"[INFO] ê´€ê³„ ì¶”ê°€ ì™„ë£Œ: {source_node_id} -[{relation_name}]-> {target_node_id}")
            return True
        except Exception as e:
            safe_print(f"[ERROR] ê´€ê³„ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def add_relationships_batch(self, relationships: List[Dict]) -> Dict[str, int]:
        """
        ì—¬ëŸ¬ ê´€ê³„ë¥¼ ì¼ê´„ ì¶”ê°€
        
        Args:
            relationships: [{"source": "...", "target": "...", "relation": "..."}, ...]
        
        Returns:
            {"success": ì„±ê³µ ìˆ˜, "failed": ì‹¤íŒ¨ ìˆ˜}
        """
        success_count = 0
        failed_count = 0
        
        for rel in relationships:
            source = rel.get("source")
            target = rel.get("target")
            relation = rel.get("relation", "relatedTo")
            
            if self.add_relationship(source, target, relation):
                success_count += 1
            else:
                failed_count += 1
        
        return {"success": success_count, "failed": failed_count}
    
    def remove_relationship(self, source_node_id: str, target_node_id: str, 
                           relation_name: str) -> bool:
        """
        ê´€ê³„ ì‚­ì œ
        
        Args:
            source_node_id: ì†ŒìŠ¤ ë…¸ë“œ ID
            target_node_id: íƒ€ê²Ÿ ë…¸ë“œ ID
            relation_name: ê´€ê³„ëª…
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not RDFLIB_AVAILABLE or self.graph is None:
            return False
        
        try:
            source_uri = URIRef(self.ns[source_node_id])
            target_uri = URIRef(self.ns[target_node_id])
            relation_uri = URIRef(self.ns[relation_name])
            
            # ê´€ê³„ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if (source_uri, relation_uri, target_uri) not in self.graph:
                safe_print(f"[WARN] ì‚­ì œí•  ê´€ê³„ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {source_node_id} -[{relation_name}]-> {target_node_id}")
                return False
            
            # ê´€ê³„ ì‚­ì œ
            self.graph.remove((source_uri, relation_uri, target_uri))
            safe_print(f"[INFO] ê´€ê³„ ì‚­ì œ ì™„ë£Œ: {source_node_id} -[{relation_name}]-> {target_node_id}")
            return True
        except Exception as e:
            safe_print(f"[ERROR] ê´€ê³„ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def update_relationship(self, source_node_id: str, target_node_id: str,
                           old_relation_name: str, new_relation_name: str,
                           new_target_node_id: Optional[str] = None) -> bool:
        """
        ê´€ê³„ ìˆ˜ì • (ê´€ê³„ëª… ë³€ê²½ ë˜ëŠ” íƒ€ê²Ÿ ë…¸ë“œ ë³€ê²½)
        
        Args:
            source_node_id: ì†ŒìŠ¤ ë…¸ë“œ ID
            target_node_id: ê¸°ì¡´ íƒ€ê²Ÿ ë…¸ë“œ ID
            old_relation_name: ê¸°ì¡´ ê´€ê³„ëª…
            new_relation_name: ìƒˆë¡œìš´ ê´€ê³„ëª…
            new_target_node_id: ìƒˆë¡œìš´ íƒ€ê²Ÿ ë…¸ë“œ ID (ì„ íƒì , Noneì´ë©´ íƒ€ê²Ÿ ë…¸ë“œëŠ” ë³€ê²½ ì•ˆ í•¨)
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not RDFLIB_AVAILABLE or self.graph is None:
            return False
        
        try:
            # ê¸°ì¡´ ê´€ê³„ ì‚­ì œ
            if not self.remove_relationship(source_node_id, target_node_id, old_relation_name):
                return False
            
            # ìƒˆë¡œìš´ íƒ€ê²Ÿ ë…¸ë“œ ê²°ì •
            final_target = new_target_node_id if new_target_node_id else target_node_id
            
            # ìƒˆë¡œìš´ ê´€ê³„ ì¶”ê°€
            if not self.add_relationship(source_node_id, final_target, new_relation_name):
                # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ê´€ê³„ ë³µêµ¬ ì‹œë„
                self.add_relationship(source_node_id, target_node_id, old_relation_name)
                return False
            
            safe_print(f"[INFO] ê´€ê³„ ìˆ˜ì • ì™„ë£Œ: {source_node_id} -[{old_relation_name}]-> {target_node_id} â†’ -[{new_relation_name}]-> {final_target}")
            return True
        except Exception as e:
            safe_print(f"[ERROR] ê´€ê³„ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            return False
    
    def get_all_relationships(self, source_node_id: Optional[str] = None,
                            target_node_id: Optional[str] = None,
                            relation_name: Optional[str] = None) -> List[Dict]:
        """
        ê´€ê³„ ì¡°íšŒ
        
        Args:
            source_node_id: ì†ŒìŠ¤ ë…¸ë“œ ID (ì„ íƒì , í•„í„°ë§)
            target_node_id: íƒ€ê²Ÿ ë…¸ë“œ ID (ì„ íƒì , í•„í„°ë§)
            relation_name: ê´€ê³„ëª… (ì„ íƒì , í•„í„°ë§)
        
        Returns:
            [{"source": "...", "target": "...", "relation": "...", "source_label": "...", "target_label": "..."}, ...]
        """
        if not RDFLIB_AVAILABLE or self.graph is None:
            return []
        
        relationships = []
        
        try:
            # í•„í„°ë§ ì¡°ê±´ ì„¤ì •
            source_uri = URIRef(self.ns[source_node_id]) if source_node_id else None
            target_uri = URIRef(self.ns[target_node_id]) if target_node_id else None
            relation_uri = URIRef(self.ns[relation_name]) if relation_name else None
            
            # ê·¸ë˜í”„ì—ì„œ ê´€ê³„ ì¡°íšŒ
            if source_uri:
                # íŠ¹ì • ì†ŒìŠ¤ ë…¸ë“œì˜ outgoing ê´€ê³„
                for _, p, o in self.graph.triples((source_uri, None, None)):
                    if isinstance(o, URIRef):
                        if target_uri and o != target_uri:
                            continue
                        if relation_uri and p != relation_uri:
                            continue
                        
                        source_local = _localname(source_uri)
                        target_local = _localname(o)
                        relation_local = _localname(p)
                        
                        # ë¼ë²¨ ê°€ì ¸ì˜¤ê¸°
                        source_label = self._get_node_label(source_uri)
                        target_label = self._get_node_label(o)
                        
                        relationships.append({
                            "source": source_local,
                            "target": target_local,
                            "relation": relation_local,
                            "source_label": source_label,
                            "target_label": target_label
                        })
            elif target_uri:
                # íŠ¹ì • íƒ€ê²Ÿ ë…¸ë“œì˜ incoming ê´€ê³„
                for s, p, _ in self.graph.triples((None, None, target_uri)):
                    if isinstance(s, URIRef):
                        if relation_uri and p != relation_uri:
                            continue
                        
                        source_local = _localname(s)
                        target_local = _localname(target_uri)
                        relation_local = _localname(p)
                        
                        source_label = self._get_node_label(s)
                        target_label = self._get_node_label(target_uri)
                        
                        relationships.append({
                            "source": source_local,
                            "target": target_local,
                            "relation": relation_local,
                            "source_label": source_label,
                            "target_label": target_label
                        })
            else:
                # ëª¨ë“  ê´€ê³„ ì¡°íšŒ
                for s, p, o in self.graph.triples((None, None, None)):
                    if isinstance(o, URIRef) and str(s).startswith(str(self.ns_legacy)) and str(o).startswith(str(self.ns_legacy)):
                        if relation_uri and p != relation_uri:
                            continue
                        
                        source_local = _localname(s)
                        target_local = _localname(o)
                        relation_local = _localname(p)
                        
                        source_label = self._get_node_label(s)
                        target_label = self._get_node_label(o)
                        
                        relationships.append({
                            "source": source_local,
                            "target": target_local,
                            "relation": relation_local,
                            "source_label": source_label,
                            "target_label": target_label
                        })
            
            return relationships
        except Exception as e:
            safe_print(f"[ERROR] ê´€ê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_node_label(self, node_uri: URIRef) -> str:
        """ë…¸ë“œì˜ ë¼ë²¨ ê°€ì ¸ì˜¤ê¸°"""
        try:
            for _, _, label in self.graph.triples((node_uri, RDFS.label, None)):
                return str(label)
            # ë¼ë²¨ì´ ì—†ìœ¼ë©´ local name ë°˜í™˜
            return _localname(node_uri)
        except Exception:
            return _localname(node_uri)
    
    def search_relationships(self, query: str, search_in_labels: bool = True) -> List[Dict]:
        """
        ê´€ê³„ ê²€ìƒ‰ (ë…¸ë“œ ID, ë¼ë²¨, ê´€ê³„ëª…ìœ¼ë¡œ ê²€ìƒ‰)
        
        Args:
            query: ê²€ìƒ‰ì–´
            search_in_labels: ë¼ë²¨ì—ì„œë„ ê²€ìƒ‰í• ì§€ ì—¬ë¶€
        
        Returns:
            ê²€ìƒ‰ëœ ê´€ê³„ ëª©ë¡
        """
        if not query:
            return []
        
        query_lower = query.lower()
        all_relationships = self.get_all_relationships()
        matched = []
        
        for rel in all_relationships:
            # ì†ŒìŠ¤ ë…¸ë“œ ID/ë¼ë²¨ ê²€ìƒ‰
            if query_lower in rel.get("source", "").lower():
                matched.append(rel)
                continue
            
            if search_in_labels and query_lower in rel.get("source_label", "").lower():
                matched.append(rel)
                continue
            
            # íƒ€ê²Ÿ ë…¸ë“œ ID/ë¼ë²¨ ê²€ìƒ‰
            if query_lower in rel.get("target", "").lower():
                matched.append(rel)
                continue
            
            if search_in_labels and query_lower in rel.get("target_label", "").lower():
                matched.append(rel)
                continue
            
            # ê´€ê³„ëª… ê²€ìƒ‰
            if query_lower in rel.get("relation", "").lower():
                matched.append(rel)
                continue
        
        return matched

    def get_entity_properties(self, entity_id: str) -> Dict[str, str]:
        """
        íŠ¹ì • ì—”í‹°í‹°ì˜ ëª¨ë“  ë°ì´í„°í˜• ì†ì„±(DatatypeProperty) ì¡°íšŒ
        
        Args:
            entity_id: ì—”í‹°í‹° ID (ì˜ˆ: TERR003, THR001)
            
        Returns:
            {í”„ë¡œí¼í‹°ëª…: ê°’} ë”•ì…”ë„ˆë¦¬
        """
        if not RDFLIB_AVAILABLE or self.graph is None:
            return {}
            
        # URI ìƒì„± ì‹œë„ (ns[entity_id] ë˜ëŠ” ns[ë©”ì´í¬_URI_safe(entity_id)])
        # instances.ttlì— ì €ì¥ëœ í˜•ì‹ì„ ê³ ë ¤í•˜ì—¬ ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„
        candidates = [
            URIRef(self.ns[entity_id]),
            URIRef(self.ns[self._make_uri_safe(entity_id)])
        ]
        
        # ì§€í˜•ì…€_ID ë“±ì˜ ì ‘ë‘ì–´ê°€ ë¶™ì€ ê²½ìš°ë„ ê³ ë ¤
        prefixes = ["ì§€í˜•ì…€_", "ìœ„í˜‘_", "ì•„êµ°ë¶€ëŒ€_", "ì êµ°ë¶€ëŒ€_"]
        for p in prefixes:
            candidates.append(URIRef(self.ns[f"{p}{entity_id}"]))
            candidates.append(URIRef(self.ns[self._make_uri_safe(f"{p}{entity_id}")]))
            
        properties = {}
        target_uri = None
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” URI ì°¾ê¸°
        for uri in candidates:
            if (uri, RDF.type, None) in self.graph:
                target_uri = uri
                break
        
        if not target_uri:
            return {}
            
        # í•´ë‹¹ URIì˜ ëª¨ë“  ì†ì„± ì¡°íšŒ
        for _, p, o in self.graph.triples((target_uri, None, None)):
            if isinstance(o, Literal):
                p_name = _localname(p)
                properties[p_name] = str(o)
                
        return properties

    def get_coordinates(self, entity_id: str) -> Optional[tuple]:
        """
        íŠ¹ì • ì—”í‹°í‹°ì˜ ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ì¢Œí‘œ(ìœ„ê²½ë„) ì¡°íšŒ
        
        Args:
            entity_id: ì—”í‹°í‹° ID (ì˜ˆ: TERR003)
            
        Returns:
            (latitude, longitude) íŠœí”Œ ë˜ëŠ” None
        """
        props = self.get_entity_properties(entity_id)
        if not props:
            return None
            
        # 1. ëª…ì‹œì  ìœ„ê²½ë„ í•„ë“œ í™•ì¸ (ns:hasLatitude, ns:hasLongitude)
        lat = props.get("hasLatitude")
        lng = props.get("hasLongitude")
        
        if lat and lng:
            try:
                return float(lat), float(lng)
            except ValueError:
                pass
                
        # 2. í†µí•© ì¢Œí‘œì •ë³´ í•„ë“œ í™•ì¸ (ns:ì¢Œí‘œì •ë³´ - "lng, lat" í˜•ì‹)
        coord_info = props.get("ì¢Œí‘œì •ë³´")
        if coord_info and "," in coord_info:
            try:
                parts = [p.strip() for p in coord_info.split(",")]
                if len(parts) >= 2:
                    # ì—‘ì…€ ë°ì´í„°ìƒ 127.0, 37.9 (lng, lat) ìˆœì„œì„ì„ ê³ ë ¤
                    return float(parts[1]), float(parts[0])
            except (ValueError, IndexError):
                pass
                
        return None


