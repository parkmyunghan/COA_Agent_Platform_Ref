# core_pipeline/rag_manager.py
# -*- coding: utf-8 -*-
"""
RAG Manager
Retrieval-Augmented Generation ê´€ë¦¬ ëª¨ë“ˆ
ê¸°ì¡´ rag_chunking.py, rag_hybrid.py í†µí•©
"""
import os
import re
import sys
import numpy as np
import logging
from typing import List, Dict, Optional
from pathlib import Path

logger = logging.getLogger(__name__)

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    logger.warning("faiss-cpu not installed. FAISS indexing will be disabled.")


def _get_windows_short_path(path: str) -> str:
    """
    Windowsì—ì„œ í•œê¸€ ê²½ë¡œë¥¼ ì§§ì€ ê²½ë¡œ(8.3 í˜•ì‹)ë¡œ ë³€í™˜
    
    Args:
        path: ì›ë³¸ ê²½ë¡œ
        
    Returns:
        ì§§ì€ ê²½ë¡œ ë˜ëŠ” ì›ë³¸ ê²½ë¡œ (ë³€í™˜ ì‹¤íŒ¨ ì‹œ)
    """
    if sys.platform != 'win32':
        return path
    
    try:
        import ctypes
        from ctypes import wintypes
        
        # GetShortPathNameW í•¨ìˆ˜ ì‚¬ìš©
        kernel32 = ctypes.windll.kernel32
        kernel32.GetShortPathNameW.argtypes = [wintypes.LPCWSTR, wintypes.LPWSTR, wintypes.DWORD]
        kernel32.GetShortPathNameW.restype = wintypes.DWORD
        
        # ë²„í¼ í¬ê¸° í™•ì¸
        buffer_size = kernel32.GetShortPathNameW(path, None, 0)
        if buffer_size == 0:
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê²½ë¡œ ë°˜í™˜
            return path
        
        # ë²„í¼ í• ë‹¹ ë° ë³€í™˜
        buffer = ctypes.create_unicode_buffer(buffer_size)
        result = kernel32.GetShortPathNameW(path, buffer, buffer_size)
        
        if result > 0:
            return buffer.value
        else:
            return path
    except Exception:
        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê²½ë¡œ ë°˜í™˜
        return path


class RAGManager:
    """RAG ê´€ë¦¬ì í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict):
        """
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.index = {}
        self.chunks = []
        self.embeddings = None
        self.embedding_model = None
        self.embedding_path = config.get("embedding_path", "./knowledge/embeddings")
        self.faiss_index = None
    
    def chunk_documents(self, docs: List[str], chunk_size: int = 500, overlap: int = 50, 
                       min_chunk_size: int = 100, use_sentence_chunking: bool = True,
                       doc_names: Optional[List[str]] = None) -> List[Dict]:
        """
        ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í•  (ê¸°ì¡´ rag_chunking.py í†µí•©)
        
        Args:
            docs: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            chunk_size: ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
            overlap: ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¬¸ì ìˆ˜
            min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸°
            use_sentence_chunking: ë¬¸ì¥ ë‹¨ìœ„ ì²­í‚¹ ì‚¬ìš© ì—¬ë¶€
            doc_names: ë¬¸ì„œ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ (ì˜µì…˜)
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸ [{"text": str, "start": int, "end": int, "doc_index": int, "source": str, ...}]
        """
        all_chunks = []
        
        # doc_namesê°€ ì—†ìœ¼ë©´ Noneìœ¼ë¡œ ì±„ì›€
        if doc_names is None:
            doc_names = [None] * len(docs)
        elif len(doc_names) != len(docs):
            logger.warning(f"ë¬¸ì„œ ìˆ˜({len(docs)})ì™€ íŒŒì¼ëª… ìˆ˜({len(doc_names)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            doc_names = [None] * len(docs)
        
        for doc_idx, doc in enumerate(docs):
            if use_sentence_chunking:
                chunks = self._chunk_text_by_sentences(doc, chunk_size, overlap, min_chunk_size)
            else:
                chunks = self._chunk_text_simple(doc, chunk_size, overlap, min_chunk_size)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            doc_name = doc_names[doc_idx]
            for i, chunk in enumerate(chunks):
                chunk["doc_index"] = doc_idx
                chunk["chunk_index"] = i
                chunk["total_chunks"] = len(chunks)
                if doc_name:
                    chunk["source"] = doc_name
                all_chunks.append(chunk)
        
        return all_chunks
    
    def _chunk_text_by_sentences(self, text: str, chunk_size: int, overlap: int, min_chunk_size: int) -> List[Dict]:
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì²­í‚¹"""
        if not text or len(text.strip()) < min_chunk_size:
            return [{"text": text.strip(), "start": 0, "end": len(text)}]
        
        sentence_pattern = r'([^.!?]+[.!?]+)'
        sentences = re.findall(sentence_pattern, text)
        
        if not sentences:
            return self._chunk_text_simple(text, chunk_size, overlap, min_chunk_size)
        
        chunks = []
        current_chunk = []
        current_length = 0
        start_pos = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            sentence_length = len(sentence)
            
            if current_length + sentence_length > chunk_size and current_chunk:
                chunk_text = " ".join(current_chunk)
                if len(chunk_text.strip()) >= min_chunk_size:
                    chunks.append({
                        "text": chunk_text.strip(),
                        "start": start_pos,
                        "end": start_pos + len(chunk_text)
                    })
                
                # Overlap ì²˜ë¦¬
                if overlap > 0 and len(current_chunk) > 0:
                    overlap_text = ""
                    overlap_sentences = []
                    for sent in reversed(current_chunk):
                        if len(overlap_text + sent) <= overlap:
                            overlap_sentences.insert(0, sent)
                            overlap_text = " ".join(overlap_sentences)
                        else:
                            break
                    
                    current_chunk = overlap_sentences + [sentence]
                    current_length = len(overlap_text) + sentence_length
                    start_pos = chunks[-1]["end"] - len(overlap_text) if chunks else 0
                else:
                    current_chunk = [sentence]
                    current_length = sentence_length
                    start_pos = start_pos + len(chunk_text) if chunks else 0
            else:
                current_chunk.append(sentence)
                current_length += sentence_length + 1
        
        # ë§ˆì§€ë§‰ ì²­í¬
        if current_chunk:
            chunk_text = " ".join(current_chunk)
            if len(chunk_text.strip()) >= min_chunk_size:
                chunks.append({
                    "text": chunk_text.strip(),
                    "start": start_pos,
                    "end": start_pos + len(chunk_text)
                })
        
        return chunks if chunks else [{"text": text.strip(), "start": 0, "end": len(text)}]
    
    def _chunk_text_simple(self, text: str, chunk_size: int, overlap: int, min_chunk_size: int) -> List[Dict]:
        """ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¶„í• """
        if not text or len(text.strip()) < min_chunk_size:
            return [{"text": text.strip(), "start": 0, "end": len(text)}]
        
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = min(start + chunk_size, text_length)
            chunk_text = text[start:end]
            
            # ë¬¸ì¥ ë ì°¾ê¸°
            if end < text_length:
                last_period = chunk_text.rfind('.')
                last_exclamation = chunk_text.rfind('!')
                last_question = chunk_text.rfind('?')
                last_newline = chunk_text.rfind('\n')
                
                last_break = max(last_period, last_exclamation, last_question, last_newline)
                
                if last_break > chunk_size * 0.7:
                    chunk_text = chunk_text[:last_break + 1]
                    end = start + last_break + 1
            
            chunk_text = chunk_text.strip()
            if len(chunk_text) >= min_chunk_size:
                chunks.append({
                    "text": chunk_text,
                    "start": start,
                    "end": end
                })
            
            start = end - overlap if overlap > 0 else end
            if start >= end:
                start = end
        
        return chunks if chunks else [{"text": text.strip(), "start": 0, "end": len(text)}]
    
    def chunk_doctrine_documents(self, docs: List[str], doc_names: Optional[List[str]] = None) -> List[Dict]:
        """
        êµë¦¬ ë¬¸ì„œë¥¼ êµë¦¬ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì²­í‚¹ (êµë¦¬ ë¬¸ì„œ ì „ìš©)
        
        êµë¦¬ ë¬¸ì„œ í˜•ì‹:
        # Doctrine_ID: DOCTRINE-XXX
        ## êµë¦¬ëª…: ...
        ## ì ìš© ì‘ì „ìœ í˜•: ...
        ## ê´€ë ¨ METT-C ìš”ì†Œ: ...
        
        ### Doctrine_Statement_ID: D-XXX-001
        - [êµë¦¬ ë¬¸ì¥]
        - **ì‘ì „ì  í•´ì„**: ...
        
        Args:
            docs: êµë¦¬ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            doc_names: ë¬¸ì„œ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ (ì˜µì…˜)
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸ [{
                "text": str,
                "doctrine_id": str,
                "statement_id": str,
                "mett_c_elements": List[str],
                "statement_text": str,  # ì‹¤ì œ êµë¦¬ ë¬¸ì¥ ë³¸ë¬¸
                "source": str,
                ...
            }]
        """
        import re
        all_chunks = []
        
        if doc_names is None:
            doc_names = [None] * len(docs)
        elif len(doc_names) != len(docs):
            print(f"[WARN] ë¬¸ì„œ ìˆ˜({len(docs)})ì™€ íŒŒì¼ëª… ìˆ˜({len(doc_names)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            doc_names = [None] * len(docs)
        
        for doc_idx, doc in enumerate(docs):
            doc_name = doc_names[doc_idx] or f"doc_{doc_idx}"
            
            # êµë¦¬ ë¬¸ì„œ í—¤ë” íŒŒì‹±
            doctrine_id_match = re.search(r'#\s*Doctrine_ID:\s*(DOCTRINE-[\w-]+)', doc, re.IGNORECASE)
            doctrine_id = doctrine_id_match.group(1) if doctrine_id_match else None
            
            mett_c_match = re.search(r'##\s*ê´€ë ¨\s*METT-C\s*ìš”ì†Œ:\s*([^\n]+)', doc, re.IGNORECASE)
            mett_c_str = mett_c_match.group(1).strip() if mett_c_match else ""
            mett_c_elements = [e.strip() for e in mett_c_str.split(',') if e.strip()] if mett_c_str else []
            
            # êµë¦¬ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            # íŒ¨í„´: ### Doctrine_Statement_ID: D-XXX-001
            statement_pattern = r'###\s*Doctrine_Statement_ID:\s*(D-[\w-]+-\d+)'
            
            statements = []
            for match in re.finditer(statement_pattern, doc):
                statement_id = match.group(1)
                start_pos = match.end()
                
                # ë‹¤ìŒ ë¬¸ì¥ IDê¹Œì§€ ë˜ëŠ” ë¬¸ì„œ ëê¹Œì§€
                next_match = None
                for next_match_iter in re.finditer(statement_pattern, doc):
                    if next_match_iter.start() > start_pos:
                        next_match = next_match_iter
                        break
                
                end_pos = next_match.start() if next_match else len(doc)
                statement_block = doc[start_pos:end_pos].strip()
                
                # êµë¦¬ ë¬¸ì¥ ë³¸ë¬¸ ì¶”ì¶œ (ì²« ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ í•­ëª© ë˜ëŠ” ì²« ë²ˆì§¸ ë¬¸ì¥)
                statement_text = ""
                lines = statement_block.split('\n')
                for line in lines:
                    line = line.strip()
                    # ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ í•­ëª© ì œê±°
                    if line.startswith('-'):
                        line = line[1:].strip()
                    # ë³¼ë“œ ì œê±°
                    line = re.sub(r'\*\*[^*]+\*\*:\s*', '', line)
                    # ì£¼ì„ ì œê±°
                    if line.startswith('#'):
                        continue
                    if line and len(line) > 10 and not line.startswith('**'):
                        statement_text = line
                        break
                
                # ì‘ì „ì  í•´ì„ ì¶”ì¶œ
                interpretation_match = re.search(r'\*\*ì‘ì „ì \s*í•´ì„\*\*:\s*([^\n]+)', statement_block, re.IGNORECASE)
                interpretation = interpretation_match.group(1).strip() if interpretation_match else ""
                
                if statement_text:
                    statements.append({
                        "statement_id": statement_id,
                        "statement_text": statement_text,
                        "interpretation": interpretation,
                        "full_block": statement_block
                    })
            
            # ê° êµë¦¬ ë¬¸ì¥ì„ ì²­í¬ë¡œ ìƒì„±
            for stmt in statements:
                # ì²­í¬ í…ìŠ¤íŠ¸: êµë¦¬ ë¬¸ì¥ ë³¸ë¬¸ + í•´ì„ (ê°„ê²°í•˜ê²Œ)
                chunk_text_parts = [stmt["statement_text"]]
                if stmt["interpretation"]:
                    chunk_text_parts.append(f"ì‘ì „ì  í•´ì„: {stmt['interpretation']}")
                
                chunk = {
                    "text": "\n".join(chunk_text_parts),
                    "doctrine_id": doctrine_id or "UNKNOWN",
                    "statement_id": stmt["statement_id"],
                    "mett_c_elements": mett_c_elements.copy(),
                    "statement_text": stmt["statement_text"],  # ì‹¤ì œ êµë¦¬ ë¬¸ì¥ ë³¸ë¬¸
                    "interpretation": stmt["interpretation"],
                    "source": doc_name,
                    "doc_index": doc_idx,
                    "chunk_type": "doctrine_statement"
                }
                all_chunks.append(chunk)
        
        return all_chunks
    
    def build_index(self, chunks: List[Dict], use_faiss: bool = True):
        """
        ì²­í¬ ì¸ë±ìŠ¤ êµ¬ì¶• (FAISS ì§€ì› ì¶”ê°€)
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸ (Dict ë˜ëŠ” str)
            use_faiss: FAISS ì¸ë±ìŠ¤ ì‚¬ìš© ì—¬ë¶€
        """
        # chunksë¥¼ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        # chunksë¥¼ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì„ë² ë”© ìƒì„±ì„ ìœ„í•´)
        self.chunks = chunks # ì›ë³¸(Dict ë¦¬ìŠ¤íŠ¸) ìœ ì§€
        
        # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        if chunks and isinstance(chunks[0], dict):
            text_chunks = [chunk.get("text", "") for chunk in chunks]
        else:
            text_chunks = [str(chunk) for chunk in chunks]
            # êµ¬í˜• ë°ì´í„° í˜¸í™˜ì„±ì„ ìœ„í•´ dictë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            self.chunks = [{"text": str(chunk)} for chunk in chunks]
        
        self.index = {i: chunk for i, chunk in enumerate(text_chunks)}
        
        # FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
        if use_faiss and FAISS_AVAILABLE and self.embedding_model is not None:
            try:
                # í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ì„ë² ë”© ê³„ì‚°
                embeddings = self.compute_embeddings(text_chunks)
                if embeddings is not None:
                    dimension = embeddings.shape[1]
                    self.faiss_index = faiss.IndexFlatL2(dimension)
                    self.faiss_index.add(embeddings.astype('float32'))
                    self.embeddings = embeddings
                    logger.info(f"FAISS index built: {len(text_chunks)} chunks, dimension {dimension}")
            except Exception as e:
                logger.warning(f"FAISS index build failed: {e}")
                self.faiss_index = None
        else:
            self.faiss_index = None
    
    def add_to_index(self, new_chunks: List[Dict]):
        """
        ê¸°ì¡´ ì¸ë±ìŠ¤ì— ìƒˆë¡œìš´ ì²­í¬ ì¶”ê°€ (ì¦ë¶„ ìƒ‰ì¸)
        
        Args:
            new_chunks: ì¶”ê°€í•  ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if not new_chunks:
            return

        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° chunks ì—…ë°ì´íŠ¸
        if new_chunks and isinstance(new_chunks[0], dict):
            new_text_chunks = [chunk.get("text", "") for chunk in new_chunks]
        else:
            new_text_chunks = [str(chunk) for chunk in new_chunks]
            new_chunks = [{"text": str(chunk)} for chunk in new_chunks] # ë©”íƒ€ë°ì´í„° ì •ê·œí™”
            
        # ê¸°ì¡´ ì²­í¬ì— ì¶”ê°€
        start_idx = len(self.chunks)
        self.chunks.extend(new_chunks) # ë©”íƒ€ë°ì´í„° í¬í•¨ ì²­í¬ ë¦¬ìŠ¤íŠ¸ í™•ì¥
        
        # ì¸ë±ìŠ¤ ë§µ ì—…ë°ì´íŠ¸
        for i, chunk_text in enumerate(new_text_chunks):
            self.index[start_idx + i] = chunk_text
            
        # 2. FAISS ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        if FAISS_AVAILABLE and self.embedding_model is not None:
            try:
                # ìƒˆ ì²­í¬ì— ëŒ€í•œ ì„ë² ë”© ê³„ì‚°
                new_embeddings = self.compute_embeddings(new_text_chunks)
                
                if new_embeddings is not None:
                    # FAISS ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                    if self.faiss_index is None:
                        dimension = new_embeddings.shape[1]
                        self.faiss_index = faiss.IndexFlatL2(dimension)
                        self.embeddings = new_embeddings
                    else:
                        # ê¸°ì¡´ ì„ë² ë”©ì— ì¶”ê°€
                        self.embeddings = np.concatenate((self.embeddings, new_embeddings), axis=0)
                    
                    # FAISSì— ì¶”ê°€
                    self.faiss_index.add(new_embeddings.astype('float32'))
                    logger.info(f"Added {len(new_chunks)} chunks to FAISS index. Total: {self.faiss_index.ntotal}")
                else:
                    logger.warning("Failed to compute embeddings for new chunks")
            except Exception as e:
                logger.warning(f"Failed to update FAISS index: {e}")
    
    def load_embeddings(self, model_path: Optional[str] = None, device: Optional[str] = None):
        """
        ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        models/embedding/rogel-embedding-v2 ê²½ë¡œì˜ ëª¨ë¸ ì‚¬ìš©
        
        Args:
            model_path: ì„ë² ë”© ëª¨ë¸ ê²½ë¡œ (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
            device: 'cpu' ë˜ëŠ” 'cuda' (Noneì´ë©´ ìë™ ì„ íƒ, GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ê²°ì •)
        """
        try:
            from sentence_transformers import SentenceTransformer
            import torch
        except Exception as e:
            logger.error(f"RAG ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return
        
        if model_path is None:
            # configì—ì„œ embedding ì„¹ì…˜ í™•ì¸
            embedding_config = self.config.get("embedding", {})
            model_path = embedding_config.get("model_path")
            
            # configì— embedding ì„¹ì…˜ì´ ì—†ìœ¼ë©´ model_config.yamlì—ì„œ ì§ì ‘ ë¡œë“œ
            if not model_path:
                try:
                    import yaml
                    base_dir = os.path.dirname(os.path.dirname(__file__))
                    model_config_path = os.path.join(base_dir, "config", "model_config.yaml")
                    if os.path.exists(model_config_path):
                        with open(model_config_path, 'r', encoding='utf-8') as f:
                            model_config = yaml.safe_load(f)
                            model_path = model_config.get("embedding", {}).get("model_path", "./models/embedding/rogel-embedding-v2")
                    else:
                        # rag_config.yamlë„ í™•ì¸
                        rag_config_path = os.path.join(base_dir, "config", "rag_config.yaml")
                        if os.path.exists(rag_config_path):
                            with open(rag_config_path, 'r', encoding='utf-8') as f:
                                rag_config = yaml.safe_load(f)
                                model_path = rag_config.get("embedding", {}).get("model_path", "./models/embedding/rogel-embedding-v2")
                        else:
                            model_path = "./models/embedding/rogel-embedding-v2"
                except Exception as e:
                    logger.warning(f"Config íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©")
                    model_path = "./models/embedding/rogel-embedding-v2"
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if model_path and not os.path.isabs(model_path):
            base_dir = os.path.dirname(os.path.dirname(__file__))
            model_path = os.path.join(base_dir, model_path)
            model_path = os.path.normpath(model_path)
        
        if not os.path.exists(model_path):
            logger.warning(f"Embedding model not found at {model_path}")
            logger.warning("   Using simple retrieval.")
            return
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        cuda_available = torch.cuda.is_available()
        
        # CPU ìš°ì„  ì •ì±… ê°•ì œ ì ìš©: ì•ˆì •ì„±ì„ ìœ„í•´ í•­ìƒ CPUë¡œ ë¡œë“œ
        if device is None or device == 'cuda':
            logger.info("CPU ìš°ì„  ì •ì±…: Embedding ëª¨ë¸ì„ CPUì— ë¡œë“œí•©ë‹ˆë‹¤.")
            device = 'cpu'
        # deviceê°€ ëª…ì‹œì ìœ¼ë¡œ 'cpu'ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
        
        # ëª¨ë¸ ë¡œë“œ (CPU ëª¨ë“œë¡œ ê°•ì œ, ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”)
        model = None
        max_retries = 3
        retry_count = 0
        
        while model is None and retry_count < max_retries:
            try:
                retry_count += 1
                if retry_count > 1:
                    logger.info(f"Embedding ëª¨ë¸ ë¡œë“œ ì¬ì‹œë„ {retry_count}/{max_retries}...")
                
                # CPU ëª¨ë“œë¡œ ì§ì ‘ ë¡œë“œ (ì•ˆì •ì„± ìš°ì„ )
                model = SentenceTransformer(model_path, device='cpu')
                logger.info(f"Embedding model loaded (CPU): {model_path}")
                device = 'cpu'
                break
                
            except Exception as e1:
                error_msg = str(e1)
                logger.warning(f"Embedding ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ (ì‹œë„ {retry_count}/{max_retries}): {error_msg[:150]}")
                
                if retry_count < max_retries:
                    # ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸°
                    import time
                    time.sleep(1)
                    continue
                else:
                    # ìµœì¢… ì‹¤íŒ¨
                    logger.error(f"Embedding model loading failed after {max_retries} attempts: {e1}")
                    import traceback
                    traceback.print_exc()
                    
                    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
                    if not os.path.exists(model_path):
                        logger.error(f"ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
                    else:
                        # ëª¨ë¸ íŒŒì¼ í™•ì¸
                        model_files = os.listdir(model_path)
                        logger.info(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ íŒŒì¼: {model_files[:10]}")
                    
                    # Noneì„ ë°˜í™˜í•˜ì§€ ì•Šê³  ê³„ì† ì§„í–‰ (RAG ê¸°ëŠ¥ì€ ì œí•œì ìœ¼ë¡œ ì‘ë™)
                    logger.warning("Embedding ëª¨ë¸ ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤. RAG ê¸°ëŠ¥ì€ ì œí•œì ì…ë‹ˆë‹¤.")
                    return
        
        self.embedding_model = model
        
        # ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë° chunks ë¡œë“œ ì‹œë„
        faiss_path = self.config.get("embedding", {}).get("index_path", 
                                                           os.path.join(self.embedding_path, "faiss_index.bin"))
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if faiss_path and not os.path.isabs(faiss_path):
            base_dir = os.path.dirname(os.path.dirname(__file__))
            faiss_path = os.path.join(base_dir, faiss_path)
            faiss_path = os.path.normpath(faiss_path)
        
        # ì €ì¥ëœ ì¸ë±ìŠ¤(chunks + FAISS) ë¡œë“œ ì‹œë„
        chunks_loaded = False
        try:
            self.load_index()  # chunksì™€ FAISS ì¸ë±ìŠ¤ ëª¨ë‘ ë¡œë“œ (ì¤‘ë³µ ë°©ì§€ ë¡œì§ í¬í•¨)
            if len(self.chunks) > 0:
                chunks_loaded = True
                logger.info(f"RAG ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.chunks)}ê°œ ì²­í¬")
                # load_index()ì—ì„œ ì´ë¯¸ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¶”ê°€ ë¡œë“œ ë¶ˆí•„ìš”
                if self.faiss_index is not None:
                    # FAISS ì¸ë±ìŠ¤ í¬ê¸°ì™€ ì²­í¬ ìˆ˜ ì¼ì¹˜ í™•ì¸
                    faiss_size = self.faiss_index.ntotal
                    chunks_size = len(self.chunks)
                    if faiss_size != chunks_size:
                        logger.warning(f"FAISS ì¸ë±ìŠ¤ í¬ê¸°({faiss_size})ì™€ ì²­í¬ ìˆ˜({chunks_size})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                        logger.warning(f"ì¸ë±ìŠ¤ ì¬êµ¬ì¶•ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                        # ë¶ˆì¼ì¹˜ ì‹œ ì´ˆê¸°í™”
                        self.faiss_index = None
                        self.chunks = []
                        chunks_loaded = False
        except Exception as e:
            logger.info(f"ì €ì¥ëœ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ì—†ìŒ: {e}")
        
        # load_index()ì—ì„œ FAISSë¥¼ ë¡œë“œí•˜ì§€ ëª»í•œ ê²½ìš°ì—ë§Œ ì—¬ê¸°ì„œ ë¡œë“œ ì‹œë„
        if chunks_loaded and self.faiss_index is None and os.path.exists(faiss_path) and FAISS_AVAILABLE:
            try:
                # Windowsì—ì„œ í•œê¸€ ê²½ë¡œ ì²˜ë¦¬
                faiss_path_normalized = _get_windows_short_path(faiss_path)
                self.faiss_index = faiss.read_index(faiss_path_normalized)
                logger.info(f"FAISS index loaded: {faiss_path}")
                
                # FAISS ì¸ë±ìŠ¤ í¬ê¸°ì™€ ì²­í¬ ìˆ˜ ì¼ì¹˜ í™•ì¸
                faiss_size = self.faiss_index.ntotal
                chunks_size = len(self.chunks)
                if faiss_size != chunks_size:
                    logger.warning(f"FAISS ì¸ë±ìŠ¤ í¬ê¸°({faiss_size})ì™€ ì²­í¬ ìˆ˜({chunks_size})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    logger.warning(f"ì¸ë±ìŠ¤ ì¬êµ¬ì¶•ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    # ë¶ˆì¼ì¹˜ ì‹œ ì´ˆê¸°í™”
                    self.faiss_index = None
                    self.chunks = []
            except Exception as e:
                logger.warning(f"FAISS index load failed: {e}")
                self.faiss_index = None
        elif os.path.exists(faiss_path) and not chunks_loaded:
            # FAISS ì¸ë±ìŠ¤ëŠ” ìˆì§€ë§Œ chunksê°€ ì—†ëŠ” ê²½ìš° (ë¶ˆì¼ì¹˜ ìƒíƒœ)
            logger.warning(f"FAISS ì¸ë±ìŠ¤ íŒŒì¼ì€ ìˆì§€ë§Œ ì²­í¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            logger.warning(f"ì¸ë±ìŠ¤ ì¬êµ¬ì¶•ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            # FAISS ì¸ë±ìŠ¤ëŠ” ë¡œë“œí•˜ì§€ ì•ŠìŒ (ë¶ˆì¼ì¹˜ ë°©ì§€)
    
    def compute_embeddings(self, texts: List[str]) -> Optional[np.ndarray]:
        """
        í…ìŠ¤íŠ¸ ì„ë² ë”© ê³„ì‚°
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ë°°ì—´ (ëª¨ë¸ì´ ì—†ìœ¼ë©´ None)
        """
        if self.embedding_model is None:
            return None
        
        try:
            embeddings = self.embedding_model.encode(texts, show_progress_bar=False)
            return embeddings
        except Exception as e:
            logger.warning(f"Failed to compute embeddings: {e}")
            return None
    
    def retrieve(self, query: str, top_k: int = 3, use_hybrid: bool = True) -> List[Dict]:
        """
        ì¿¼ë¦¬ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì§€ì›)
        ê¸°ì¡´ rag_hybrid.pyì˜ hybrid_search ë¡œì§ í†µí•©
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ kê°œ ê²°ê³¼
            use_hybrid: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€ (TF-IDF + Vector)
            
        Returns:
            [{"text": str, "score": float}] ë¦¬ìŠ¤íŠ¸
        """
        # ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìœ¼ë©´ ë¡œë“œ ì‹œë„ (Self-healing)
        if not self.index:
            try:
                logger.info("RAG ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆì–´ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
                self.load_index()
            except Exception as e:
                logger.warning(f"RAG ì¸ë±ìŠ¤ ìë™ ë¡œë“œ ì‹¤íŒ¨: {e}")

        if not self.index:
            logger.warning("RAG ì¸ë±ìŠ¤ê°€ ì—¬ì „íˆ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ.")
            return []
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (TF-IDF + Vector)
        if use_hybrid and self.embedding_model is not None:
            try:
                # Vector ê²€ìƒ‰ (ì„ë² ë”© ê¸°ë°˜)
                vector_results = self._vector_search(query, top_k)
                
                # TF-IDF ê²€ìƒ‰ (í‚¤ì›Œë“œ ê¸°ë°˜)
                tfidf_results = self._tfidf_search(query, top_k)
                
                # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê²°í•© (0.3 * TF-IDF + 0.7 * Vector)
                merged = {}
                for r in vector_results:
                    idx = r.get("index", -1)
                    if idx >= 0:
                        merged[idx] = {
                            "text": r.get("text", ""),
                            "vector_score": r.get("score", 0.0),
                            "tfidf_score": 0.0
                        }
                
                for r in tfidf_results:
                    idx = r.get("index", -1)
                    if idx >= 0:
                        if idx in merged:
                            merged[idx]["tfidf_score"] = r.get("score", 0.0)
                        else:
                            merged[idx] = {
                                "text": r.get("text", ""),
                                "vector_score": 0.0,
                                "tfidf_score": r.get("score", 0.0)
                            }
                
                # ìµœì¢… ì ìˆ˜ ê³„ì‚°
                final_results = []
                for idx, data in merged.items():
                    final_score = 0.3 * data["tfidf_score"] + 0.7 * data["vector_score"]
                    final_results.append({
                        "text": data["text"],
                        "score": final_score,
                        "index": idx
                    })
                
                final_results.sort(key=lambda x: -x["score"])
                return final_results[:top_k]
                
            except Exception as e:
                logger.warning(f"Hybrid search failed: {e}. Using simple retrieval.")
        
        # ë‹¨ìˆœ ì„ë² ë”© ê²€ìƒ‰
        if self.embedding_model is not None:
            try:
                query_embedding = self.embedding_model.encode([query], show_progress_bar=False)[0]
                
                if self.embeddings is None:
                    chunk_texts = list(self.index.values())
                    self.embeddings = self.compute_embeddings(chunk_texts)
                
                if self.embeddings is not None:
                    # FAISS ì¸ë±ìŠ¤ ì‚¬ìš©
                    if self.faiss_index is not None:
                        query_emb = query_embedding.reshape(1, -1).astype('float32')
                        distances, indices = self.faiss_index.search(query_emb, top_k)
                        
                        results = []
                        for i, idx in enumerate(indices[0]):
                            if idx < len(self.chunks):
                                # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (1 / (1 + distance))
                                score = 1.0 / (1.0 + float(distances[0][i]))
                                results.append({
                                    "text": self.chunks[idx],
                                    "score": score,
                                    "index": int(idx)
                                })
                        return results
                    else:
                        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                        scores = np.dot(self.embeddings, query_embedding) / (
                            np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_embedding)
                        )
                        
                        top_indices = np.argsort(scores)[::-1][:top_k]
                        
                        results = []
                        for idx in top_indices:
                            results.append({
                                "text": self.index[idx],
                                "score": float(scores[idx]),
                                "index": int(idx)
                            })
                        return results
            except Exception as e:
                logger.warning(f"Embedding-based retrieval failed: {e}. Using keyword retrieval.")
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (fallback)
        return self._tfidf_search(query, top_k)
    
    def _vector_search(self, query: str, top_k: int) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰"""
        if self.embedding_model is None or self.embeddings is None:
            return []
        
        try:
            query_embedding = self.embedding_model.encode([query], show_progress_bar=False)[0]
            scores = np.dot(self.embeddings, query_embedding) / (
                np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_embedding)
            )
            
            top_indices = np.argsort(scores)[::-1][:top_k]
            results = []
            for idx in top_indices:
                results.append({
                    "text": self.index[idx],
                    "score": float(scores[idx]),
                    "index": int(idx)
                })
            return results
        except Exception:
            return []
    
    def _tfidf_search(self, query: str, top_k: int) -> List[Dict]:
        """TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰"""
        from collections import Counter
        
        query_tokens = re.findall(r"[\wê°€-í£_-]+", query.lower())
        if not query_tokens:
            return []
        
        scored = []
        for idx, chunk in self.index.items():
            chunk_tokens = re.findall(r"[\wê°€-í£_-]+", chunk.lower())
            if not chunk_tokens:
                continue
            
            # ê°„ë‹¨í•œ TF-IDF ì ìˆ˜ ê³„ì‚°
            chunk_counter = Counter(chunk_tokens)
            score = sum(chunk_counter.get(token, 0) for token in query_tokens) / max(len(chunk_tokens), 1)
            
            if score > 0:
                scored.append({
                    "text": chunk,
                    "score": score,
                    "index": idx
                })
        
        scored.sort(key=lambda x: -x["score"])
        return scored[:top_k]
    
    def save_index(self, path: Optional[str] = None):
        """
        ì¸ë±ìŠ¤ ì €ì¥ (FAISS ì¸ë±ìŠ¤ í¬í•¨)
        
        Args:
            path: ì €ì¥ ê²½ë¡œ
        """
        if path is None:
            path = os.path.join(self.embedding_path, "rag_index.json")
        
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        import json
        index_data = {
            "chunks": self.chunks,
            "index": self.index
        }
        
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        if self.faiss_index is not None and FAISS_AVAILABLE:
            faiss_path = self.config.get("embedding", {}).get("index_path", 
                                                               os.path.join(self.embedding_path, "faiss_index.bin"))
            try:
                # Windowsì—ì„œ í•œê¸€ ê²½ë¡œ ì²˜ë¦¬
                faiss_path_normalized = _get_windows_short_path(faiss_path)
                faiss.write_index(self.faiss_index, faiss_path_normalized)
                logger.info(f"FAISS index saved: {faiss_path}")
            except Exception as e:
                logger.warning(f"FAISS index save failed: {e}")
    
    def load_index(self, path: Optional[str] = None):
        """
        ì¸ë±ìŠ¤ ë¡œë“œ (FAISS ì¸ë±ìŠ¤ í¬í•¨)
        
        Args:
            path: ë¡œë“œ ê²½ë¡œ
        """
        # ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ì¤‘ë³µ ë¡œë“œ ë°©ì§€
        if self.faiss_index is not None and len(self.chunks) > 0:
            return  # ì´ë¯¸ ë¡œë“œë˜ì—ˆìœ¼ë¯€ë¡œ ì¬ë¡œë“œ ë¶ˆí•„ìš”
        
        if path is None:
            path = os.path.join(self.embedding_path, "rag_index.json")
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if path and not os.path.isabs(path):
            base_dir = os.path.dirname(os.path.dirname(__file__))
            path = os.path.join(base_dir, path)
            path = os.path.normpath(path)
        
        # FAISS ì¸ë±ìŠ¤ ê²½ë¡œ í™•ì¸
        faiss_path = self.config.get("embedding", {}).get("index_path", 
                                                           os.path.join(self.embedding_path, "faiss_index.bin"))
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if faiss_path and not os.path.isabs(faiss_path):
            base_dir = os.path.dirname(os.path.dirname(__file__))
            faiss_path = os.path.join(base_dir, faiss_path)
            faiss_path = os.path.normpath(faiss_path)
        
        has_faiss_file = os.path.exists(faiss_path) and FAISS_AVAILABLE
        
        # rag_index.json íŒŒì¼ í™•ì¸
        has_index_file = os.path.exists(path)
        
        # ë¶ˆì¼ì¹˜ ê°ì§€ ë° ê²½ê³ 
        if has_faiss_file and not has_index_file:
            logger.warning(f"FAISS ì¸ë±ìŠ¤ íŒŒì¼ì€ ìˆì§€ë§Œ rag_index.jsonì´ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ ë¶ˆì¼ì¹˜ ê°€ëŠ¥ì„±.")
            logger.warning(f"FAISS ì¸ë±ìŠ¤ íŒŒì¼: {faiss_path}")
            logger.warning(f"rag_index.json íŒŒì¼: {path}")
            logger.warning(f"ì²­í¬ ë°ì´í„° ì—†ì´ FAISS ì¸ë±ìŠ¤ë§Œ ë¡œë“œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ ì¬êµ¬ì¶•ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        if not has_index_file:
            logger.warning(f"Index file not found: {path}")
            return
        
        import json
        with open(path, 'r', encoding='utf-8') as f:
            index_data = json.load(f)
        
        self.chunks = index_data.get("chunks", [])
        self.index = {int(k): v for k, v in index_data.get("index", {}).items()}
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        if has_faiss_file:
            try:
                # Windowsì—ì„œ í•œê¸€ ê²½ë¡œ ì²˜ë¦¬
                faiss_path_normalized = _get_windows_short_path(faiss_path)
                self.faiss_index = faiss.read_index(faiss_path_normalized)
                logger.info(f"FAISS index loaded: {faiss_path}")
                
                # FAISS ì¸ë±ìŠ¤ í¬ê¸°ì™€ ì²­í¬ ìˆ˜ ì¼ì¹˜ í™•ì¸
                faiss_size = self.faiss_index.ntotal
                chunks_size = len(self.chunks)
                if faiss_size != chunks_size:
                    logger.warning(f"FAISS ì¸ë±ìŠ¤ í¬ê¸°({faiss_size})ì™€ ì²­í¬ ìˆ˜({chunks_size})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    logger.warning(f"ì¸ë±ìŠ¤ ì¬êµ¬ì¶•ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"FAISS index load failed: {e}")
                self.faiss_index = None
    
    def retrieve_with_context(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í›„, ê° ë¬¸ì„œ chunkì™€ scoreë¥¼ ìƒì„¸ ì •ë³´ì™€ í•¨ê»˜ ë°˜í™˜
        ì¸ìš© ê¸°ë°˜ ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ í™•ì¥ ë²„ì „
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ kê°œ ê²°ê³¼
            
        Returns:
            [{"doc_id": int, "text": str, "score": float, "index": int, "metadata": dict}] ë¦¬ìŠ¤íŠ¸
        """
        # ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰
        results = self.retrieve(query, top_k=top_k, use_hybrid=True)
        
        # ìƒì„¸ ì •ë³´ ì¶”ê°€
        detailed_results = []
        for i, result in enumerate(results):
            idx = result.get("index", i)
            text = result.get("text", "")
            score = result.get("score", 0.0)
            
            # ğŸ”¥ ê°œì„ : ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (chunk ì •ë³´ì—ì„œ, êµë¦¬ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í¬í•¨)
            metadata = {}
            if idx < len(self.chunks):
                chunk_info = self.chunks[idx] if isinstance(self.chunks[idx], dict) else {"text": self.chunks[idx]}
                metadata = {
                    "chunk_index": idx,
                    "start": chunk_info.get("start", 0),
                    "end": chunk_info.get("end", len(text)),
                    "doc_index": chunk_info.get("doc_index", 0),
                    "chunk_index_in_doc": chunk_info.get("chunk_index", 0),
                    "source": chunk_info.get("source", "")
                }
                
                # ğŸ”¥ êµë¦¬ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í¬í•¨
                if chunk_info.get("chunk_type") == "doctrine_statement":
                    metadata.update({
                        "doctrine_id": chunk_info.get("doctrine_id"),
                        "statement_id": chunk_info.get("statement_id"),
                        "statement_text": chunk_info.get("statement_text"),
                        "interpretation": chunk_info.get("interpretation"),
                        "mett_c_elements": chunk_info.get("mett_c_elements", []),
                        "operation_type": chunk_info.get("operation_type")
                    })
            
            # ğŸ”¥ ë©”íƒ€ë°ì´í„°ë¥¼ ê²°ê³¼ì— ì§ì ‘ í¬í•¨ (í•˜ìœ„ í˜¸í™˜ì„±)
            result_dict = {
                "doc_id": idx,
                "text": text,
                "score": score,
                "index": idx,
                "metadata": metadata
            }
            
            # ğŸ”¥ êµë¦¬ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ìµœìƒìœ„ ë ˆë²¨ì—ë„ í¬í•¨
            if metadata.get("doctrine_id"):
                result_dict.update({
                    "doctrine_id": metadata["doctrine_id"],
                    "statement_id": metadata.get("statement_id"),
                    "statement_text": metadata.get("statement_text"),
                    "mett_c_elements": metadata.get("mett_c_elements", [])
                })
            
            detailed_results.append(result_dict)
        
        return detailed_results
    
    def is_available(self) -> bool:
        """
        RAG Manager ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        
        Returns:
            ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ (ì„ë² ë”© ëª¨ë¸ê³¼ ì¸ë±ìŠ¤ê°€ ìˆëŠ” ê²½ìš° True)
        """
        # ì„ë² ë”© ëª¨ë¸ì´ ìˆê³ , ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ì‚¬ìš© ê°€ëŠ¥
        has_model = self.embedding_model is not None
        has_index = len(self.chunks) > 0 or (self.faiss_index is not None and FAISS_AVAILABLE)
        return has_model and has_index

    def get_indexed_sources(self) -> set:
        """
        í˜„ì¬ ì¸ë±ìŠ¤ì— í¬í•¨ëœ ë¬¸ì„œë“¤ì˜ ì†ŒìŠ¤(íŒŒì¼ëª…) ëª©ë¡ ë°˜í™˜
        """
        sources = set()
        for chunk in self.chunks:
            if isinstance(chunk, dict) and "source" in chunk:
                sources.add(chunk["source"])
        return sources




